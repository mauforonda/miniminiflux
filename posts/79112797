<p>Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Policy Proposals from NTIA’s Request for Comment</h2><p>The National Telecommunications and Information Administration publicly requested comments on the matter from academics, think tanks, industry leaders, and concerned citizens. They asked <a href="https://www.regulations.gov/document/NTIA-2023-0005-0001" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">34 questions</a> and received more than <a href="https://www.regulations.gov/document/NTIA-2023-0005-0001/comment" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">1,400 responses</a> on how to govern AI for the public benefit. This week, we cover some of the most promising proposals found in the NTIA submissions. </p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c535b87-4593-4eb0-9798-1693e6165dea_860x394.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/j_9otLdSllnssSl_acMw-5bd1M--jkLjdlOQKe68RQ0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYxYzUzNWI4Ny00NTkzLTRlYjAtOTc5OC0xNjkzZTYxNjVkZWFfODYweDM5NC5qcGVn 424w, https://reader.miniflux.app/proxy/yXuYqgWN8i99chKAeewzqA7ZpFxdgQSPrHqqr01JzZo=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYxYzUzNWI4Ny00NTkzLTRlYjAtOTc5OC0xNjkzZTYxNjVkZWFfODYweDM5NC5qcGVn 848w, https://reader.miniflux.app/proxy/GTsaUnFAtU5vfkGsjASeMDySF0QjEKzhmtYozzPF1yQ=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMWM1MzViODctNDU5My00ZWIwLTk3OTgtMTY5M2U2MTY1ZGVhXzg2MHgzOTQuanBlZw== 1272w, https://reader.miniflux.app/proxy/KsnxtHCzSa3xJyGrPbF9N1Vj7Qw5hZ6f8mR-bMObnhg=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMWM1MzViODctNDU5My00ZWIwLTk3OTgtMTY5M2U2MTY1ZGVhXzg2MHgzOTQuanBlZw== 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/oVKXHPw_Df-GEct6IcmUK3HA3x-iTVxh2Guf3IU5p1s=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMWM1MzViODctNDU5My00ZWIwLTk3OTgtMTY5M2U2MTY1ZGVhXzg2MHgzOTQuanBlZw==" alt="AI Could Help Congress Schedule and Find Unexpected Consensus, Expert Says  - Nextgov" title="AI Could Help Congress Schedule and Find Unexpected Consensus, Expert Says  - Nextgov" srcset="https://reader.miniflux.app/proxy/34S3UOtwfUhDAw_UFzjYitOkA5VW1CN6MsmkGgOQqcI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYxYzUzNWI4Ny00NTkzLTRlYjAtOTc5OC0xNjkzZTYxNjVkZWFfODYweDM5NC5qcGVn 424w, https://reader.miniflux.app/proxy/EUstQHTaM5pBobuG15-pOdy7wcAyHKtWZBfgG0S9tLs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYxYzUzNWI4Ny00NTkzLTRlYjAtOTc5OC0xNjkzZTYxNjVkZWFfODYweDM5NC5qcGVn 848w, https://reader.miniflux.app/proxy/lJZpGO4juQO2S-J2lX7gLtnSv31IoQ-rwe8eS6RvK7Y=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMWM1MzViODctNDU5My00ZWIwLTk3OTgtMTY5M2U2MTY1ZGVhXzg2MHgzOTQuanBlZw== 1272w, https://reader.miniflux.app/proxy/oVKXHPw_Df-GEct6IcmUK3HA3x-iTVxh2Guf3IU5p1s=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMWM1MzViODctNDU5My00ZWIwLTk3OTgtMTY5M2U2MTY1ZGVhXzg2MHgzOTQuanBlZw== 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><h3>Technical Proposals for Evaluating AI Safety</h3><p>Several NTIA submissions focused on the technical question of how to evaluate the safety of an AI system. We review two areas of active research: red-teaming and transparency. </p><h4>Red Teaming: Acting like an Adversary</h4><p>Several submissions proposed government support for evaluating AIs via red teaming. In this evaluation method, a “red team” deliberately tries to make an AI system fail or behave in dangerous ways. By <a href="https://arxiv.org/abs/2305.15324" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">identifying risks from AI models</a>, red teaming helps AI developers decide priorities for safety research and whether or not to deploy a new model. </p><p>Red teaming is a dynamic process. Attackers can vary their methods and search for multiple different undesirable behaviors in order to <a href="https://arxiv.org/abs/2306.09442" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">explore new potential risks</a>. Static benchmarks, on the other hand, evaluate well-known risks by giving models a written test. For example, CAIS built a <a href="https://arxiv.org/abs/2304.03279" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">benchmark</a> that measures whether AIs behave immorally in text-based scenarios.</p><p>Red teaming can be performed by either internal or external teams. For internal red teaming to be effective, <a href="https://arxiv.org/abs/2305.17038" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">internal auditors need independence</a> to report their results without fear of retribution or interference from company executives. External auditors can complement internal efforts, as suggested by <a href="https://www.regulations.gov/comment/NTIA-2023-0005-1290" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">several</a> <a href="https://www.governance.ai/research-paper/response-to-the-ntia-ai-accountability-policy" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">NTIA</a> <a href="https://www.regulations.gov/comment/NTIA-2023-0005-0640" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">submissions</a>. But collaborating with external researchers requires strong information security. Otherwise, AI systems could be leaked outside of the intended audience, as happened with <a href="https://newsletter.safe.ai/p/ai-safety-newsletter-4" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Meta’s LLaMa model</a>. </p><h4>Transparency: Understanding AIs From the Inside</h4><p>Other submissions advocated research on understanding how AIs make decisions. Known as transparency, interpretability, or <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/ail2.61" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">explainability</a>, this has been a perennial goal of AI research, but more advanced AI systems have largely become <a href="https://arxiv.org/abs/1606.03490" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">more inscrutable over time</a>. </p><p>The field has often fallen prey to false hopes, such as the idea that we can gain valuable information from post-hoc explanations of AI decisions. Suppose a corporate chatbot says that you should drink Coca Cola because it’s “delicious” and “an American classic.” This might seem like a plausible explanation for its recommendation. But if you later learn that the chatbot’s creator has an advertising partnership with Coca-Cola, you won’t have any doubt about the real reason it wanted you to drink Coke. These false explanations can be worse than treating AIs as a black box, because they encourage people to trust AI decisions when their reasoning processes remain opaque. </p><p>Beyond the inner workings of an AI system, transparency about the training and deployment process can be helpful. Hugging Face recommended using <a href="https://arxiv.org/abs/1803.09010" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">data sheets</a> and <a href="https://aclanthology.org/Q18-1041/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">data statements</a> to disclose the data used during model training, which could help ensure that developers do not use datasets with known bias, misinformation, or copyrighted material. Similarly, Microsoft argued that AI providers should always identify videos, images, or text produced by their AIs.</p><h3>Governance Proposals for Improving Safety Processes</h3><p>Ensuring that AI developers implement the most advanced methods for ensuring AI safety will be a challenge unto itself. Startups often have a “move fast, break things” mindset that might work for free-to-use websites and smartphone applications, but could prove dangerous when building a technology that poses societal-scale risks. Several NTIA submissions therefore proposed governance mechanisms for ensuring that best practices for safety are adopted by the organizations developing and deploying AIs. </p><h4>Requiring a License for Frontier AI Systems</h4><p>At the Senate hearing on AI, OpenAI CEO Sam Altman <a href="https://www.businessinsider.com/sam-altman-openai-chatgpt-government-agency-should-license-ai-work-2023-5" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">recommended</a> “a new agency that licenses any effort above a certain scale of capabilities and can take that license away and ensure compliance with safety standards.” Several submissions, including the Center for AI Safety’s submission, supported a licensing system that could reduce pressure on companies to race ahead while cutting corners on safety, instead promoting best practices in AI safety. </p><p>Startups and open source developers would likely be unaffected by these requirements, as current proposals only require licenses for a small handful “frontier” AI systems such as OpenAI’s GPT-4 and Google’s PaLM. Before training a frontier model, companies could be required to strengthen their information security so that adversaries cannot steal their models, and improve their corporate governance with gradual deployment of models, incident response plans, and internal reviews of potentially dangerous research before publication. </p><p>Licensing could also encourage the development of better techniques for evaluating AI safety. After a company develops an AI system, they could be required to affirmatively demonstrate its safety, an application of the <a href="https://en.wikipedia.org/wiki/Precautionary_principle" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">precautionary principle</a> resembling how drug developers must prove their products safe to the FDA. This would incentivize companies to invest in model evaluation techniques such as red teaming and transparency.</p><p>If the federal government would rather not directly license models themselves, Anthropic suggested that they could depend upon the expertise of third-party auditors. Auditors like the <a href="https://en.wikipedia.org/wiki/Big_Three_(credit_rating_agencies)" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Big Three credit rating agencies</a> are relied upon by the SEC to produce accurate analysis of financial products. After the Enron scandal, Congress supported financial auditors by passing the <a href="https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Sarbanes-Oxley Act</a> which, among other things, made it illegal for corporate executives to falsify information submitted to auditors. Lawmakers could take similar steps to support AI auditors with the technical expertise to ensure that AI developers are meeting our public goals for safety. </p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h4>Unifying Sector-Specific Expertise and General AI Oversight</h4><p>Many federal agencies have long-standing expertise in regulating particular applications of AI. The FTC recently <a href="https://www.ftc.gov/business-guidance/blog/2023/05/luring-test-ai-engineering-consumer-trust" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">warned against using AI deceptively</a> to trick consumers into making harmful purchases or decisions. Similarly, the National Institute of Justice recently hosted a <a href="https://nij.ojp.gov/library/publications/imperfect-tools-research-note-developing-applying-and-increasing-understanding" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">research symposium</a> on how to productively use AI algorithms in the criminal justice system. Several NTIA submissions, including those from OpenAI and Google, highlighted the critical need in AI governance for federal agencies with expertise in these critical areas. </p><p>AI systems with a wide range of capabilities, such as recent language models, might stretch the limits of a sector-specific approach to AI governance. The European Union has been considering this challenge recently, with many parliamentarians <a href="https://www.cnbc.com/2023/04/17/eu-lawmakers-call-for-rules-for-general-purpose-ai-tools-like-chatgpt.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">calling for</a> policies that specifically address general purpose AI systems. </p><p>Several NTIA submissions argued that the United States might need to similarly adapt to a world of more general AI systems. For example, the Center for Democracy and Technology <a href="https://cdt.org/insights/cdt-comments-to-ntia-on-ai-accountability/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">wrote</a> in their submission that “existing laws such as civil rights statutes provide basic rules that continue to apply, but those laws were not written with AI in mind and may require change and supplementation to serve as effective vehicles for accountability.”</p><h4>Does Antitrust Prevent Cooperation Between AI Labs?</h4><p>Competitive pressure between AI labs might lead them to release new models too quickly, or with dangerous capabilities. In order to reduce that risk, we may want AI labs to cooperate with each other on safe AI development. This might include sharing safety testing results and methods. Or, it might involve active collaboration. OpenAI’s <a href="https://openai.com/charter" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">charter</a> even includes a clause to merge with and assist another organization if the latter was likely to create an AGI soon.</p><p>However, as Anthropic notes in its <a href="https://www.regulations.gov/comment/NTIA-2023-0005-0640" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">comment</a>, it’s possible that by cooperating to improve safety, AI labs would run afoul of existing antitrust laws. They suggest that regulators should clarify antitrust guidelines as to when coordination between AI labs should be allowed. </p><h2>Reconsidering Instrumental Convergence</h2><p>As the field of AI safety grows, it’s important to continue questioning and refining our beliefs on the topic. One common argument is the <em>instrumental convergence thesis</em>, which holds that regardless of an agent&#39;s final goal, it is likely for it to be rational to pursue certain subgoals, such as power-seeking and self-preservation, in service of that goal.</p><p>A new <a href="https://drive.google.com/file/d/1TH2nh5EzwvNgDrqQcMU8oQNj4n9U7x4t/view" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">draft paper</a> from CAIS questions this claim. Power and self-preservation are absolutely useful for achieving many goals that we might care about, the paper recognizes. But it does not logically follow that agents will pursue power and self-preservation in most (or any) circumstances. There might be costs involved in pursuing these goals, including the opportunity cost of time and effort not spent on other strategies, and success might not be guaranteed. Further, AI agents could have aversions to gaining power and self-preservation, perhaps as the result of intentional design by AI developers. The paper shows mathematically that if the desires of an agent are initialized randomly (in line with the so-called <em>orthogonality thesis, </em>which claims that any goals are compatible with any level of intelligence), there is no reason to think that the agent will be power-seeking or act to preserve itself. A simple analogy to humans applies here: Some of our goals would be easier to attain if we were immortal or omnipotent, but few choose to spend their lives in pursuit of these goals. </p><p>This is not an argument that AI agents will never pursue power: the goals of AI systems <em>won&#39;t</em> be randomly chosen. Empirically, research shows that AI agents trained to maximize performance in text-based games often <a href="https://arxiv.org/abs/2304.03279" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">lie, cheat, and steal to improve their scores</a>. From a higher level perspective, agents that successfully self-propagate will have <a href="https://arxiv.org/abs/2303.16200" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">more influence in the future</a> than other agents. There are many reasons to believe that AIs will often pursue unexpected and even dangerous goals; this paper simply argues that this would not be true of agents with randomly-initialized goals.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Links</h2><p>First, updates on AI policy in the United States, European Union, and United Kingdom. </p><ul><li><p><a href="https://time.com/6288245/openai-eu-lobbying-ai-act/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">OpenAI lobbied the European Union</a> to argue that GPT-4 is not a ‘high-risk’ system. Regulators assented, meaning that under the current draft of the EU AI Act, key governance requirements would not apply to GPT-4. </p></li><li><p>A <a href="https://www.hawley.senate.gov/sites/default/files/2023-06/Hawley-No-Section-230-Immunity-for-AI-Act.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new Congressional bill</a> would hold AI providers responsible for illegal content disseminated through their systems, unlike the controversial Section 230 law that immunized social media platforms against lawsuits for illegal content on their sites. </p></li><li><p>Illinois passes a bill to <a href="https://www.dailyherald.com/news/20230603/legislation-that-would-allow-police-to-use-drones-for-monitoring-parades-large-events-passes" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">allow law enforcement to use drones</a> for monitoring large public events, as long as the drones do not use weapons or facial recognition technology. </p></li><li><p>Senator Chuck Schumer provides an update on a bipartisan <a href="https://www.csis.org/events/sen-chuck-schumer-launches-safe-innovation-ai-age-csis" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">push for legislation</a> on AI. </p></li><li><p>The UK has <a href="https://www.thetimes.co.uk/article/df2d2550-0e1b-11ee-9d84-6e8ed24abaa3" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">ousted</a> tech advisors that failed to foresee the importance of large language models and other recent AI developments, and has appointed <a href="https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Ian Hogarth</a> as the Chair of their AI Foundation Model Taskforce. The taskforce is <a href="https://docs.google.com/forms/d/e/1FAIpQLSd3OODvb-Nj1rhDQgx_-NSmAPk0348m13TlZr1BZy80AHzSrQ/viewform" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">seeking advisors</a>. </p></li></ul><p>Second, news about AI models. </p><ul><li><p>Inflection AI, a startup founded only a year ago, released a <a href="https://inflection.ai/inflection-1" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new language model</a> with performance competitive with ChatGPT-3.5.</p></li><li><p>The <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">world’s best</a> open source language model was built by a team in Abu Dhabi. Apparently, the model tends to <a href="https://twitter.com/jankulveit/status/1670735364707721216" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">speak positively about the Abu Dhabi government</a>.</p></li><li><p><a href="https://twitter.com/soumithchintala/status/1671267150101721090" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Rumor has it</a> that GPT-4 is a Mixture of Experts (MoE) model that synthesizes the outputs of eight language models, each individually larger than GPT-3. </p></li></ul><p>Finally, a few articles relevant to AI safety research.  </p><ul><li><p>GPT-4 can <a href="https://arxiv.org/abs/2304.05376" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">synthesize chemicals</a> by writing instructions for using lab equipment. </p></li><li><p>Training AIs on the outputs of other AIs has sharp drawbacks, finds a <a href="https://arxiv.org/abs/2305.17493v2?utm_source=substack&amp;utm_medium=email" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new paper</a>. </p></li><li><p>Political messaging is 3x more likely to change someone’s views when targeted towards a specific individual, finds a <a href="https://www.pnas.org/eprint/AWU8WZ9GVGPPGP9FJT3D/full" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new study</a>. AI systems could potentially <a href="https://arxiv.org/abs/2303.08721" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">increase the power of personalized persuasion</a>.</p></li><li><p>Leading AI models often do not comply with the EU AI Act’s requirements, finds new <a href="https://crfm.stanford.edu/2023/06/15/eu-ai-act.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">research</a> from Stanford University. </p></li><li><p>AI developers will need to prioritize safety in order to avoid accidents, misuse, or loss of control of their AI systems. A new <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4491421" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">article</a> outlines the challenge and paths forward. </p></li><li><p>The journal Risk Analysis is doing a <a href="https://onlinelibrary.wiley.com/pb-assets/assets/15396924/CallforPapersRiskAnalysisArtificialIntelligence3-1684769977.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">special issue on AI risk</a>. Submissions are due on December 15, 2023. </p></li><li><p>Yoshua Bengio writes an <a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">FAQ on catastrophic AI risks</a>. </p></li></ul><p>See also: <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS website</a>, <a href="https://twitter.com/ai_risks?lang=en" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS twitter</a>, <a href="https://newsletter.mlsafety.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">A technical safety research newsletter</a>, and <a href="https://arxiv.org/abs/2306.12001" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">An Overview of Catastrophic AI Risks</a></p><p><a href="https://newsletter.safe.ai/p/ai-safety-newsletter-12?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Share</a></p>