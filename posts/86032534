<p><strong>I.</strong></p><p>Last month, Ben West of the Center for Effective Altruism <strong><a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">hosted a debate</a></strong> among long-termists, forecasters, and x-risk activists about pausing AI.</p><p>Everyone involved thought AI was dangerous and might even destroy the world, so you might expect a pause - maybe even a full stop - would be a no-brainer. It wasn’t. Participants couldn’t agree on basics of what they meant by “pause”, whether it was possible, or whether it would make things better or worse.</p><p>There was at least some agreement on what a successful pause would have to entail. Participating governments would ban “frontier AI models”, for example models using more training compute than GPT-4. Smaller models, or novel uses of new models would be fine, or else face an FDA-like regulatory agency. States would enforce the ban against domestic companies by monitoring high-performance microchips; they would enforce it against non-participating governments by banning export of such chips, plus the usual diplomatic levers for enforcing treaties (eg nuclear nonproliferation). </p><p>The main disagreements were:</p><ol><li><p>Could such a pause possibly work?</p></li><li><p>If yes, would it be good or bad?</p></li><li><p>If good, when should we implement it? When should we lift it?</p></li></ol><p>I’ve grouped opinions into five categories:</p><p><strong>Simple Pause: </strong>What if we just asked AI companies to pause for six months? Or maybe some longer amount of time?</p><p>This was the request in the <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">FLI Pause Giant AI Experiments open letter</a>, signed by thousands of AI scientists, businesspeople, and thought leaders, including many participants in this debate. So you might think the debate organizers could find one person to argue for it. They couldn’t. The letter was such a watered-down compromise that nobody really supported it, even though everyone signed it to express support for one or another of the positions it compromised between.</p><p>Why don’t people want this? First, most people think it will take the AI companies more than six months of preliminary work before they start training their next big model anyway, so it’s useless. Second, even if we do it, six months from now the pause will end, and then we’re more or less where we are right now. Except worse, for two reasons:</p><ol><li><p>COMPUTE OVERHANG. We expect AI technology to advance over time for two reasons. First, <em>algorithmic progress</em> - people learn how to make AIs in cleverer ways. Second, <em>hardware progress</em> - Moore’s Law produces faster, cheaper computers, so for a given budget, we can train/run the AI on more powerful hardware. A pause might slow algorithmic progress very slightly, with fewer big AIs to test new algorithms on. But it wouldn’t slow hardware progress at all. At the end of the pause, hardware would have progressed some amount, and instead of AIs progressing gradually over the next six months, they would progress in one giant jump when the pause ended, and all the companies rushed to build new AIs that took advantage of the past six months of progress. But gradual progress (which allows iteration and debugging in relatively simple AIs) seems safer than sudden progress (where all at once we have an AI much more powerful than anything we’ve ever seen before). Since a pause like this simply replaces gradual progress with sudden progress, it would be counterproductive.</p></li><li><p>BURNING TIMELINE IN A RACE. Suppose that we prefer America get strong AIs before China. If America pauses but China doesn’t, then after the pause we’d be exactly where we were before, except that China would have caught up relative to America. More generally, companies that care most about AI safety are most likely to obey the pause. So unless we’re very good at enforcing the pause even on non-cooperators, this just hurts the companies that care about safety the most, for no gain.</p></li></ol><p>These are counterbalanced by one benefit:</p><ol><li><p>MORE TIME FOR ALIGNMENT. Maybe we can use those six months to learn more about how to control AIs, or to prepare for them socially/politically.</p></li></ol><p>This benefit is real, but this kind of pause doesn’t optimize it. Technical alignment research benefits from advanced models to experiment on; the Surgical Pause strategy takes this consideration more seriously. And social/political preparation depends on some kind of plan: this is what the Regulatory Pause strategy adds.</p><p><strong>Surgical Pause: </strong>The Surgical Pause tweaks the Simple Pause to add two extra considerations:</p><ol><li><p>WHEN TO PAUSE. If we’re going to pause for six months, which six months should it be? Right now? A few years from now? Just before dangerous AI is invented? The main benefit to a pause is to give alignment research time to catch up. But alignment research works better when researchers have more advanced AIs to experiment on. So probably we should have the six month pause right before dangerous AI is invented.</p></li><li><p>HOW LONG TO PAUSE. The biggest disadvantage of pausing for a long time is that it gives bad actors (eg China)<a id="footnote-anchor-1" href="#footnote-1">1</a> a chance to catch up. Suppose the West is right on the verge of creating dangerous AI, and China is two years away. It seems like the right length of pause is 1.9999 years, so that we get the benefit of maximum extra alignment research and social prep time, but the West still beats China.</p></li></ol><p>Obviously the problem with the Surgical Pause is that we might not know when we’re on the verge of dangerous AI, and we might not know how much of a lead “the good guys” have. Surgical Pause proponents suggest being very conservative with both free variables. This is less of a well-thought-out plan and more saying “come on guys, let’s at least <em>try </em>to be strategic here”. At the limit, it suggests we probably shouldn’t pause for six months, starting right now.</p><p>Since this involves leading labs burning their lead time for safety, in theory it could be done unilaterally by the single leading lab, without international, governmental, or even inter-lab coordination. But you could buy more time if you got those things too. Some leading labs have promised to do this when the time is right - for example <a href="https://openai.com/blog/planning-for-agi-and-beyond" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">OpenAI</a> and (a previous iteration of) <a href="https://www.lesswrong.com/posts/SbAgRYo8tkHwhd9Qx/deepmind-the-podcast-excerpts-on-agi#_Avengers_assembled__for_AI_Safety__Pause_AI_development_to_prove_things_mathematically" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">DeepMind</a> - with varying levels of believability. </p><p>AnonResearcherAtMajorAILab discussed some of the strategy here in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/BFbsqwCuuqueFRfpW" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Aim For Conditional AI Pauses</a>, and <a href="https://www.lesswrong.com/posts/YkwiBmHE3ss7FNe35/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this Less Wrong post</a> is also very good.</p><p><strong>Regulatory Pause: </strong>If one benefit of the Simple Pause is to use the time to prepare for AI socially and politically, maybe we should just pause until we’ve completed social and political preparations.</p><p>David Manheim suggests a monitoring agency like the FDA. It would “fast-track” small AIs and trivial re-applications of existing AIs, but carefully monitor new “frontier models” for signs of danger. Regulators might <a href="https://evals.alignment.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">look for dangerous capabilities</a> by asking AIs to hack computers or spread copies of themselves, or test whether they’ve been programmed against bias/misinformation/etc. We could pause only until we’ve set up the regulatory agency, and take hostile actions (like restrict chip exports) only to other countries that don’t cooperate with our regulators or set up domestic regulators of their own.</p><p>Many people in tech are regulation-skeptical libertarians, but proponents point out that regulation fails in a predictable direction: it usually <em>does </em>successfully prevent bad things, it just also prevents good things too. Since the creation of the Nuclear Regulatory Commission in 1975, there has never been a major nuclear accident in the US. And sure, this is because the NRC prevented any nuclear plants from being built in the United States at all from 1975 to 2023 (one was <a href="https://abcnews.go.com/US/wireStory/american-nuclear-reactor-built-scratch-decades-enters-commercial-101861665" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">finally built</a> in July). Still, they <em>technically</em> achieved their mandate. Likewise, most medications in the US are safe and relatively effective, at the cost of an FDA approval process being so expensive that we only get a tiny trickle of new medications each year and hundreds of thousands of people die from unnecessary delays. But medications <em>are</em> safe and effective. Or: San Francisco housing regulators almost never approve new housing, so housing costs millions of dollars and thousands of San Franciscans are homeless - but certainly there’s no epidemic of bad houses getting approved and then ruining someone’s view or something. If we extrapolate this track record to AI, AI regulators will be overcautious, progress will slow by orders of magnitude or stop completely - but AIs will be safe.</p><p>This is a depressing prospect if you think the problems from advanced AI would be limited to more spam or something. But if you worry about AI destroying the world, maybe you should accept a San-Francisco-housing-level of impediment and frustration.</p><p>A regulatory pause could be better than a total stop if you think it will be more stable (lots of industries stay heavily regulated forever, and only a few libertarians complain), or if you think <em>maybe</em> the regulator will occasionally let a tiny amount of safe AI progress happen.</p><p>But it could be worse than a total stop if you expect continued progress will eventually produce unsafe AIs regardless of regulation. You might expect this if you’re worried about deceptive alignment, eg superintelligent AIs that deliberately trick regulators into thinking they’re safe. Or you might think AIs will eventually be so powerful that they can endanger humanity from a walled-off test environment even before official approval. The classic Bostrom/Yudkowsky model of alignment implies both of these things.</p><p>David Manheim and Thomas Larsen set out their preferred versions of this strategy in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/3hSEQnEN2D3SSzHWn" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">What’s In A Pause?</a> and <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/DG6bf5YW3jxLRD7KN" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Policy Ideas For Mitigating AI Risk</a>.</p><p><strong>Total Stop:</strong> If you expect AIs to exhibit deceptive alignment capable of fooling regulators, or to be so dangerous that even testing them on a regulator’s computer could be apocalyptic, maybe the only option is a total stop.</p><p>It’s tough to imagine a total stop that works for more than a few years. You have at least three problems:</p><ol><li><p>NON-PARTICIPANTS. As with any pause proposal, unfriendly countries (eg China) can keep working on AI. You can refuse to export chips to them, which will slow them down a little, but their own chips will eventually be up to the task. You will either need a diplomatic miracle, or willingness to resort to less diplomatic forms of coercion. This doesn’t have to be immediate war: Israel has come up with “creative” ways to slow Iran’s nuclear program, and countries trying to frustrate China’s chip industry could do the same. But great powers playing these kinds of games against each other risks wider conflict. </p></li><li><p>ALGORITHMIC PROGRESS. Suppose the government banned anyone except heavily-regulated companies from having a computer bigger than a laptop. Right now you can’t train a good AI on a laptop, or even a cluster of laptops. But AI training methods get more efficient every year. If current research progress continues, then at some point - even if it’s decades from now - you <em>will</em> be able to train cutting-edge AIs on laptops.</p></li><li><p>HARDWARE PROGRESS. Also the laptops keep getting better, because of Moore’s Law.</p></li></ol><p>Regulators can plausibly control the flow of supercomputers, at least domestically. But eventually technology will advance to the point where you can train an AI on anything. Then you either have to ban all computing, restrict it at gradually more extreme levels (1990 MS-DOS machines! No, punch cards!) or accept that AI is going to happen.</p><p>Still, you can imagine this buying us a few decades. Rob Bensinger defended this view in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/fSeDA7B7Hve5LeaWq" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Comments On Manheim’s “What’s In A Pause?”</a>, and it’s the backdrop to Holly Elmore’s <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/Y4SaFM5LfsZzbnymu" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Case For AI Advocacy To The Public</a><a id="footnote-anchor-2" href="#footnote-2">2</a>. </p><p><strong>No Pause: </strong>Or we could not do any of that.</p><p>If we think alignment research is going well, and that a pause would mess it up, or cause a compute overhang leading to un-research-able fast takeoff, or cede the lead to China, maybe we should stick with the current rate of progress. </p><p>Nora Belrose made this argument in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI Pause Will Likely Backfire</a>. Specifically:</p><blockquote><p>[A pause] would have several predictable negative effects:</p><ol><li><p>Illegal AI labs develop inside pause countries, remotely using training hardware outsourced to non-pause countries to evade detection. Illegal labs would presumably put much less emphasis on safety than legal ones.</p></li><li><p>There is a brain drain of the least safety-conscious AI researchers to labs headquartered in non-pause countries. Because of remote work, they wouldn’t necessarily need to leave the comfort of their Western home.</p></li><li><p>Non-pause governments make opportunistic moves to encourage AI investment and R&amp;D, in an attempt to leap ahead of pause countries while they have a chance. Again, these countries would be less safety-conscious than pause countries.</p></li><li><p>Safety research becomes subject to government approval to assess its potential capabilities externalities. This slows down progress in safety substantially, just as the FDA slows down medical research.</p></li><li><p>Legal labs exploit loopholes in the definition of a “frontier” model. Many projects are allowed on a technicality; e.g. they have fewer parameters than GPT-4, but use them more efficiently. This distorts the research landscape in hard-to-predict ways.</p></li><li><p>It becomes harder and harder to enforce the pause as time passes, since training hardware is increasingly cheap and miniaturized.</p></li><li><p>Whether, when, and how to lift the pause becomes a highly politicized culture war issue, almost totally divorced from the actual state of safety research. The public does not understand the key arguments on either side.</p></li><li><p>Relations between pause and non-pause countries are generally hostile. If domestic support for the pause is strong, there will be a temptation to wage war against non-pause countries before their research advances too far. “If intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.” — <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Eliezer Yudkowsky</a></p></li><li><p>There is intense conflict <em>among</em> pause countries about when the pause should be lifted, which may also lead to violent conflict.</p></li><li><p>AI progress in non-pause countries sets a deadline after which the pause <em>must</em> end, if it is to have its desired effect.<a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#fngpfbywtblcj" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><sup>[8]</sup></a> As non-pause countries start to catch up, political pressure mounts to lift the pause as soon as possible. This makes it hard to lift the pause gradually, increasing the risk of dangerous fast takeoff scenarios.</p></li></ol></blockquote><p></p><p>Along with this overall arc, the debate included a few other points:</p><p><strong>Holly Elmore</strong> argued in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/Y4SaFM5LfsZzbnymu" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">The Case For AI Advocacy To The Public</a> that pro-pause activists should be more willing to take their case to the public. EA has a long history of trying to work with companies and regulators, and has been less confident in its ability to execute protests, ads, and campaigns. But in most Western countries, the public hates AI and wants to stop it. If you also want to stop it, the democratic system provides fertile soil. Holly is putting her money where her mouth is and <a href="https://insidebigdata.com/2023/09/25/protestors-to-meta-ai-sharing-model-weights-is-fundamentally-unsafe/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">leading anti-AI protests at the Meta office in San Francisco</a>; the first one was last month, but there might be more later.</p><p><strong>Matthew Barnett</strong> said in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">The Possibility Of An Indefinite AI Pause</a> that it might be hard to control the length of a pause once started, and might drag on longer than people who expected a well-planned surgical pause might like. He points to supposedly temporary moratoria that later became permanent (eg aboveground nuclear test ban, various bans on genetic engineering) and regulatory agencies that became so strict they caused the subject of their regulation to essentially cease to happen (eg nuclear plant construction for several decades). Such an indefinite pause would either collapse in a disastrous actualization of compute overhang, or require increasingly draconian international pressure to sustain. He thinks of this as a strong argument against most forms of pause, although he is willing to consider a “licensing” system that looks sort of like regulation.</p><p><strong>Quintin Pope</strong> said in <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/zd5inbT4kYKivincm" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI Is Centralizing By Default, Let’s Not Make It Worse</a> that the biggest threat from AI is centralizing power, either to dictators or corporations. AIs are potentially more loyal flunkies than humans, and let people convert power (including political power and money) into intelligence more efficiently than the usual methods. His interest is mostly in limiting the damage, putting him skew to most of the other people in this debate. He would support regulation that makes it easier for small labs to catch up to big ones, or that limits the power-centralizing uses of AI, but oppose regulation focused on centralizing AI power into a few big, supposedly-safer corporations.</p><p></p><p><strong>II.</strong></p><p>For a “debate”, this lacked much inter-participant engagement. Most people posted their manifesto and went home.</p><p>The exception was the comments section of Nora’s post, <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI Pause Will Likely Backfire</a>. As usual, a lot of the discussion was just clarifying what everyone was fighting about, but there were also a few real fights:</p><ul><li><p>Gerald Monroe <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=FGSp5PBjm6o9rcY2g" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">thought </a>that the history of nuclear weapons suggested pauses like this were impossible (because many countries did build nuclear weapons). David Manheim thought it suggested pauses like this could work (because there were some successful arms limitation treaties, and less nuclear proliferation than would have happened without international cooperation). Manheim also brought up the successful bans on ozone-destroying CFCs and on human cloning.</p></li><li><p>Nora thought most treaties like this fail, and a successful one would have to involve some level of global tyranny. David Manheim <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=KRw7Q3rM3R6vg83Kc" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">thought </a>most treaties sort of do some good, even if they don’t accomplish exactly what they wanted, and none of them so far have led to global tyranny. Cf. <a href="https://www.astralcodexten.com/p/your-book-review-the-internationalists" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the Kellogg-Briand Pact</a> for an example of a treaty that didn’t succeed perfectly but was probably net good.</p></li><li><p>Nora thought it was important to give alignment researchers advanced models to experiment with, because the sort of armchair alignment research before interesting AIs existed (eg Bostrom’s <em>Superintelligence) </em>wasn’t just wrong, but fostered dead-end worse-than-nothing paradigms that continue to confuse the field. Daniel Filan <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=FeYj7vCfMpn5hxFpD" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">objected</a> that Bostrom got some things right and even described something like the direction that modern alignment research is taking. There was a long argument about this, which I think reduces to “Bostrom said some useful theoretical things, speculated about practical direction, and a few of his speculations were right but most now seem outdated”.</p></li><li><p>Zach Stein-Perlman <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=HMipMShBr6ymoNzcd" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">made some good points</a> about the technical factors that made pauses better vs. worse, which I’ve tried to fold into the Surgical Pause section above.</p></li><li><p>Nora thought that success at making language models behave (eg refuse to say racist things even when asked) suggests alignment is going pretty well so far. Many other people (eg <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=5aE2ZTt2RbADFJpq3" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Rafael Harth</a>, <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=kqSzGBBr4TCu5ztbq" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Steven Byrnes</a>) suggested this would produce deceptive alignment, ie AI that says nice things to humans who have power over it, but secretly has different goals, and so success in this area says nothing about true alignment success and is even kind of worrying. The question remained unresolved.</p></li></ul><p>In <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/fwdjMtJLpkyJ2Gice" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">How Could A Moratorium Fail?</a>, David Manheim discussed his own takeaways from the debate:</p><blockquote><p>My biggest surprise was how misleading the terms being used were, and think that many opponents were opposed to something different than what supporters were interested in suggesting. Even some supporters  Second, I was very surprised to find opposition to the claim that AI might not be safe, and could pose serious future risks, largely because the systems would be aligned by default - i.e. without any enforced mechanisms for safety. I also found out that there was a non-trivial group that wants to roll back AI progress to before GPT-4 for safety reasons, as opposed to job displacement and copyright reasons.  I <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=uaotaqDdgX9tDWh53" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">was convinced by Gerald Monroe</a> that getting a full moratorium was harder than I have previously argued based on an analogy to nuclear weapons. (I was not convinced that it “isn&#39;t going to happen without a series of extremely improbable events happening simultaneously” - largely because I think that countries will be motivated to preserve the status quo.) I am mostly convinced by Matthew Barnett’s claim that <a href="https://forum.effectivealtruism.org/posts/k6K3iktCLCTHRMJsY/the-possibility-of-an-indefinite-ai-pause#The_possibility_of_an_indefinite_pause" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">advanced AI could be delayed by a decade, if restrictions are put in place</a> - I was less optimistic, or what he would claim is pessimistic. As explained above, I was very much not convinced that a policy which was agreed to be irrelevant would remain in place indefinitely. I also didn’t think that there’s any reason to expect a naive pause for a fixed period, but he convinced me that this is more plausible than I had previously thought - and I agree with him, and disagree with Rob Bensinger, about how bad this might be. Lastly, I have been <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire?commentId=ekvpGt53R2DupGniT" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">convinced by Nora</a> that the vast majority of the differences in positions is predictive, rather than about values. Those optimistic about alignment are against pausing, and in most cases, I think those pessimistic about alignment are open to evidence that specific systems are safe. This is greatly heartening, because I think that over time, we’ll continue to see evidence in one direction or another about what is likely, and if we can stay in a scout-mindset, we will (eventually) agree on the path forward.</p></blockquote><p><strong>III.</strong></p><p>Some added thoughts of my own:</p><p><strong>First,</strong> I think it’s silly to worry about world dictatorships here. The failure mode for global treaties is that the treaty doesn’t get signed or doesn’t work. Consider the various global warming treaties (eg Kyoto) or the United Nations. Even though many ordinary people (ie non-x-risk believers) dislike AI enough to agree to a ban, they’re not going to support it when it starts interfering with their laptops or gaming rigs, let alone if it requires ceding national sovereignty to the UN or something.</p><p><strong>Second</strong>, if we never get AI, I expect the future to be short and grim. Most likely we kill ourselves with synthetic biology. If not, some combination of <a href="https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">technological</a> and <a href="https://slatestarcodex.com/2017/02/09/considerations-on-cost-disease/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">economic </a>stagnation, rising <a href="https://www.astralcodexten.com/p/bad-definitions-of-democracy-and" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">totalitarianism</a> + <a href="https://www.astralcodexten.com/p/theses-on-the-current-moment" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">illiberalism</a> <a href="https://www.theintrinsicperspective.com/p/the-gossip-trap" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">+ </a><a href="https://slatestarcodex.com/2019/06/07/addendum-to-enormous-nutshell-competing-selectors/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">mobocracy</a>, <a href="https://www.astralcodexten.com/p/slightly-against-underpopulation" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">fertility collapse and dysgenics</a> will impoverish the world and accelerate its decaying institutional quality. I don’t spend much time worrying about any of these, because I think they’ll take a few generations to reach crisis level, and I expect technology to flip the gameboard well before then. But if we ban all gameboard-flipping technologies (the only other one I know is genetic enhancement, which is even more bannable), then we <em>do</em> end up with bioweapon catastrophe or social collapse. I’ve said before I think there’s a <a href="https://www.astralcodexten.com/p/the-extinction-tournament" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">~20%</a> chance of AI destroying the world. But if we don’t get AI, I think there’s a 50%+ chance in the next 100 years we end up dead or careening towards Venezuela. That doesn’t mean I have to support AI accelerationism because 20% is smaller than 50%. Short, carefully-tailored pauses could improve the chance of AI going well by a lot, without increasing the risk of social collapse too much. But it’s something on my mind.</p><p><strong>Third</strong>, most participants agree that a pause would necessarily be temporary. There’s no easy way to enforce it once technology gets so good that you can train an AI on your laptop, and (absent much wider adoption of x-risk arguments) government’s won’t have the stomach for hard ways. The <a href="https://takeoffspeeds.com/playground.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">singularity prediction widget</a> currently predicts 2040. If I make drastic changes to starve everybody of computational resources, the furthest I can push it back is 2070. This somewhat reassures me about my concerns above, but not completely. Matthew Barnett <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">talks about whether a temporary pause could become permanent</a>, and concludes probably not without a global police state. But I think people 100 years ago would be surprised that the state of California has managed to effectively ban building houses. I think if some anti-house radical had proposed this 100 years ago, people would have told her that would be impossible without a hypercompetent police state<a id="footnote-anchor-3" href="#footnote-3">3</a>.</p><p><strong>Fourth, </strong>there are many arguments that a pause would be impossible, but they mostly don’t argue against <em>trying</em>. We could start negotiating an international AI pause treaty, and only sign it if enough other countries agree that we don’t expect to be unilaterally-handicapping ourselves. So “China will never agree!” isn’t itself an argument against beginning diplomacy, unless you expect that just starting the negotiations would cause irresistible political momentum toward signing even if the end treaty was rigged against us.</p><p><strong>Fifth, </strong>a lot hinges on whether alignment research would be easier with better models. I’ve only talked to a handful of alignment researchers about this, but they say they still have their hands full with GPT-4. I would like to see broader surveys about this (probably someone has done these, I just don’t know where).</p><p>I find myself willing to consider trying a Regulatory or Surgical Pause - a strong one if proponents can secure multilateral cooperation, otherwise a weaker one calculated not to put us behind hostile countries (this might not be as hard as it sounds; so far China has just copied US advances; it remains to be seen if they can do cutting-edge research). I don’t entirely trust the government to handle this correctly, but I’m willing to see what they come up with before rejecting it.</p><p>Thanks to Ben and everyone who participated. You can find all posts, including some unofficial late posts I didn’t cover, <a href="https://forum.effectivealtruism.org/topics/ai-pause-debate" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>.</p><p></p><a id="footnote-1" href="#footnote-anchor-1">1</a><p>Zach writes in an email: “Much/most of my concern about China isn&#39;t <em>China has worse values than US</em> or even <em>Chinese labs are less safe than Western labs</em> but rather <em>it&#39;s better for leading labs to be friendly with each other (mostly to better coordinate and avoid racing near the end), so (a) it&#39;s better for there to be fewer leading labs and (b) given that there will be Western leading labs it&#39;s better for all leading labs to be in the West, and ideally in the US</em> […] <br/>In addition to a pause causing e.g. China to catch up (with the above downsides), there&#39;s the risk that the US realizes that China is catching up and then ends the pause. (To some extent this is just a limitation of the pause, but it&#39;s actual-downside-risk-y if you were hoping that your &#39;pause&#39; would last through AGI/whatever—with the final progress contributed by algorithmic progress or limited permitted compute scaling, so that labs never have an opportunity to exploit the compute overhang—but now your pause ends prematurely and the compute overhang is exploited.)”</p><a id="footnote-2" href="#footnote-anchor-2">2</a><p>Holly writes in an email: “I also think [you’re] taking the distinction between a mere pause and a regulatory pause too much from the opponents. The people who are out asking for a pause (like me and PauseAI) mostly want a long pause in which alignment research could either work, effective regulations could be put in place, or during which we don’t die if alignment isn’t going to be possible.I suppose I didn’t get into that in my entry but I would Iike to see [you] engage with the possibility that alignment doesn’t happen, especially since [you] seem to think civilization will decline for one reason or another without AI in the future. I think the assumption of [this] piece was too much AI development as the default. “</p><a id="footnote-3" href="#footnote-anchor-3">3</a><p>Matthew responds in an email: “I&#39;d like to point out that the modern practice of restricting housing can be traced back to 1926 when the Supreme Court ruled that enforcing land-use regulation and zoning policy was a valid exercise of a state&#39;s police power. The idea that we could effectively ban housing would not have been inconceivable to people 100 years ago, and indeed many people (including the plaintiffs in the case) were worried about this type of outcome.I don&#39;t think people back then would have said that zoning would require a hypercompetent police state. It&#39;s more likely that they would say that zoning requires an intrusive expansion of government powers. I think they would have been correct in this assessment, and we got the expansion that they worried about.Unlike banning housing, banning AI requires that we can&#39;t have any exceptions. It&#39;s not enough to ban AI in the United States if AI can trained in Switzerland. This makes the proposal for an indefinite pause different from previous regulatory expansions, and in my opinion much more radical.To the extent you think that such crazy proposals simply aren&#39;t feasible, then you likely agree with me that we shouldn&#39;t push for an indefinite pause. That said, you also predicted that if current trends continued, &#34;rising totalitarianism + illiberalism + mobocracy, fertility collapse and dysgenics will impoverish the world and accelerate its decaying institutional quality&#34;. This prediction doesn&#39;t seem significantly less crazy to me than the prediction that governments around the will attempt to ban AI globally (sloppily, and with severe negative consequences). I don&#39;t think it makes much sense to take one of these possibilities seriously and dismiss the other.”</p><p>My answer: I think there’s a difference between the regulatory framework for something existing vs. expecting it. It’s constitutional and legal for the US to raise the middle-class tax rate to 99%, but most people would still be surprised if it happened. I’m surprised how easy it is for governments to effectively ban things without even trying just by making them annoying. Could this create an AI pause that lasts decades? My Inside View answer is no; my Outside View answer has to be “maybe”. Maybe they could make hardware progress and algorithmic progress so slow that AI never quite reaches the laptop level before civilization loses its ability to do technological advance entirely? Even though this would be a surprising world, I have more probability on something like this than on a global police state. Possible exception if AI does something crazy (eg launches nukes) that makes all world governments over-react and shift towards the police state side, but at that point we’re not discussing policies in the main timeline anymore.</p>