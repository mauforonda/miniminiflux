Published on October 7, 2023 11:30 PM GMT<br/><br/><p>Readers may have noticed many similarities between Anthropic&#39;s recent publication <a href="https://transformer-circuits.pub/2023/monosemantic-features/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a> (<a href="https://www.lesswrong.com/posts/TDqvQFks6TWutJEKu/towards-monosemanticity-decomposing-language-models-with" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">LW post</a>) and my team&#39;s recent publication <a href="https://arxiv.org/abs/2309.08600" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Sparse Autoencoders Find Highly Interpretable Directions in Language Models</a> (<a href="https://www.lesswrong.com/posts/Qryk6FqjtZk9FHHJR/sparse-autoencoders-find-highly-interpretable-directions-in" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">LW post</a>). Here I want to compare our techniques and highlight what we did similarly or differently. My hope in writing this is to help readers understand the similarities and differences, and perhaps to lay the groundwork for a future synthesis approach. </p><p>First, let me note that we arrived at similar techniques in similar ways: both Anthropic and my team follow the lead of Lee Sharkey, Dan Braun, and beren&#39;s <a href="https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">[Interim research report] Taking features out of superposition with sparse autoencoders</a>, though I don&#39;t know how directly Anthropic was inspired by that post. I believe both our teams were pleasantly surprised to find out the other one was working on similar lines, serving as a form of replication.</p><p>Some disclaimers: This list may be incomplete. I didn&#39;t give Anthropic a chance to give feedback on this, so I may have misrepresented some of their work, including by omission. Any mistakes are my own fault.</p><h2>Target of Dictionary Learning/Sparse Autoencoding</h2><p>A primary difference is that we looked for language model features in different parts of the model. <strong>My team trained our sparse autoencoder on the residual stream of a language model, whereas Anthropic trained on the activations in the MLP layer.</strong> </p><p>These objects have some fundamental differences. For instance, the residual stream is (<a href="https://transformer-circuits.pub/2023/privileged-basis/index.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">potentially</a>) <a href="https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">almost completely linear</a> whereas the MLP activations have just gotten activated, so their values will be positive-skewed. However, it&#39;s encouraging that this technique seems to work on both the MLP layer and residual stream. Additionally, my coauthor Logan Riggs successfully applied it to the output of the attention sublayer, so both in theory and in practice the dictionary learning approach seems to work well on each part of a language model.</p><h2>Language Model Used</h2><p>Another set of differences comes from which language model our teams used to train the autoencoders. <strong>My team used Pythia-70M and Pythia-410M</strong>, whereas <strong>Anthropic&#39;s language model was custom-trained for this study </strong>(I think). Some differences in the language model architectures:</p><ul><li><strong>Layers: </strong>The Pythia models have <strong>6 or 24</strong> <strong>layers</strong>. The Anthropic language model had just <strong>1 layer</strong>.</li><li><strong>Residual Stream Dimension: </strong>The Pythia models have <strong>512- and 1024-dimensional residual streams</strong>. The Anthropic model has a <strong>128-dimensional residual stream</strong> (but recall that they study their MLP layer, which has dimensionality <strong>512</strong>). </li><li><strong>Parameters: </strong>As implied by the names, the Pythia models have <strong>70M and 410M parameters</strong>. Anthropic does not specify the number of parameters in the language model, but from the information they give we can estimate their model had <strong>~13M parameters</strong> (see calculations <a href="https://docs.google.com/spreadsheets/d/1saCqZbmlY6F69qIxHjxXbxNYbbwn7EibOs8kWi3W4AE/edit#gid=899002403" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a> and explanation of my estimation method <a href="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>). This estimate largely depends on assuming 1) they use untied embedding/unembedding weights and 2) they used a vocabulary size of ~50K (similar to Pythia and GPT-2/3), but this number isn&#39;t actually very important, because the more relevant statistic is...</li><li><strong>Non-Embedding Parameters</strong>: For language models of this size, most of the parameters are used for the word embedding/unembedding matrix, so for scaling laws its more important to measure non-embedding parameters. In particular, the <strong>Pythia models have only 19M and 300M non-embedding parameters</strong>, and for <strong>Anthropic&#39;s language model has only ~200K non-embedding parameters</strong>.</li><li><strong>Training Data: </strong>The Pythia models were trained on 300B tokens from The Pile, and Anthropic&#39;s was trained on 100B tokens also from The Pile. These ar<strong>e both significantly overtrained</strong> for this number of parameters <a href="https://browse.arxiv.org/pdf/2001.08361v1.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">per the scaling laws</a>. </li><li><strong>Parallelization: </strong>The Pythia models apply their attention and MLP <strong>sublayers in parallel</strong> at the same time. Anthropic&#39;s model follows the more conventional approach of applying the <strong>attention sublayer before the MLP sublayer</strong>.</li></ul><h2>Sparse Autoencoder Architecture</h2><p>Similarities:</p><ul><li>Both teams used an autoencoder with <strong>loss = (reconstruction loss)+(sparsity loss)</strong>. </li><li>Both teams used a <strong>single hidden layer which was wider </strong>than the input/output layers.</li><li>Both teams used <strong>ReLU activations on the hidden layer</strong>.</li></ul><figure><img src="https://reader.miniflux.app/proxy/v81aRT4Hf9Cr9GNKaLOgTB3IgQ_uOQN6bHU4VTdexVI=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9zNzdjanBqY3p5cjI2ZmN5dGZmMw==" srcset="https://reader.miniflux.app/proxy/UuZhqkokP6ZWXhJ8GXVQpKZ_S0NxdvXdruum_KQlVkU=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9wdDVraGU0ZGRub3BjZGZxam1yaQ== 110w, https://reader.miniflux.app/proxy/kIdv_XV764Puc7WZEQ-WbVczwf3chvXGgaGzVLe9Q7U=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9vM2I5b291Z20ydWxzcW1mYzRnbQ== 220w, https://reader.miniflux.app/proxy/ur_ALlAbSA4aht3yi7K69cjpLww1N6tWgtKBwubkg3Y=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9mbjQ5bzd3eTViZ2hvZWZ1cHVjaw== 330w, https://reader.miniflux.app/proxy/nOzcnfwUWFnuvU02_xPBq41aDmrl6ptTCRFLuGU51l8=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9ubXd3bWlvN21rcmpmdTBlN2t5cA== 440w, https://reader.miniflux.app/proxy/vIZLjjyM0QvI3UruG0MpkcERHfkKafcS8KMBzm6S-Rg=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9udWRqdTZnZzd0dmFzaDVldzhuag== 550w, https://reader.miniflux.app/proxy/5U-1B-Ndlt6Ky-COrISscz06ESxbh-S1AKsvlyklScE=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9wcjhrOGJ3eHkxYW0xcGg3ejBpYQ== 660w, https://reader.miniflux.app/proxy/hKHP45l3dzN09LiOkH39UwNYA9Ed935yjIApFcYNUn0=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy91anpnajViY2h6eHpjZ3dsNnlpYw== 770w, https://reader.miniflux.app/proxy/bSmVhLvJPS7tGItx1Dtj6x1CsjYuNkTJ6NF06g-rsbk=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9pdnA4anV4dGF3ajhwampoMmtmbA== 880w, https://reader.miniflux.app/proxy/RGsb_AjoFvMWizYx-E2f_e4fdWzo91mgCzQ_WOU4QW0=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9jY2YwZmMwaG5sZjh5cXJmYm5rbw== 990w, https://reader.miniflux.app/proxy/uUGe7ABYm4HnMTyRUwKfcA_fVri_lT1saKZvTinkdPQ=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9GNGlvZ0s1eGROZDdqRE55dy9jbHFxZGtjbWRzZGVudmVieHJ5aQ== 1097w" loading="lazy"/><figcaption>The overview diagram of our approach from <a href="https://arxiv.org/abs/2309.08600" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">our paper</a>. The middle column shows the sparse autoencoder architecture.</figcaption></figure><p>But some significant differences remain:</p><ul><li><strong>We use a bias term on just the encoding step, Anthropic has a bias term on both their encoding and decoding steps.</strong></li><li><strong>We use tied embeddings</strong> between our encoder and decoder, <strong>Anthropic uses untied embeddings</strong>.</li></ul><p>In other words, we perform this calculation:</p><p>^x=WT∗ReLU(W∗x+b)</p><p>whereas Anthropic does this calculation:</p><p>^x=VT∗ReLU(W∗x+b)+c</p><p>more precisely, Anthropic writes their calculation in these terms:<br/>¯x=x−bdf=ReLU(We¯x+be)^x=Wdf+bd</p><p>which is equivalent to the above with b=−Webd+be etc.</p><h2>Sparse Autoencoder Training</h2><p>There are two main differences between how we trained our sparse autoencoders and how Anthropic trained theirs:</p><ul><li><strong>Size of training set:</strong> We trained our autoencoders for <strong>10M tokens</strong>. Anthropic trained theirs for much longer, <strong>8B tokens</strong>.</li><li><strong>Dead Neurons and Reinitialization:</strong> A perennial problem with ReLU-activated neural networks is the presence of a dead neuron, i.e. a neuron that never activations and therefore does not get trained. Both teams encountered dead neurons, but Anthropic did something to address it: <strong>Anthropic resampled their dead neurons at checkpoints during training, while we did not.</strong></li></ul><h2>Checking Success</h2><p>[Epistemic status warning: I&#39;m less sure I&#39;ve fully capture Anthropic&#39;s work in this section.] </p><p>Finally, how did we decide the features were interpretable?</p><ul><li>Both teams did &#34;manual inspection&#34;, i.e., one of the coauthors was able to better interpret the features.</li><li>Both teams used the OpenAI autointerpretability technique, albeit with different models (we used GPT-3.5, Anthropic naturally used Claude 2).</li><li>Both teams present individual features with data showing that tokens activating the feature follow a simple description. <a href="https://browse.arxiv.org/pdf/2309.08600.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Our paper&#39;s</a> Figure 4 (left) shows that we have a feature activating primarily apostrophes, while Anthropic&#39;s gorgeous <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#feature-arabic-activations" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">&#34;Feature Activation Distribution&#34;</a> graphic shows a feature activating primarily on Arabic text.</li><li>Both teams measure how model predictions change by editing along a feature. In particular, in our Figure 4 (right), we show that editing the model activations to &#34;turn off&#34; the apostrophe feature decreases the model prediction of the &#34;s&#34; token, while Anthropic shows in their <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#feature-arabic-effect" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">&#34;Feature Downstream Effects&#34;</a> section that turning off the feature decreases the chance of predicting future tokens in Arabic, while &#34;pinning&#34; it to the on state makes the model produce text in Arabic.</li></ul><p>Our team also performed these measures:</p><ul><li>Checking for human-understandable connections between features across layers of the language model (see our Figure 5).</li><li>Measuring how many features need to be patched to change the model&#39;s behavior on a particular task (see our Section 4).</li></ul><p>Anthropic also performed these measures:</p><ul><li>Confirmation that features are not neurons (in several places, including <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#feature-arabic-neuron" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>)</li><li>Checking for human-understandable connections between features across steps (token positions). For instance, in their <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-fsa" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">&#34;Finite State Automata&#34; section</a> they describe a feature activating on all-caps text and increasing predictions of underscores, and a second feature that activates on underscores and predicts all-caps text. These two features together aid the model in writing text in all-caps <a href="https://en.wikipedia.org/wiki/Snake_case" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">snake case</a>.</li><li>A second form of automatic interpretability, in which Claude is shown the feature description and then asked &#34;would this feature predict [token X] is likely to come next?&#34;</li></ul><p>Thanks to Logan and Aidan for feedback on an earlier draft of this post.</p><br/><br/><a href="https://www.alignmentforum.org/posts/F4iogK5xdNd7jDNyw/comparing-anthropic-s-dictionary-learning-to-ours#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>