Published on October 5, 2023 9:01 PM GMT<br/><br/><p><em>Text of post based on <a href="https://www.anthropic.com/index/decomposing-language-models-into-understandable-components" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">our blog post</a> as <a href="https://transformer-circuits.pub/2023/monosemantic-features/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a linkpost for the full paper</a> which is considerably longer and more detailed.</em></p>
<p>Neural networks are trained on data, not programmed to follow rules. We understand the math of the trained network exactly – each neuron in a neural network performs simple arithmetic – but we don&#39;t understand why those mathematical operations result in the behaviors we see. This makes it hard to diagnose failure modes, hard to know how to fix them, and hard to certify that a model is truly safe.</p>
<p>Luckily for those of us trying to understand artificial neural networks, we can simultaneously record the activation of every neuron in the network, intervene by silencing or stimulating them, and test the network&#39;s response to any possible input.</p>
<p>Unfortunately, it turns out that the individual neurons do not have consistent relationships to network behavior. For example, <a href="https://transformer-circuits.pub/2023/monosemantic-features/vis/a-neurons.html#feature-83" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a single neuron</a> in a small language model is active in many unrelated contexts, including: academic citations, English dialogue, HTTP requests, and Korean text. In a classic vision model, <a href="https://distill.pub/2017/feature-visualization/#diversity" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a single neuron</a> responds to faces of cats and fronts of cars. The activation of one neuron can mean different things in different contexts.</p>
<p><img src="https://reader.miniflux.app/proxy/tQl-3a8TzrtQrfKBn-CkCq6buqscJxaf4T6dmV7jPLY=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9URHF2UUZrczZUV3V0SkVLdS9qd2h6NWJla3RkZnBhaXlsdHd4bw==" alt="" loading="lazy"/></p>
<p>In our latest paper, <a href="https://transformer-circuits.pub/2023/monosemantic-features/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em></a>, we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics.</p>
<p>In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation.</p>
<p><img src="https://reader.miniflux.app/proxy/IOJA6iV2YT5voAY_RPQoswgtJp38thWzx_2yX_jQbss=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9URHF2UUZrczZUV3V0SkVLdS9ja2R4eWw4YTlzYjkydWZybWpxeg==" alt="" loading="lazy"/></p>
<p>To validate that the features we find are significantly more interpretable than the model&#39;s neurons, we have a blinded human evaluator score their interpretability. The features (red) have much higher scores than the neurons (teal).</p>
<p><img src="https://reader.miniflux.app/proxy/_yMx6c3JVHGLt72WAkRwFloyBXNOssQn8tJMnVgQkMk=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9URHF2UUZrczZUV3V0SkVLdS95cXo0YXA2enhhM2J6c3ZpcHdqYQ==" alt="" loading="lazy"/></p>
<p>We additionally take an &#34;autointerpretability&#34; approach by using a large language model to generate short descriptions of the small model&#39;s features, which we score based on another model&#39;s ability to predict a feature&#39;s activations based on that description. Again, the features score higher than the neurons, providing additional evidence that the activations of features and their downstream effects on model behavior have a consistent interpretation.</p>
<p>Features also offer a targeted way to steer models. As shown below, artificially activating a feature causes the model behavior to change in predictable ways.</p>
<p><img src="https://reader.miniflux.app/proxy/_pP3bn8X_xD0L_totlMNLN3toqFJ1Dhrm7occ4sp_oA=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9URHF2UUZrczZUV3V0SkVLdS9zaDg0ZnBpdDAzaW03aHllcGFwcg==" alt="" loading="lazy"/></p>
<p>Finally, we zoom out and look at the feature set as a whole. We find that the features that are learned are largely universal between different models, so the lessons learned by studying the features in one model may generalize to others. We also experiment with tuning the number of features we learn. We find this provides a &#34;knob&#34; for <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#phenomenology-feature-splitting" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">varying the resolution</a> at which we see the model: decomposing the model into a small set of features offers a coarse view that is easier to understand, and decomposing it into a large set of features offers a more refined view revealing subtle model properties.</p>
<p>This work is a result of Anthropic’s investment in Mechanistic Interpretability – one of our longest-term research bets on AI safety. Until now, the fact that individual neurons were uninterpretable presented a serious roadblock to a mechanistic understanding of language models. Decomposing groups of neurons into interpretable features has the potential to move past that roadblock. We hope this will eventually enable us to monitor and steer model behavior from the inside, improving the safety and reliability essential for enterprise and societal adoption.</p>
<p>Our next challenge is to scale this approach up from the small model we demonstrate success on to frontier models which are many times larger and substantially more complicated. <strong>For the first time, we feel that the next primary obstacle to interpreting large language models is engineering rather than science.</strong></p>
<br/><br/><a href="https://www.lesswrong.com/posts/TDqvQFks6TWutJEKu/towards-monosemanticity-decomposing-language-models-with#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>