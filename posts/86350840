Published on October 10, 2023 7:41 AM GMT<br/><br/><p>TL;DR: <a href="https://www.pibbss.ai/affiliate" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><strong>PIBBSS is hiring research affiliates!</strong></a><strong> </strong></p><p>PIBBSS is a research initiative facilitating work that draws on the parallels between intelligent behavior in natural and artificial systems, and leveraging these insights towards making AI systems safe, beneficial and aligned.</p><p>We want to support excellent researchers pursuing “PIBBSS-style” AI alignment research by providing them with tailored longer-term support: a full-time salary, a lively research community, operational support and more. The initial commitment is 6 months, with potential extensions to 12 or more. </p><p><strong>To apply, click </strong><a href="https://docs.google.com/forms/d/e/1FAIpQLScwMZBTMErBhuXNGNfY_dubgcV8FT3eqYOioxluV82v1V766Q/viewform?usp=sf_link" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><strong>here</strong></a><strong>.</strong> To be considered as part of the next iteration of affiliates, apply by <strong>November 5th</strong>.</p><p>At <a href="https://www.pibbss.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">PIBBSS</a> (“Principles of Intelligent Behavior in Biological and Social Systems”), our mission is to facilitate AI alignment research that draws on the parallels between intelligent behavior in natural and artificial systems. (For more details, see the section &#34;About PIBBSS and our research priorities&#34; below.)</p><p>Since PIBBSS’ inception in 2022, we have run two iterations of our <a href="https://www.pibbss.ai/fellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">3-months research fellowships</a>, as well as <a href="https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a reading group</a>, <a href="https://www.youtube.com/channel/UCMo5Ei9xLHbk9sqNMSH3mdQ" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">an ongoing speaker series</a>, and a number of research retreats exploring topics in line with PIBBSS’ research interests. Our alumni have gone on to pursue AI alignment research at places like OpenAI, Anthropic, the Alignment of Complex Systems research group, and academia, as well as through independent funding. </p><p>While we continue to be excited about the value of those activities, we also believe that there is more value to be had! In particular, we see potential in a program geared towards <strong>supporting longer-term, more sustained research efforts</strong> in line with PIBBSS’ research bet. </p><p>As such, we are delighted to launch our new <a href="https://www.pibbss.ai/affiliate" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><strong>PIBBSS Affiliate Program</strong></a>. </p><h2><strong>Goals and key features of the Affiliate Program</strong></h2><p>At its core, our <strong>goal </strong>in running this program is to counterfactually help produce substantial, high-quality and legible research progress on problems in AI risk, alignment and governance. The affiliate program aims to do this by supporting promising individual researchers in a highly tailored fashion to scope, develop, and progress on their personal research agendas. </p><p>Concrete <strong>success scenarios </strong>might look like (but are not limited to) affiliates publishing insightful research results; joining the efforts of existing research groups or founding new ones; identifying, scoping, testing and building traction on novel or neglected research avenues; maturing their personal research agenda and secure independent funding; etc.</p><p>The PIBBSS Affiliate Program has the following <strong>key features</strong>: </p><ul><li>Affiliates receive a <strong>salary </strong>of 5,000 USD/month/FTE. We are open to paying higher salaries to exceptional candidates. <ul><li>We are, by default, looking for affiliates who will be joining the program full-time. That said, we are open to adapting to affiliates’ circumstances where this seems appropriate, such as joining the program in a part-time fashion. We also have the capacity to offer support to people who do not want to leave their current research positions.</li></ul></li><li>Affiliates benefit from a range of <strong>support structures </strong>aimed at helping them focus on and advance in their research endeavors, such as:<ul><li><strong>Quarterly research retreats </strong>bringing together affiliates as well as other alignment researchers frothe the m our extended network </li><li><strong>Personalized research and strategic support</strong>, typically in the form of bi-weekly calls with your point person (with option to adapt frequency of nature of support to the ffiliate’s needs and preferences)</li><li><strong>Access to the PIBBSS research community </strong>and extended network, including joining our slack space, tailored introductions, opportunities to participate in workshops, etc. </li><li><strong>Financial and administrative support</strong> in order to e.g. visit relevant research conferences, get access to base models and compute where required for research, etc.</li><li><strong>Access to office space depending on the needs/preferences of affiliates.</strong></li><li>Note: A key feature of the program is to provide highly tailored support to our affiliates, according to what counterfactually helps them most make progress on their research. As such, the above list, while representative, is not necessarily fully precise or comprehensive.</li></ul></li><li>The <strong>initial duration of the affiliateship is 6 months</strong>, at which point we review affiliates for a possible <strong>extension to 12 months</strong>. In exceptional circumstances, we will make initial offers of 12 months. <ul><li>Our vision is for the affiliate program to provide a serious long-term home for promising researchers pursuing PIBBSS-style AI alignment research at time scales of 1 year and more, including the possibility of permanent positions. </li></ul></li><li>We want to build on and strengthen the <strong>supportive, curious and dedicated culture </strong>that PIBBSS has built over the last two years. We care about good epistemic practices while navigating the intricacies of making sense of a complex world and particularly emphasize the value of <a href="https://www.alignmentforum.org/posts/wi3upQibefMcFs5to/levels-of-pluralism" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">epistemic pluralism</a> in doing so. </li></ul><p><br/> At the current point in time, we orient to the affiliate program with an <strong>experimental </strong>lens - testing the hypothesis that this program can provide significant value, and learning about how exactly we can support our affiliates most effectively. Depending on what we learn, we might scale up or scale down, streamline the program or make it more tailored to individual affiliates, aim to have affiliates for time-limited or for open-ended periods, or do other things which increase the impact of the program. </p><h2><strong>Requirements of the Affiliate Program</strong></h2><p>The affiliate program is a very open-ended program with limited requirements. Here is what we asked from our affiliates:</p><ul><li>Submitting a <strong>quarterly</strong> <strong>report</strong> reflecting on your research progress and prospective research plan. <ul><li>The reports are meant to help the affiliate by providing an accountability structure and encouraging them to zoom out and reflect on a regular basis. They also help us understand what affiliates have been working on and whether there is anything we can do to support them better going forward.</li></ul></li><li>Participating in our ~quarterly <strong>research retreats</strong> </li><li>A <strong>genuine commitment </strong>to making progress on questions related to AI safety, alignment and governance, and constructive, truth-seeking and kind participation in our shared epistemic culture </li></ul><h2><strong>What are we looking for in an ideal affiliate?</strong></h2><p>Affiliates are selected for being in a strong position to pursue promising PIBBSS-aligned research directions in AI alignment in a highly self-directed fashion. As such, we are looking for (a mix of) the following characteristics:</p><ul><li>Scholarly competence/caliber in your domain(s) of expertise </li><li>Strong prior understanding of AI Risk &amp; Alignment</li><li>Evidence of good research judgment/taste</li><li>Evidence for good research productivity, including strong self-management skills</li><li>Promisingness of your proposed line of research</li><li>Ability to counterfactually benefit from what PIBBSS can offer </li></ul><p>Note that, in our experience, great PIBBSS-style researchers have often had non-standard backgrounds. As such, we wish to encourage individuals with varied backgrounds and perspectives to apply. </p><h2><strong>About PIBBSS and our research priorities</strong></h2><p>PIBBSS is a research initiative facilitating work that draws on the parallels between intelligent behavior in natural and artificial systems, and leveraging these insights towards making AI systems safe, beneficial and aligned.</p><p>In essence, we believe that the study of complex natural and social systems can help us better understand the risk involved in implementing such behaviors in artificial substrates and improve our ability to develop AI systems such that they reliably display desired safety- and governability-relevant properties. (We describe in some more detail the nature of and some background assumptions going into our epistemic bet in <a href="https://www.alignmentforum.org/posts/FuToH2KHxKmJLGk2B/ai-alignment-as-navigating-the-space-of-intelligent" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this write up</a>, as well as in <a href="https://youtu.be/2UabDJKVQcI?si=abcbqyDX6xPvcgmK" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this talk</a>). </p><p>To provide more concrete pointers towards the type of research we’re excited about, here is a sample of the work that we have supported in the past: </p><ul><li>Aiming to develop a principled understanding of the dynamics emerging from <strong>multi-agent interactions between AI systems</strong>,<strong> </strong>such as by drawing on <a href="https://youtu.be/h8ZbjKS9OTA" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the study of collective behavior in army ants</a> or models from evolutionary biology and ecology </li><li>Exploring how to include understanding of multi-agent AI dynamics in developing <strong>model evaluations </strong>[forthcoming] </li><li>Exploring <strong>novel avenues for interpretability</strong>, such as <a href="https://youtu.be/cvA3C68Hzqg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">A Geometric Approach to Interpretability</a> and <a href="https://www.youtube.com/watch?v=2dx_VRZtoCs&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Singular Learning Theory</a></li><li>Building towards <strong>naturalized theories of agency</strong>, such as by exploring the <a href="https://youtu.be/vx9iHJLd9xI" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">role of abstractions in the phenomenon of life</a>;  testing the <a href="https://www.alignmentforum.org/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update#What_high_level_progress_have_you_personally_made_in_the_past_year__Any_mistakes_made_or_things_to_change_going_forward_" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">natural abstraction hypothesis in the game of life</a>; thinking about the characteristics of agents that apply the intentional stance on themselves <a href="https://www.youtube.com/watch?v=bFGcBrnrdmk&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">(“auto-intentional agents”)</a>; or applying a dynamical systems perspective on goal-oriented behavior </li><li>Tackling problems in <strong>decision theory</strong>, such as <a href="https://www.youtube.com/watch?v=uNv1Cq-aVKY&amp;t=46s&amp;pp=ygURcGliYnNzIGZlbGxvd3NoaXA%3D&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">formalizing updateless decision theory</a>, <a href="https://www.youtube.com/watch?v=ivqSoZJEiBc&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">robustness to self-modification</a>, or trying to bridge <a href="https://www.alignmentforum.org/posts/rQDYQrDjPGqjrf8Mk/bridging-expected-utility-maximization-and-optimization" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">expected utility maximization and optimization</a></li><li>Developing more <strong>realistic account of values and practical reasoning</strong>, and exploring their implications for our understanding of the AI alignment problem [forthcoming] </li><li>Evaluating the <strong>potential and limits of existing </strong><a href="https://www.youtube.com/watch?v=05iQTypREPc&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><strong>legal tools</strong></a> for reducing catastrophic risks from AI</li><li>Testing the potential for <strong>productive epistemic exchange </strong>between AI alignment and other fields/topics, e.g. <a href="https://humanvaluesandartificialagency.com/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">artificial life</a>, <a href="https://www.youtube.com/watch?v=RwEKg5cjkKQ&amp;pp=ygURcGliYnNzIGZlbGxvd3NoaXA%3D&amp;ab_channel=PIBBSSFellowshiphttps://www.youtube.com/watch?v=RwEKg5cjkKQ&amp;pp=ygURcGliYnNzIGZlbGxvd3NoaXA%3D&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">basal cognition</a>, <a href="https://www.youtube.com/watch?v=7-wvArSvHsc&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">morphological computation</a>, active inference, critical social theory, etc.</li><li>Looking at <strong>philosophy of sciences (and the special sciences) </strong>to find ways to make <a href="https://www.lesswrong.com/posts/FuToH2KHxKmJLGk2B/ai-alignment-as-navigating-the-space-of-intelligent" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">our. </a><a href="https://www.youtube.com/watch?v=itvIBZ8a5BU&amp;t=947s&amp;pp=ygURcGliYnNzIGZlbGxvd3NoaXA%3D" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">research</a>. <a href="https://www.youtube.com/watch?v=vUxTiHUazCo&amp;ab_channel=PIBBSSFellowship" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">efforts</a>. <a href="https://www.lesswrong.com/posts/CewHdaAjEvG3bpc6C/epistemic-artefacts-of-conceptual-ai-alignment-research" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">more</a>. <a href="https://www.lesswrong.com/posts/b9sGz74ayftqPBDYv/the-space-of-systems-and-the-space-of-maps" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">productive</a>.</li></ul><p>Our fellows and collaborators have brought expertise from many domains to bear on the question of AI alignment, including but not limited to:</p><ul><li>Ecology</li><li>Evolutionary and developmental biology</li><li>Systems biology</li><li>Biophysics</li><li>Neuroscience</li><li>Cognitive science</li><li>Linguistics </li><li>Legal theory</li><li>Political theory</li><li>Economic theory</li><li>Social theory</li><li>Complex systems studies </li><li>Cybernetics</li><li>Network science</li><li>Physics</li><li>Philosophy of Science</li></ul><h2><strong>How to apply</strong></h2><p>Applications are now open. You can apply by following <a href="https://docs.google.com/forms/d/e/1FAIpQLScwMZBTMErBhuXNGNfY_dubgcV8FT3eqYOioxluV82v1V766Q/viewform?usp=sf_link" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this link</a>. </p><p>We accept new affiliates on a ~quarterly basis. To be considered as part of the next iteration, apply by <strong>November 5th</strong>.</p><p>We expect to hire 2-6 (median 4) affiliates over the coming 6 months.</p><p><strong>About the application process: </strong>The details of the application process may vary between candidates in virtue of which information we think will most help us make well-informed decisions. In most cases, the initial stage (application form) is followed by one or more interviews to discuss your research in more depth. We may also reach out to your references, or in some cases, ask you to submit a work task of ~2-5h. </p><br/><br/><a href="https://www.alignmentforum.org/posts/JvL3SC6tPjFyCiHad/become-a-pibbss-research-affiliate-1#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>