<figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee74c0a0-322e-4c5a-b2c8-c7f4081c07a6_943x517.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/oH1aOCu0ni8LfC4cD13NQ7jOsE7riPMNDu4U-vcXqj0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZTc0YzBhMC0zMjJlLTRjNWEtYjJjOC1jN2Y0MDgxYzA3YTZfOTQzeDUxNy5qcGVn 424w, https://reader.miniflux.app/proxy/6AfurztFoTNhi3oQMYAMXOe4GsRK5Z4qTFqghYx5fXs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZTc0YzBhMC0zMjJlLTRjNWEtYjJjOC1jN2Y0MDgxYzA3YTZfOTQzeDUxNy5qcGVn 848w, https://reader.miniflux.app/proxy/y6hkOHGPWHjvtRbcvUdOGFs5OktJDt1MsP36gI0ToDA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWU3NGMwYTAtMzIyZS00YzVhLWIyYzgtYzdmNDA4MWMwN2E2Xzk0M3g1MTcuanBlZw== 1272w, https://reader.miniflux.app/proxy/6aC2GKd9X6ozjNYGAhPZI_rp9x2KCdeEV9JNO5ZQR10=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWU3NGMwYTAtMzIyZS00YzVhLWIyYzgtYzdmNDA4MWMwN2E2Xzk0M3g1MTcuanBlZw== 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/jw7TwVYpkHQun_ae_es7a667pG5bYbD5zEtA3JQW2zc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWU3NGMwYTAtMzIyZS00YzVhLWIyYzgtYzdmNDA4MWMwN2E2Xzk0M3g1MTcuanBlZw==" alt="" srcset="https://reader.miniflux.app/proxy/-0i9kG7cM_Mhy2rZNi5si-j7ZY4nAB657tMv9ml2BFo=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZTc0YzBhMC0zMjJlLTRjNWEtYjJjOC1jN2Y0MDgxYzA3YTZfOTQzeDUxNy5qcGVn 424w, https://reader.miniflux.app/proxy/Uurxbk4BJ--MCzb-2xQn9gvQ-lzEEdBjNqaK4_No38k=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZTc0YzBhMC0zMjJlLTRjNWEtYjJjOC1jN2Y0MDgxYzA3YTZfOTQzeDUxNy5qcGVn 848w, https://reader.miniflux.app/proxy/SiO1pQS1J1QDalNQO4VB743Hzr-dY52uJCNPn5hb1Ck=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWU3NGMwYTAtMzIyZS00YzVhLWIyYzgtYzdmNDA4MWMwN2E2Xzk0M3g1MTcuanBlZw== 1272w, https://reader.miniflux.app/proxy/jw7TwVYpkHQun_ae_es7a667pG5bYbD5zEtA3JQW2zc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWU3NGMwYTAtMzIyZS00YzVhLWIyYzgtYzdmNDA4MWMwN2E2Xzk0M3g1MTcuanBlZw== 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption>Donâ€™t Look Up</figcaption></figure><p>No, I donâ€™t think there is a 100% chance that humans will go extinct any time soon, nor a 0% chance either.  ğ©(ğğ¨ğ¨ğ¦)â€”I canâ€™t figure out exactly where the term originated, and it is often used a bit tongue-in-cheekâ€”is supposed to be the chance that we are all going to die in an extinction event. </p><p>In the limit, of course, the probability is one, and too depressing<a id="footnote-anchor-1" href="#footnote-1">1</a> to talk about in polite society. When the universe goes down, all of our descendants, if we have any left, will go down with the ship.</p><p>But we blunder on. When people talk about p(doom) they are really talking about the chance that humans will go extinct <em>soon</em>, say in the next hundred years. </p><p>I was at a workshop in the Bay Area recently, and the â€œicebreakerâ€ was, â€œsay your name and give p(doom)â€.  Usually when people are talking about p(doom), they are thinking specifically about AI â€“ could AI kill us all? Whatâ€™s the odds of that? A lot of people in Silicon Valley are pretty worried that p(doom) might be something other than zero; some even seem to put the number at ten percent or higher. (Eliezer Yudkowsky seems to put it at near 100%). One survey, which was not perhaps entirely representative, put it a lot higher than that. And that was before GPT-4 which some people (not me) think somehow increases the odds of doom.</p><p>Personally, despite the name of this essay, I am not all <em>that</em> worried about humans going extinct anytime very soon. Itâ€™s hard for me to construct a realistic scenario of literal, full-on extinction that I take seriously. We are a pretty resourceful species, spread throughout the globe. Covid was pretty terrible, but it didnâ€™t come remotely close to killing us all (about 1 in a 1000 humans died). Nuclear war would kill a very large number of people, and set back civilization immensely, but I doubt that literally every human in every corner of the earth would die. </p><p>But who cares what I think?</p><p>The fact that I, sitting on my couch, canâ€™t think of a compelling scenario in which machines, directly or indirectly, on purpose or by accident, of their own volition or via the deliberate commands of a madman , could kill us all doesnâ€™t mean it wonâ€™t happen.  Poverty-of-the-imagination arguments donâ€™t have a great track record. (Some onlookers couldnâ€™t possibly imagine that heavier-than-air aircraft could fly, others couldnâ€™t imagine people would be able build  nuclear weapons;  even in the Reagan era I didnâ€™t imagine that a <em>game show host</em> could become President, etc).  There are in fact already some scenarios that are faintly imaginable now (e.g. bad actors using machines to create novel toxins, in ways that go awry), and over time could be more. The reality is we just arenâ€™t really good at imagining technology fifty or one hundred years out. Just because we canâ€™t imagine something doesnâ€™t mean it might now happen in 50 years. Who among us anticipated social networks with a billion users in 1990? Or any of their side effects on mental health or public discourse, and perhaps even geopolitics? </p><p>Meanwhile, p(doom) per se is not the only thing we should be worried about. Perhaps extinction is vanishingly unlikely, but what I will call ğ©(ğœğšğ­ğšğ¬ğ­ğ«ğ¨ğ©ğ¡ğ) â€“ the chance that an incident that kills (say) one percent or more of the population, seems a lot higher. For literal extinction, you would have to kill every human in every nook and cranny on earth; thatâ€™s hard, even if you do it on purpose. Catastrophe is a lot easier to imagine. I can think of a LOT of scenarios where that could come to pass (e.g., bad actors using AI to short the stock market could try, successfully, to shut down the power grid and the internet in the US, and the US could wrongly blame Russia, and conflict could ensure and escalate into a full-on physical war). </p><p>When Sam Altman said in May at our Senate hearing â€œMy worst fears are that we cause significant, we, the field, the technology, the industry â€” cause significant harm to the worldâ€ he was not wrong. Can any of us truly say thatâ€™s simply not possible?</p><p>Â§Â </p><p>Yet a really large fraction of the AI and AI-adjacent community doesnâ€™t really want to talk about any of this at all. Many scoff at the whole idea of discussing p(doom), and few seem to want to discuss p(catastrophe) either, when I try to bring it up. Just a couple days ago, one serious computer scientist asserted to me that â€œclimate change is real and &#34;AI Risk&#34; is notâ€, as if we were wasting time even thinking about the problem.</p><p>One common argument is that we have no idea how to estimate it, another that most of the common scenarios proposed sound like science-fiction.</p><p>Neither counter remotely helps me sleep at night.</p><p>The problem with the latter argument is that science-fiction does sometimes come true. We didnâ€™t have a lot of orbiting satellites when Arthur C Clarke first wrote about them, now we have thousands. Star Trek communicators came (in the form of Motorola flip phones) and have now long since been left in the dust. </p><p>The problem with the former argumentâ€”that we have no idea how to estimate p(doom)â€”is twofold. First, it confuses epistemology (what we can know and how we can come to know it) with probability. Just because we canâ€™t estimate something well doesnâ€™t mean it canâ€™t kill us.  Unfortunately, the fact that we canâ€™t predict the course of technology with any precision even a decade out doesnâ€™t magically immunize us from its effects, nor relieve us of a moral responsbility to try to take precautions. We canâ€™t really estimate the probability that synthetic biology will kill us all, but we that shouldnâ€™t stop us from taking precautions.</p><p>Whatâ€™s ğ©(ğ›ğ¢ğ¨ğ¥ğ¨ğ ğ¢ğœğšğ¥ ğœğšğ­ğšğ¬ğ­ğ«ğ¨ğ©ğ¡ğ)? We probably canâ€™t predict it with precision; there many unknowns. Any confidence interval would necessarily be broad. But does that mean we shouldnâ€™t really worry about biological catastrophe? Or course not.</p><p>Meanwhile, we humans have a rather bad history of neglecting issues that are both serious and dark on the grounds that they rest on things we havenâ€™t figured out. Hereâ€™s a summary of the situation with respect to climate in the 1970s:</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5be1c5-01b8-4105-a350-f81ea37e4889_1544x784.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/ccXHSBL9-AhSr3ixSUHpdjvBaeCAmPmXdzFA2J2dZ7I=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZjViZTFjNS0wMWI4LTQxMDUtYTM1MC1mODFlYTM3ZTQ4ODlfMTU0NHg3ODQuanBlZw== 424w, https://reader.miniflux.app/proxy/TY795LF--T41tzj1bmteUl8-4QPYMwdpPMNlz3eGNqs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZjViZTFjNS0wMWI4LTQxMDUtYTM1MC1mODFlYTM3ZTQ4ODlfMTU0NHg3ODQuanBlZw== 848w, https://reader.miniflux.app/proxy/h_FfBJf7G-lMz3wYNf--kxBeUL2ZDkQTyKp8x0XMeF4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWY1YmUxYzUtMDFiOC00MTA1LWEzNTAtZjgxZWEzN2U0ODg5XzE1NDR4Nzg0LmpwZWc= 1272w, https://reader.miniflux.app/proxy/K4P4Wq5fgPy9NaTDtiRVDsu1EOSiwf1jPaARNh9YGd0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWY1YmUxYzUtMDFiOC00MTA1LWEzNTAtZjgxZWEzN2U0ODg5XzE1NDR4Nzg0LmpwZWc= 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/fPuQREGCUccg47nUZ5WwX20fyi0T2VIY1I8_ySEGtiM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWY1YmUxYzUtMDFiOC00MTA1LWEzNTAtZjgxZWEzN2U0ODg5XzE1NDR4Nzg0LmpwZWc=" alt="" srcset="https://reader.miniflux.app/proxy/5jdhd65zdCU-urd2yk-q6rbrTWi5_9_S6Fjtvw69lDE=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZjViZTFjNS0wMWI4LTQxMDUtYTM1MC1mODFlYTM3ZTQ4ODlfMTU0NHg3ODQuanBlZw== 424w, https://reader.miniflux.app/proxy/Px1-U8IGmt2ZzBdVN0KoqHxsQ_VmoHTtZFH3d-OoGKU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZlZjViZTFjNS0wMWI4LTQxMDUtYTM1MC1mODFlYTM3ZTQ4ODlfMTU0NHg3ODQuanBlZw== 848w, https://reader.miniflux.app/proxy/rUVMh6MEOrtUICAQdj6KZrPK_fb9LeG7SDWETBiriu4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWY1YmUxYzUtMDFiOC00MTA1LWEzNTAtZjgxZWEzN2U0ODg5XzE1NDR4Nzg0LmpwZWc= 1272w, https://reader.miniflux.app/proxy/fPuQREGCUccg47nUZ5WwX20fyi0T2VIY1I8_ySEGtiM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGZWY1YmUxYzUtMDFiOC00MTA1LWEzNTAtZjgxZWEzN2U0ODg5XzE1NDR4Nzg0LmpwZWc= 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption>From  â€œ<a href="https://www.theguardian.com/science/2021/jul/05/sixty-years-of-climate-change-warnings-the-signs-that-were-missed-and-ignored" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Sixty years of climate change warning: the signs that were missed (and ignored)</a>â€ published by the Guardian in 2021</figcaption></figure><p>Sound familiar? The phrase ğ©(ğğ¨ğ¨ğ¦) might be new, but the practicing of <em>ridiculing</em> doomers goes back at least a half century. In the case of climate change, those who did the ridicule were on the wrong side of history. </p><p>Â§</p><p>Another common argument is to say that there is no reason whatsoever to expect AI to control (or annihilate) humans. Yann LeCun wrote a long screed along these lines (which Steve Pinker later endorsed), saying we donâ€™t need to worry about AI achieving dominance. Some of what he says is right, but thereâ€™s a fallacy here, too. LeCunâ€™s premise that the smartest doesnâ€™t always win is of course right but his argument as a whole sputters out in the end:</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91591e9c-8fec-4cb1-adfc-5a4a339c2457_1344x1958.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/jLNFcWEAfoipx-ZIKbmcFigyr7vpGWyNChVbVMjokU0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY5MTU5MWU5Yy04ZmVjLTRjYjEtYWRmYy01YTRhMzM5YzI0NTdfMTM0NHgxOTU4LmpwZWc= 424w, https://reader.miniflux.app/proxy/3NJO4-AJpWUI5t21Smm_VxB9lcyoqWTRCVHrxpDyGLI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY5MTU5MWU5Yy04ZmVjLTRjYjEtYWRmYy01YTRhMzM5YzI0NTdfMTM0NHgxOTU4LmpwZWc= 848w, https://reader.miniflux.app/proxy/Z5WCBwTjCEZA9kMIDm5qbw_ejTN-tGUuuZZcKhj2uto=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGOTE1OTFlOWMtOGZlYy00Y2IxLWFkZmMtNWE0YTMzOWMyNDU3XzEzNDR4MTk1OC5qcGVn 1272w, https://reader.miniflux.app/proxy/aTzubMj_G6SdLYZuda3R-Yx231Zzhr5m4IAskZDlY5E=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGOTE1OTFlOWMtOGZlYy00Y2IxLWFkZmMtNWE0YTMzOWMyNDU3XzEzNDR4MTk1OC5qcGVn 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/KpaO7oe0nAtrgW-60iq3gYGxTUhnRPxk8lHGti-q1RU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGOTE1OTFlOWMtOGZlYy00Y2IxLWFkZmMtNWE0YTMzOWMyNDU3XzEzNDR4MTk1OC5qcGVn" alt="" srcset="https://reader.miniflux.app/proxy/wOZW3j3f2BqpHo_xa9Sq_LKPh7BwHP2Rsg9GmZObRSU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY5MTU5MWU5Yy04ZmVjLTRjYjEtYWRmYy01YTRhMzM5YzI0NTdfMTM0NHgxOTU4LmpwZWc= 424w, https://reader.miniflux.app/proxy/oWTEQiQe-0KS2M2FUYcHytX2VsaWp9syarnwQCtzmac=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY5MTU5MWU5Yy04ZmVjLTRjYjEtYWRmYy01YTRhMzM5YzI0NTdfMTM0NHgxOTU4LmpwZWc= 848w, https://reader.miniflux.app/proxy/vCbseKfYwnXYfSdTUAtqU7W0jo4LmctIQSaJYCKgNso=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGOTE1OTFlOWMtOGZlYy00Y2IxLWFkZmMtNWE0YTMzOWMyNDU3XzEzNDR4MTk1OC5qcGVn 1272w, https://reader.miniflux.app/proxy/KpaO7oe0nAtrgW-60iq3gYGxTUhnRPxk8lHGti-q1RU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGOTE1OTFlOWMtOGZlYy00Y2IxLWFkZmMtNWE0YTMzOWMyNDU3XzEzNDR4MTk1OC5qcGVn 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p>LeCunâ€™s conclusion â€”that humans will always set the agendaâ€”<em>might</em> be true, but he hasnâ€™t given a real argument.  Instead LeCun has actually <em>assumed his conclusion</em>â€”a cardinal sin in logicâ€”as his premise. The whole thing is circular:  we will be able to set the agenda therefore we will be able to set the agenda. The question is not whether we <em>want </em>to design AI to be like a supersmart-but-non-dominating staff member (obviously we are trying), but whether we <em>can</em>.  </p><p>Unfortunately, LeCun gives us no hint as how to we might, and no guarantee that doing so is even possible.<a id="footnote-anchor-2" href="#footnote-2">2</a></p><p>When I think about how poor his company (Meta) is even at making LLMs that donâ€™t hallucinate, I get worried. And when I think about Metaâ€™s track record, for emphasizing profit versus global good,  I get even more worried.</p><p>If push came to shove, could we have confidence that Meta (or any other big tech company) would build maximally safe but perhaps less profitable AI rather than a maximally profitable but possibly less safe AI, given the choice?</p><p>Sometimes all you can do is laugh.</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8953be5d-4262-4e9c-960b-38d52122b9ba_941x953.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/84rGqmXcEghdpbYFNQeZFaRowBOpAnI40Ve-7dxJp04=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY4OTUzYmU1ZC00MjYyLTRlOWMtOTYwYi0zOGQ1MjEyMmI5YmFfOTQxeDk1My5qcGVn 424w, https://reader.miniflux.app/proxy/L97oqIh044uoFF_vjTszHCP79Yu6spKwhNqlHJZbpa4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY4OTUzYmU1ZC00MjYyLTRlOWMtOTYwYi0zOGQ1MjEyMmI5YmFfOTQxeDk1My5qcGVn 848w, https://reader.miniflux.app/proxy/iBEmeSG73P0kbE3Bfha9rbYKXYHntyB2ITNk8UW1wGA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGODk1M2JlNWQtNDI2Mi00ZTljLTk2MGItMzhkNTIxMjJiOWJhXzk0MXg5NTMuanBlZw== 1272w, https://reader.miniflux.app/proxy/_bmawph8T-TbxvVR5QrgWxyHHqRN9f0It5bhXruuWPc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGODk1M2JlNWQtNDI2Mi00ZTljLTk2MGItMzhkNTIxMjJiOWJhXzk0MXg5NTMuanBlZw== 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/ci00XtvBnXY1H6gNTjNFrviHnmrAbra_LE0jCQixKzw=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGODk1M2JlNWQtNDI2Mi00ZTljLTk2MGItMzhkNTIxMjJiOWJhXzk0MXg5NTMuanBlZw==" alt="" srcset="https://reader.miniflux.app/proxy/3VqvfEsDc1Y6leCSLFEqkC0GK8BVux2g8KUrKIeeb3U=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY4OTUzYmU1ZC00MjYyLTRlOWMtOTYwYi0zOGQ1MjEyMmI5YmFfOTQxeDk1My5qcGVn 424w, https://reader.miniflux.app/proxy/-a2Wb79o6-rU1LCzv_nHIOysqVupeAYfGRJhzqJcKEI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY4OTUzYmU1ZC00MjYyLTRlOWMtOTYwYi0zOGQ1MjEyMmI5YmFfOTQxeDk1My5qcGVn 848w, https://reader.miniflux.app/proxy/WOxCjw35fKgojZI_yA8f4muuoLOi4pZa3GLjjARw5D0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGODk1M2JlNWQtNDI2Mi00ZTljLTk2MGItMzhkNTIxMjJiOWJhXzk0MXg5NTMuanBlZw== 1272w, https://reader.miniflux.app/proxy/ci00XtvBnXY1H6gNTjNFrviHnmrAbra_LE0jCQixKzw=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGODk1M2JlNWQtNDI2Mi00ZTljLTk2MGItMzhkNTIxMjJiOWJhXzk0MXg5NTMuanBlZw== 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p>As ed tech researcher Nirmal Patel put it last night, â€Probability of AI doom depends on both technology capabilities and nature of world leaders. We have atomic bombs and one crazy leader is what it will take for doomâ€; eventually, we might have AI that is equally risky in the hands of a desperate, deranged leader.  Should we think about how to prepare for such scenarios now? Or later? </p><p>Â§</p><p>None of this means of course that I suddenly think that we are close to super-smart machines. (Spoiler alert: we are not). </p><p>But if and when we get there, will they be dominating? Non-dominating? The honest answer is we havenâ€™t a clue. Will errant dictators use them for profit or political gain? All we have is a bunch of analogies.</p><p>Which makes actually calculating p(doom) fiendishly hard. What you want is sort of like this, a calculator someone (pausai.info) is in the process of putting together on the web:</p><p></p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40794063-6033-4a87-9ab8-1d4db3239191_1168x2141.jpeg" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/JmT6u3QJjfVbwVXsCPnjIE6ZEktGIMSVdp9Ie7aYHJs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MDc5NDA2My02MDMzLTRhODctOWFiOC0xZDRkYjMyMzkxOTFfMTE2OHgyMTQxLmpwZWc= 424w, https://reader.miniflux.app/proxy/u1kx8icukxwhNAi11P-bvPf-ciIyhHHsuqMqIh_sRIY=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MDc5NDA2My02MDMzLTRhODctOWFiOC0xZDRkYjMyMzkxOTFfMTE2OHgyMTQxLmpwZWc= 848w, https://reader.miniflux.app/proxy/n874umvRtTZ2lwXrZ3vliG11DcVQLSw8lXI5X9xS19Y=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDA3OTQwNjMtNjAzMy00YTg3LTlhYjgtMWQ0ZGIzMjM5MTkxXzExNjh4MjE0MS5qcGVn 1272w, https://reader.miniflux.app/proxy/jJxNPzYYlHJC45JyMIBUYAy1roGf6vEgjhBUwHqmjqM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDA3OTQwNjMtNjAzMy00YTg3LTlhYjgtMWQ0ZGIzMjM5MTkxXzExNjh4MjE0MS5qcGVn 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/bX2nwWl4CSY8Zv0XGwuME2LEAz83-3m2PqDhCiTYaF8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDA3OTQwNjMtNjAzMy00YTg3LTlhYjgtMWQ0ZGIzMjM5MTkxXzExNjh4MjE0MS5qcGVn" alt="" srcset="https://reader.miniflux.app/proxy/x-RvBfEuK6-808kNGBrz1BSS-QKHiqFPaHwVN783xLU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MDc5NDA2My02MDMzLTRhODctOWFiOC0xZDRkYjMyMzkxOTFfMTE2OHgyMTQxLmpwZWc= 424w, https://reader.miniflux.app/proxy/XHNh3oKSocqDXVcVqBY4T3Xugo9yaL_5fmotEAtMeAw=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MDc5NDA2My02MDMzLTRhODctOWFiOC0xZDRkYjMyMzkxOTFfMTE2OHgyMTQxLmpwZWc= 848w, https://reader.miniflux.app/proxy/6CWdUNyI1G-s283rU11bSQqPZuZMevCjROJV2zBVv5g=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDA3OTQwNjMtNjAzMy00YTg3LTlhYjgtMWQ0ZGIzMjM5MTkxXzExNjh4MjE0MS5qcGVn 1272w, https://reader.miniflux.app/proxy/bX2nwWl4CSY8Zv0XGwuME2LEAz83-3m2PqDhCiTYaF8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDA3OTQwNjMtNjAzMy00YTg3LTlhYjgtMWQ0ZGIzMjM5MTkxXzExNjh4MjE0MS5qcGVn 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption>hsndy-dandy <a href="http://pauseai.info" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">p(doom) calculator</a></figcaption></figure><p>You can <a href="http://pauseai.info" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">try it out</a>; you set your best guesses for a bunch of sliders, and out comes your own personal p(doom) estimate, suitable for discussing at Silicon Valley cocktail parties. </p><p>Â§Â </p><p>Aryeh Englander, a graduate student at the University of Maryland, is trying to formulate these calculates this in a considerably more detailed way, writing his entire dissertation around trying to make serious, thoughtful calculations on p(doom) - and p(catastophe) as well â€“  with proper confidence intervals, and so on. He pointed me to a <a href="https://arxiv.org/pdf/2206.09360.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">147 page analysis that already exists</a>, and a <a href="https://www.alignmentforum.org/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">more recent technical note from earlier this week,</a> and hopes to produce an even deeper analysis. (As he notes the term p(doom) is itself just tongue-in-cheek shorthand, a sort of fieldwide in-joke to quickly point up some more nuanced concepts that he will treat at length.)</p><p>As Englander well knows, we will never get <em>perfect</em> estimates, but even imperfect estimates might help guide policy decisions as we try to balance attention to short-term and long-term risks in AI. In his words, in an email earlier this week:</p><blockquote><p><em>We only really care about &#34;p(doom)&#34; or the like as it relates to specific decisions. In particular, I think the reason most people in policy discussions care about something like p(doom) is because for many people higher default p(doom) means they&#39;re willing to make larger tradeoffs to reduce that risk. For example, if your p(doom) is very low then you might not want to restrict AI progress in any way just because of some remote possibility of catastrophe (although you might want to regulate AI for other reasons!). But if your p(doom) is higher then you start being willing to make harder and harder sacrifices to avoid really grave outcomes.</em>â€ </p></blockquote><p>Itâ€™s therefore worth getting a rough idea of the number, even if we canâ€™t achieve consensus on the exact number. With policy decisions at stake, the more confidence we can place in the estimate, the better. But imprecision is not an excuse for  inaction. </p><p>Â§</p><p> I donâ€™t self-identify as a â€œdoomerâ€ (unlike say Eliezer Yudkowsky who may). Unlike Yudkowsky I donâ€™t think the probabilities are high enoughâ€”or imminent enough- to warrant bombing data centers any time soon. (Or probably ever.)</p><p>But all the ridicule I am seeing just doesnâ€™t make sense.  Some of the specific arguments Yudkowsky and Bostrom have made may well be weak, but there is no solid argument that the number is zero, or even that the risk is infinitesimal.  </p><p>We waited too long on climate change; to the extent that there may be any real risk, either of outright doom or â€œmereâ€ catastrophe, letâ€™s not wait too long on AI. </p><p>Â§</p><p>An important clarification before I conclude, prompted by thoughtful comments on an earlier draft by Ã‰mile Torres: AI risk â‰  Superintelligence risk per se. Some of the risk from AI comes from the possibility of an uncontrollable superintelligence; a lot of it does not. </p><p>Most of the public discourse around AI risk is about whether <em>superintelligent </em>AI (if and when it exists) will extinguish our species - an important question that deserves answers. </p><p>But bluntly, uncontrollable superintelligence is by no means the only risk from AI, and from a policy perspective may not be the most important risk, given both the lowish (?) probability and the time horizon. I am, at least for now, <em>much</em> more worried about the overapplication of <em>mediocre</em> AI (eg what happens if we put large language models into running military strategy or into hiring decisions that affect millions of people) than the use of AI that might be so smart it could catch its own errors. And, as noted, I more worried about catastrophe than extinction per se. But I would count <em>all</em> of that as AI risk. I do think there is a lot of risk that AI will cause chaos. (<em>Controllable </em>superintelligence, by the way, has pretty serious risks, too, depending on who controls it, as Nirmal pointed above).</p><p>Within the field, there is a lot of conflict around how to think about p(doom | superintelligence) [â€œthe probability of doom given the existence of superintelligenceâ€, which is a special form of AI], but no matter what you feel about that p(doom | superintelligence), whether you see it as sci-fi or not, you should be worried about p(catastrophe | widespread AI adoption) [the probability of catastrophe given widespread AI adoption].  </p><p>And you should care about p(catastrophe | widespread AI adoption) whatever you think about p(doom). The risks of current AI (bias, defamation, cybercrime, wholesale disinformation, etc) are already starting to be well documented and may themselves quickly escalate, and could lead to geopolitical instability even over the next few years. </p><p>And what really gets the goat of people like Timnit Gebru and Abeba Birhane, not unreasonably, is that there is a good chance that any immediate effects will disproprotionately affect the dispossessed; they are very likely right about that. Do we want, for example, a world in which rich people hire human tutors and poor people get stuck with hallucinating but cheap AI tutors that take live human teachers away from the masses? Itâ€™s a story for another day, but clearly you can have catastrophe <em>without </em>mass death, and we are heading in that direction. </p><p>Â§</p><p>One of the <a href="https://forum.effectivealtruism.org/posts/whEmrvK9pzioeircr/will-ai-end-everything-a-guide-to-guessing-or-eag-bay-area" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">most intellectually honest things I have read about the whole topic is from Katja Grace</a>, who goes through a pretty lengthy calculation on p(doom) that I wonâ€™t recount here, and then lands both on the right note of <em>humility</em> and the fact that we still have <em>agency</em>. </p><p>Iâ€™ve boldfaced the bits I am most fond of. </p><blockquote><p>In conclusion: this morning when I could think more clearly &#39;cause I wasn&#39;t standing on a stage, I thought the overall probability of doom was 19% .. <strong>but I don&#39;t think you should listen to that very much &#39;cause I might change it tomorrow or something</strong>.</p><p><strong>I think an important thing to note with many of the things in these arguments is that they could actually be changed by us doing different things</strong>. Like if we put more effort into figuring out what&#39;s going on inside AI systems, that would actually change the â€˜how likely is it that we can&#39;t make AI systems do the things we want?â€™ parameter.&#34;</p></blockquote><p>Words to live by. We arenâ€™t going to be able calculate p(doom) to three decimal places. But letâ€™s not give up on our efforts to ponder on the future just because itâ€™s hard to identify it with certainty, nor become fatalists. Letâ€™s see what we can do to maximize our chances.</p><p><a href="https://garymarcus.substack.com/p/d28?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Share</a></p><p>Gary Marcus is the co-author of <em>Rebooting AI</em>, author and editor of 6 other books, founder of two companies, CEO of ca-tai.org and host of the eight-part podcast <em>Humans vs Machines</em>. </p><p><a href="https://garymarcus.substack.com/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><a id="footnote-1" href="#footnote-anchor-1">1</a><p>In the opening of <em>Annie Hall</em>, Alvy Singer, maybe 10 years old, talks to a psychiatrist, with his mother (Mrs Singer) in the room:</p><blockquote><p><strong>Mrs. Singer:</strong><br/>He&#39;s been depressed. All of a sudden, he can&#39;t do anything.</p><p><strong>Dr. Flicker:</strong><br/>Why are you depressed, Alvy?</p><p><strong>Mrs. Singer:</strong><br/>Tell Dr. Flicker. It&#39;s something he read.</p><p><strong>Dr. Flicker:</strong><br/>Something you read, huh?</p><p><strong>Alvy Singer:</strong><br/>The universe is expanding.</p><p><strong>Dr. Flicker:</strong><br/>The universe is expanding?</p><p><strong>Alvy Singer:</strong><br/>Well, the universe is everything, and if it&#39;s expanding, someday it will break apart, and that will be the end of everything.</p><p><strong>Mrs. Singer:</strong><br/>What is that your business? He&#39;s stopped doing his homework.</p><p><strong>Alvy Singer:</strong><br/>What&#39;s the point?</p><p><strong>Mrs. Singer:</strong><br/>What has the universe got to do with it. You&#39;re here, in Brooklyn.</p></blockquote><a id="footnote-2" href="#footnote-anchor-2">2</a><p>At the opposite extreme, Yudkowsky claims that AGI will kill us all by default, because superintelligence will by default dominate us and harvest our atoms; this too seems like a rather large assumption that is far from proven, a possibility but not demonstrably a default,</p>