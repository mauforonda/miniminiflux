Published on October 5, 2023 6:34 PM GMT<br/><br/><p>ETA: I&#39;m not saying that MIRI thought AIs wouldn&#39;t understand human values. If there&#39;s only one thing you take away from this post, please don&#39;t take away that. <a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=N9ManBfJ7ahhnqmu7" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Here is Linch&#39;s attempted summary of this post</a>, which I largely agree with.</p><p>Recently, many people have talked about whether some of the main MIRI people (Eliezer Yudkowsky, Nate Soares, and Rob Bensinger<sup><a href="#fnvb0p26f2dod">[1]</a></sup>) should update on whether value alignment is easier than they thought given that GPT-4 seems to follow human directions and act within moral constraints pretty well (here are two specific examples of people talking about this: <a href="https://twitter.com/perrymetzger/status/1648116235480039424" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">1</a>, <a href="https://twitter.com/robbensinger/status/1651138126520406017" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">2</a>). Because these conversations are often hard to follow without much context, I&#39;ll just provide a brief caricature of how I think this argument has gone in the places I&#39;ve seen it, which admittedly could be unfair to MIRI<sup><a href="#fnp1cybyyf6uj">[2]</a></sup>. Then I&#39;ll offer my opinion that, overall, I think MIRI people should probably update in the direction of alignment being easier than they thought in light of this information, despite their objections.</p><p>Note: I encourage you to read this post carefully to understand my thesis. This topic can be confusing, and there are many ways to misread what I&#39;m saying. Also, make sure to read the footnotes if you&#39;re skeptical of some of my claims.</p><p>Here&#39;s my very rough caricature of the discussion so far, plus my response:</p><p><strong>Non-MIRI people:</strong> Yudkowsky talked a great deal in the sequences about how it was hard to get an AI to understand human values. For example, his essay on the <a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Hidden Complexity of Wishes</a> made it sound like it would be really hard to get an AI to understand common sense. In that essay, the genie did silly things like throwing your mother out of the building rather than safely carrying her out. Actually, it turned out that it was pretty easy to get an AI to understand common sense. LLMs are essentially safe-ish genies that do what you intend. MIRI people should update on this information.<br/><br/><strong>MIRI people (Eliezer Yudkowsky, Nate Soares, and Rob Bensinger):</strong> You misunderstood the argument. The argument was never about getting an AI to understand human values, but about getting an AI to care about human values in the first place. Hence &#39;<a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">The genie knows but doesn&#39;t care</a>&#39;. There&#39;s no reason to think that GPT-4 cares about human values, even if it can understand them. We always thought the hard part of the problem was about inner alignment, or, pointing the AI in a direction you want. We think figuring out how to point an AI in whatever direction you choose is like 99% of the problem; the remaining 1% of the problem is getting it to point at the &#34;right&#34; set of values.<sup><a href="#fnp1cybyyf6uj">[2]</a></sup></p><p><strong>My response:</strong>¬†<br/><br/>I agree that MIRI people never thought the problem was about getting AI to merely understand human values, and that they have generally maintained there was extra difficulty in getting an AI to care about human values. However, I distinctly recall MIRI people making a big deal about the <a href="https://arbital.com/p/value_identification/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">value identification problem</a> (AKA the value specification problem), for example in <a href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this 2016 talk from Yudkowsky</a>.<sup><a href="#fnd8h9qi4uiz5">[3]</a></sup>¬†The value identification problem is the problem of &#34;pinpointing valuable outcomes to an advanced agent and distinguishing them from non-valuable outcomes&#34;. In other words, it&#39;s the problem of specifying a utility function that reflects the &#34;human value function&#34; with high fidelity, i.e. the problem of specifying a utility function that can be optimized safely. See this footnote<sup><a href="#fnjovthjmofq">[4]</a></sup>¬†for further clarification about how I view the value identification/specification problem.</p><p>The key difference between the value identification/specification problem and the problem of getting an AI to understand human values is the transparency and legibility of how the values are represented: if you solve the problem of value identification, that means you have an actual function that can tell you the value of any outcome (which you could then, hypothetically, hook up to a generic function maximizer to get a benevolent AI). If you get an AI that merely understands human values, you can&#39;t necessarily use the AI to determine the value of any outcome, because, for example, the AI might lie to you, or simply stay silent.</p><p>The primary foreseeable difficulty Yudkowsky offered for the value identification problem is that <a href="https://www.lesswrong.com/tag/complexity-of-value" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">human value is complex</a>.<sup><a href="#fnimecejrosi">[5]</a></sup>¬†In turn, the idea that value is complex was stated multiple times as a premise for why alignment is hard.<sup><a href="#fndh0kj63c0st">[6]</a></sup>¬†Another big foreseeable difficulty with the value identification problem is the problem of <a href="https://arbital.com/p/edge_instantiation/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">edge instantiation</a>, which was talked about extensively in early discussions on LessWrong.</p><p>MIRI people frequently claimed that solving the value identification problem would be hard, or at least non-trivial.<sup><a href="#fn9woqm9jgal">[7]</a></sup>¬†For instance, Nate Soares wrote in his <a href="https://intelligence.org/files/ValueLearningProblem.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">2016 paper on value learning</a>, that &#34;Human preferences are complex, multi-faceted, and often contradictory. Safely extracting preferences from a model of a human would be no easy task.&#34;</p><p>I claim that GPT-4 is already pretty good at extracting preferences from human data. It exhibits common sense. If you talk to GPT-4 and ask it ethical questions, it will generally give you reasonable answers. It will also generally follow your intended directions, rather than what you literally said. Together, I think these facts indicate that GPT-4 is probably on a path towards an adequate solution to the value identification problem, where &#34;adequate&#34; means &#34;about as good as humans&#34;. And to be clear, I don&#39;t mean that GPT-4 merely passively &#34;understands&#34; human values. I mean that asking GPT-4 to distinguish valuable and non-valuable outcomes works pretty well at approximating the human value function in practice, and this will become increasingly apparent in the near future as models get more capable and expand to more modalities.<sup><a href="#fnw03nolsevv">[8]</a></sup></p><p>I&#39;m not arguing that GPT-4 actually cares about maximizing human value. However, I am saying that the system is able to transparently pinpoint to us which outcomes are good and which outcomes are bad, with fidelity approaching an average human, albeit in a text format. Crucially, GPT-4 can do this visibly to us, in a legible way, rather than merely passively knowing right from wrong in some way that we can&#39;t access. This fact is key to what I&#39;m saying because it means that, in the near future, we can literally just query multimodal GPT-N about whether an outcome is bad or good, and use that as an adequate &#34;human value function&#34;. That wouldn&#39;t solve the problem of getting an AI to care about maximizing the human value function, but it would arguably solve the problem of creating an adequate function that we can put into a machine to begin with.</p><p>In other words, I agree we haven&#39;t solved the problem of being able to point an AI in any direction we choose, but it seems like GPT-4 represents significant progress in developing an adequate target that we can point our AIs at in the first place, and previously people seemed to believe this problem would be harder than it actually was.</p><p>Maybe you think &#34;the problem&#34; was always that we can&#39;t rely on a solution to the value identification problem that only works as well as a human, and we require a much higher standard than &#34;human-level at moral judgement&#34; to avoid a catastrophe. But personally, I think having such a standard is both unreasonable and inconsistent with the implicit standard set by essays from Yudkowsky and other MIRI people. In Yudkowsky&#39;s essay on <a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the hidden complexity of wishes</a>, he wrote,</p><blockquote><p>You failed to ask for what you really wanted. ¬†You wanted your mother to go on living, but you wished for her to become more distant from the center of the building.¬†</p><p>Except that&#39;s not all you wanted.¬† If your mother was rescued from the building but was horribly burned, that outcome would rank lower in your preference ordering than an outcome where she was rescued safe and sound.¬† So you not only value your mother&#39;s life, but also her health. [...]</p><p>Your brain is not infinitely complicated; there is only a finite Kolmogorov complexity / message length which suffices to describe all the judgments you would make. ¬†But just because this complexity is finite does not make it small. ¬†We value many things, and no they are not reducible to valuing happiness or valuing reproductive fitness.</p></blockquote><p>I interpret this passage as saying that &#39;the problem&#39; is extracting all the judgements that &#34;you would make&#34;, and putting that into a wish. I think he&#39;s implying that these judgements are essentially fully contained in your brain. I don&#39;t think it&#39;s credible to insist he was referring to a hypothetical ideal human value function that ordinary humans only have limited access to, at least in this essay.<sup><a href="#fnlez4twpqdl9">[9]</a></sup></p><p>Here&#39;s another way of putting my point: In general, there are at least two ways that someone can fail to follow your intended instructions. Either your instructions aren&#39;t well-specified, or the person doesn&#39;t want to obey your instructions even if they understand what you want. Practically all the evidence that I&#39;ve found seems to indicate that MIRI people thought that both problems would be hard to solve for AI, not merely the second problem.</p><p>For example, a straightforward reading of <a href="https://intelligence.org/2017/04/12/ensuring/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Nate Soares&#39; 2017 talk</a> supports this interpretation. In the talk, Soares provides a fictional portrayal of value misalignment, drawing from the movie Fantasia. In the story, Mickey Mouse attempts to instruct a magical broom to fill a cauldron, but the broom follows the instructions literally rather than following what Mickey Mouse intended, and floods the room. Soares comments: &#34;I claim that as fictional depictions of AI go, this is pretty realistic.&#34;<sup><a href="#fnis1nkrdupa">[10]</a></sup></p><p>Perhaps more important to my point, Soares presented a clean separation between the part where we specify an AI&#39;s objectives, and the part where the AI tries to maximizes those objectives. He draws two arrows, indicating that MIRI is concerned about both parts. He states, &#34;My view is that the critical work is mostly in designing an effective value learning process, and in ensuring that the sorta-argmax process is correctly hooked up to the resultant objective function ùó®:&#34;<sup><a href="#fn9scxdnc5aoo">[11]</a></sup></p><figure><img src="https://reader.miniflux.app/proxy/6jxYk9PDvYbKUDKGYMB5CEBVr3LVYAeCk01XNd1NeVo=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9kNHVqeW5nbHV0a2FjcHpnbHZjcg==" srcset="https://reader.miniflux.app/proxy/Jt9zyTvk8MnyRbnUOSsLoM3WAhcJ4oX_hz-GFZKBpmA=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9jbHJibHYyeDgyMTlvNDVzaXVxaA== 100w, https://reader.miniflux.app/proxy/gEGgwN09CWAKy5oZTrGxbGTC4VGIwNJoC0eVbeL6_B4=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS90OWl2d25reXZocW5jNmc3cXJ1aA== 200w, https://reader.miniflux.app/proxy/Gx1wl24aGa29BMtKwh5y6JwxEdw_P0xx-fT18skXaS8=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS90OGdtd2NvYnNzc2p1eDJ6aW11NA== 300w, https://reader.miniflux.app/proxy/lmSZnUG3DtYBB6P97fdjEOGmwUedKnnEVC1uZ93aT3w=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS91dWg1bXRrMWZxeXVwYnBsb29iaQ== 400w, https://reader.miniflux.app/proxy/F6fhj5aBy74Qpr9XJN-9mNMQeGoRVk0GAjuj6IX1w0U=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS90eHBnMzBzbDNma2dwbHZkdWVncg== 500w, https://reader.miniflux.app/proxy/IgYFfPHZiEznUDQWocTCBcgyFJ74bsyTaazMt4CNmAg=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9oMHBvdXgycmNnajN2bDM0MHlpdQ== 600w, https://reader.miniflux.app/proxy/3a59tMZRMx33n0XunkGunkd60RVg9AND4HgojiYDLrc=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9naXR5YzU3ZXVkZ2hkZHU1cHJ0cQ== 700w, https://reader.miniflux.app/proxy/N6A5DhkIIBECdqWAl5MtmViEAYSZ_dFQ6x9lkP0XIjI=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9rbnB0amNsMHN6cW41YzFpbmM1dw== 800w, https://reader.miniflux.app/proxy/tShu11hadpqZcFDQKxVigegKsX1xQB8MfLQlpajwwco=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS90d3l4a3g5Y3VibGh2NGY1NHk4MQ== 900w, https://reader.miniflux.app/proxy/w2StHPUQWPnIcIAxzhDiZknu7W9Qyh7p_w8t5Mnzuv0=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9rMGJ2dDFjemhqeHI5a2F5c3V2Zg== 975w" loading="lazy"/></figure><p>In the talk Soares also says, &#34;The serious question with smarter-than-human AI is how we can ensure that the objectives we‚Äôve specified are correct, and how we can minimize costly accidents and unintended consequences in cases of misspecification.&#34; I believe this quote refers directly to the value identification problem, rather than the problem of getting an AI to care about following the goals we&#39;ve given it. This attitude is reflected in other MIRI essays.</p><p>The point of &#34;<a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the genie knows but doesn&#39;t care</a>&#34; wasn&#39;t that the AI would take your instructions, know what you want, and yet disobey the instructions because it doesn&#39;t care about what you asked for. If you read <a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Rob Bensinger&#39;s essay</a> carefully, you&#39;ll find that he&#39;s actually warning that the AI will care too much about the utility function you gave it, and maximize it exactly, against your intentions<sup><a href="#fnuahq0ks4gl">[12]</a></sup>. The sense in which the genie &#34;doesn&#39;t care&#34; is that it doesn&#39;t care what you intended; it only cares about the objectives that you gave it. That&#39;s not the same as saying the genie doesn&#39;t care about the objectives you specified.</p><p>Given the evidence, it seems to me that the following conclusions are probably accurate:</p><ol><li>The fact that GPT-4 can reliably follow basic instructions, is able to distinguish moral from immoral actions somewhat reliably, and generally does what I intend rather than what I literally asked, is all evidence that the value identification problem is easier than how MIRI people originally portrayed it. While I don&#39;t think the value identification problem has been completely solved yet, I don&#39;t expect near-future AIs will fail dramatically on the &#34;fill a cauldron&#34; task, or any other functionally similar tasks.</li><li>MIRI people used to think that it would be hard to both (1) specify an explicit function that corresponds to the &#34;human value function&#34; with fidelity comparable to the judgement of an average human, and (2) separately, get an AI to care about maximizing this function. The idea that MIRI people only ever thought (2) was the hard part appears false.<sup><a href="#fnaj3oawf86mb">[13]</a></sup></li><li>Non-MIRI people sometimes strawman MIRI people as having said that AGI would literally lack an understanding of human values. I don&#39;t endorse this, and I&#39;m not saying this.</li><li>The &#34;complexity of value&#34; argument pretty much just tells us that we need an AI to learn human values, rather than hardcoding a utility function from scratch. That&#39;s a meaningful thing to say, but it doesn&#39;t tell us much about whether alignment is hard, especially in the deep learning paradigm; it just means that extremely naive approaches to alignment won&#39;t work.</li></ol><p>As an endnote, I don&#39;t think it really matters whether MIRI people had mistaken arguments about the difficulty of alignment ten years ago. It matters far more what their arguments are right now. However, I do care about accurately interpreting what people said on this topic, and I think it&#39;s important for people to acknowledge when the evidence has changed.</p><ol><li id="fnvb0p26f2dod"><sup><strong><a href="#fnrefvb0p26f2dod">^</a></strong></sup><p>I recognize that these people are three separate individuals and each have their own nuanced views. However, I think each of them have expressed broadly similar views on this particular topic, and I&#39;ve seen each of them engage in a discussion about how we should update about the difficulty of alignment given what we&#39;ve seen from LLMs.</p></li><li id="fnp1cybyyf6uj"><sup><strong><a href="#fnrefp1cybyyf6uj">^</a></strong></sup><p>I&#39;m not implying MIRI people would necessarily completely endorse everything I&#39;ve written in this caricature. I&#39;m just conveying how they&#39;ve broadly come across to me, and I think the basic gist is what&#39;s important here. If some MIRI people tell me that this caricature isn&#39;t a fair summary of what they&#39;ve said, I&#39;ll try to edit the post later to include real quotes.</p><p>For now, I&#39;ll point to <a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this post from Nate Soares</a> in which he stated,</p><blockquote><p>I have long said that the lion&#39;s share of the AI alignment problem seems to me to be about¬†pointing powerful cognition at anything at all, rather than¬†figuring out what to point it at.</p><p>It‚Äôs recently come to my attention that some people have misunderstood this point, so I‚Äôll attempt to clarify here.</p></blockquote></li><li id="fnd8h9qi4uiz5"><sup><strong><a href="#fnrefd8h9qi4uiz5">^</a></strong></sup><p>More specifically, in <a href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the talk</a>, at one point Yudkowsky asks &#34;Why expect that [alignment] is hard?&#34; and goes on to tell a fable about programmers misspecifying a utility function, which then gets optimized by an AI with disastrous consequences. My best interpretation of this part of the talk is that he&#39;s saying the value identification problem is one of the primary reasons why alignment is hard. However, I encourage you to read <a href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the transcript</a> yourself if you are skeptical of my interpretation.</p></li><li id="fnjovthjmofq"><sup><strong><a href="#fnrefjovthjmofq">^</a></strong></sup><p>I am mainly talking about the problem of how to specify (for example, write into a computer) an explicit function that reflects the human value function with high fidelity, in the sense that judgements from this function about the value of outcomes fairly accurately reflect the judgements of ordinary humans. I think this is simply a distinct concept from the idea of getting an AI to understand human values.</p><p>I was not able to find a short and crisp definition of the value identification/specification problem from MIRI. However, in the Arbital page on the <a href="https://arbital.com/p/updated_deference/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Problem of fully updated deference</a>, the problem is described as follows,</p><blockquote><p>One way to look at the central problem of value identification in superintelligence is that we&#39;d ideally want some function that takes a complete but purely physical description of the universe, and spits out our true intended notion of value V in all its glory. Since superintelligences would probably be pretty darned good at collecting data and guessing the empirical state of the universe, this probably solves the whole problem.</p><p>This is not the same problem as writing down our true V by hand. The minimum algorithmic complexity of a meta-utility function ŒîU which outputs V after updating on all available evidence, seems plausibly much lower than the minimum algorithmic complexity for writing V down directly. But as of 2017, nobody has yet floated any formal proposal for a ŒîU of this sort which has not been immediately shot down.</p></blockquote><p>In MIRI&#39;s <a href="https://intelligence.org/files/TechnicalAgenda.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">2017 technical agenda</a>, they described the problem as follows, which I believe roughly matches how I&#39;m using the term,</p><blockquote><p>A highly-reliable, error-tolerant agent design does not guarantee a positive impact; the effects of the system still depend upon whether it is pursuing appropriate goals. A superintelligent system may find clever, unintended ways to achieve the specific goals that it is given. Imagine a superintelligent system designed to cure cancer which does so by stealing resources, proliferating robotic laboratories at the expense of the biosphere, and kidnapping test subjects: the intended goal may have been ‚Äúcure cancer without doing anything bad,‚Äù but such a goal is rooted in cultural context and shared human knowledge.¬†</p><p>It is not sufficient to construct systems that are smart enough to figure out the intended goals. Human beings, upon learning that natural selection ‚Äúintended‚Äù sex to be pleasurable only for purposes of reproduction, do not suddenly decide that contraceptives are abhorrent. While one should not anthropomorphize natural selection, humans are capable of understanding the process which created them while being completely unmotivated to alter their preferences. For similar reasons, when developing AI systems, it is not sufficient to develop a system intelligent enough to figure out the intended goals; the system must also somehow be deliberately constructed to pursue them (Bostrom 2014, chap. 8).¬†</p><p>However, the ‚Äúintentions‚Äù of the operators are a complex, vague, fuzzy, context-dependent notion (Yudkowsky 2011; cf. Sotala and Yampolskiy 2017). Concretely writing out the full intentions of the operators in a machine-readable format is implausible if not impossible, even for simple tasks. An intelligent agent must be designed to learn and act according to the preferences of its operators.6 This is the value learning problem.¬†</p><p>Directly programming a rule which identifies cats in images is implausibly difficult, but specifying a system that inductively learns how to identify cats in images is possible. Similarly, while directly programming a rule capturing complex human intentions is implausibly difficult, intelligent agents could be constructed to inductively learn values from training data.</p></blockquote></li><li id="fnimecejrosi"><sup><strong><a href="#fnrefimecejrosi">^</a></strong></sup><p>To support this claim, I&#39;ll point out that <a href="https://arbital.com/p/value_identification/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the Arbital page for the value identification problem</a> says, &#34;A central foreseen difficulty of value identification is <a href="https://arbital.com/p/complexity_of_value/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Complexity of Value</a>&#34;.</p></li><li id="fndh0kj63c0st"><sup><strong><a href="#fnrefdh0kj63c0st">^</a></strong></sup><p>For example, in <a href="https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this post</a>, Yudkowsky gave &#34;five theses&#34;, one of which was the &#34;complexity of value thesis&#34;. He wrote, that the &#34;five theses seem to imply two important lemmas&#34;, the first lemma being &#34;Large bounded extra difficulty of Friendliness.&#34;, i.e. the idea that alignment is hard.</p><p>Another example comes from <a href="https://www.youtube.com/watch?v=Uoda5BSj_6o&amp;t=1604s" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">this talk</a>. I&#39;ve linked to a part in which Yudkowsky begins by talking how human value is complex, and moves to talking about how that fact presents challenges for aligning AI.</p></li><li id="fn9woqm9jgal"><sup><strong><a href="#fnref9woqm9jgal">^</a></strong></sup><p>My guess is that the perceived difficulty of specifying objectives was partly a result of MIRI people expecting that natural language understanding wouldn&#39;t occur in AI until just barely before AGI, and at that point it would be too late to use AI language comprehension to help with alignment.</p><p>Rob Bensinger <a href="https://twitter.com/robbensinger/status/1651148171677171713" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">said</a>,</p><blockquote><p>It&#39;s true that Eliezer and I didn&#39;t predict AI would achieve GPT-3 or GPT-4 levels of NLP ability so early (e.g., before it can match humans in general science ability), so this is an update to some of our models of AI.</p></blockquote><p>In 2010, Eliezer Yudkowsky <a href="https://www.lesswrong.com/posts/zPFojkLmiMJadaBCr/existential-risk-and-public-relations?commentId=bDKzZwd28JxznnSue" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">commented</a>,</p><blockquote><p>&gt; I think controlling Earth&#39;s destiny is only modestly harder than understanding a sentence in English.</p><p>Well said. I shall have to try to remember that tagline.</p></blockquote></li><li id="fnw03nolsevv"><sup><strong><a href="#fnrefw03nolsevv">^</a></strong></sup><p>If you disagree that AI systems in the near-future will be capable of distinguishing valuable from non-valuable outcomes about as reliably as humans, then I may be interested in operationalizing this prediction precisely, and betting against you. I don&#39;t think this is a very credible position to hold as of 2023, barring a pause that could slow down AI capabilities very soon.</p></li><li id="fnlez4twpqdl9"><sup><strong><a href="#fnreflez4twpqdl9">^</a></strong></sup><p>I mostly interpret Yudkowsky&#39;s <a href="https://intelligence.org/files/CEV.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Coherent Extrapolated Volition</a> as an aspirational goal for what we could best hope for in an ideal world where we solve every part of alignment, rather than a minimal bar for avoiding human extinction. In Yudkowsky&#39;s <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">post on AGI ruin</a>, he stated,</p><blockquote><p>When I say that alignment is lethally difficult, I am not talking about ideal or perfect goals of &#39;provable&#39; alignment, nor total alignment of superintelligences on exact human values, nor getting AIs to produce satisfactory arguments about moral dilemmas which sorta-reasonable humans disagree about, nor attaining an absolute certainty of an AI not killing everyone.¬† When I say that alignment is difficult, I mean that in practice, using the techniques we actually have, &#34;please don&#39;t disassemble literally everyone with probability roughly 1&#34; is an overly large ask that we are not on course to get.</p></blockquote></li><li id="fnis1nkrdupa"><sup><strong><a href="#fnrefis1nkrdupa">^</a></strong></sup><p>I don&#39;t think I&#39;m taking him out of context. Here&#39;s a longer quote from <a href="https://intelligence.org/2017/04/12/ensuring/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the talk</a>,</p><blockquote><p>When Mickey runs this program, everything goes smoothly at first. <a href="https://www.youtube.com/watch?v=UEYy3osi8Gs&amp;t=3m25s" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Then</a>:</p><p>[Image of the cauldron overflowing with water]</p><p>I claim that as fictional depictions of AI go, this is pretty realistic.</p><p>Why would we expect a generally intelligent system executing the above program to start overflowing the cauldron, or otherwise to go to extreme lengths to ensure the cauldron is full?</p><p>The first difficulty is that the objective function that Mickey gave his broom left out <a href="https://intelligence.org/files/ComplexValues.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a bunch of other terms</a> Mickey cares about.</p><figure><img src="https://reader.miniflux.app/proxy/bBUmAVUxZj1cmRawDRUFWmdT9yhBLg6LNTsb6lEVbok=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9oZGg2b2dmb3FtdTlxcGFrMGRyZQ==" srcset="https://reader.miniflux.app/proxy/_DImr3tFlvGma78ZPDxO_fZLRRCy7_gl5DL0WAAH-jE=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9rcWN3MGY0cHdqeGoyaHFibGcwbw== 170w, https://reader.miniflux.app/proxy/m6zSeo8DM9DcEYmObbzS3L-vz7Bjm0w3I9QX1foOxv4=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9tbHJxM2h1czVhZW14eHRid3B5bA== 340w, https://reader.miniflux.app/proxy/HYjRpsoiSaijK9cHyjKohu_q_5eelDk4oMHuyeZtg6I=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS91aHZpbHVxc21hb2podXFjMHJ4NA== 510w, https://reader.miniflux.app/proxy/hJPAKYTVILqjQ4Dor7LfSShFgA9cF73Pqll2BDe6maY=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9ud3N6d3hraHQ0d3FmaGJnbDZ4eA== 680w, https://reader.miniflux.app/proxy/4KUKfNjQ8GUN_iEDIXfcSrvkRnJ9ixC3UZv_GxlpBOQ=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9zeGZjYzJnNGltY3RqaGRtaW9icw== 850w, https://reader.miniflux.app/proxy/IYUYaebIi4S27NsgAZ8tTLs1M-ccmYMF2VEM69bZSNs=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9meGZzeXVmenBodnd4Z3p4dmw4Yg== 1020w, https://reader.miniflux.app/proxy/2QjWMQZbfYQFtookfcadsX58nrpvhjiZv8iSjitFfXA=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS90dnJleGhqdmxmNm9sbXBldjZvNQ== 1190w, https://reader.miniflux.app/proxy/A3LYfne3WB2eU-Qly3g5x7ujKNdmbv27lGvS4cjov6Q=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9tdnZrb2cyNGxoMnZkczJranVraw== 1360w, https://reader.miniflux.app/proxy/E_aDe56_Esk1J2z3zPI-fG9mBHp3hP39VCDSn4K8aSQ=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9mYTVxbHl2eXZrODV4MnB5aGR0aw== 1530w, https://reader.miniflux.app/proxy/j_0kIZjo94p2cQigF71h4S-4xTBVlrfl1lrhAQtaGBU=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9mYWNjdTdzb2ZxZXhoZHFhOHBzMw== 1681w" loading="lazy"/></figure></blockquote></li><li id="fn9scxdnc5aoo"><sup><strong><a href="#fnref9scxdnc5aoo">^</a></strong></sup><p>The full quote is,</p><blockquote><p>Another common thread is ‚ÄúWhy not just tell the AI system to (insert intuitive moral precept here)?‚Äù On this way of thinking about the problem, often (perhaps unfairly) associated with Isaac Asimov‚Äôs writing, ensuring a positive impact from AI systems is largely about coming up with natural-language instructions that are vague enough to subsume a lot of human ethical reasoning:</p><p><img src="https://reader.miniflux.app/proxy/0s6aEayeVfF_JgMGLgu9ReMhkXpOklauFStjN0E6tbY=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9pNWtpamNqRkpENmJuN2R3cS9zOWEzaGFxbWR2bmFxeHppbWhpbw==" alt="intended-values" loading="lazy"/></p><p>In contrast, precision is a virtue in real-world safety-critical software systems. Driving down accident risk requires that we begin with limited-scope goals rather than trying to ‚Äúsolve‚Äù all of morality at the outset.<a href="https://intelligence.org/2017/04/12/ensuring/#footnote_4_15407" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><sup>5</sup></a></p><p>My view is that the critical work is mostly in designing an effective value learning process, and in ensuring that the sorta-argmax process is correctly hooked up to the resultant objective function ùó®:</p><p><img src="https://reader.miniflux.app/proxy/A5uU_wofTIsqoNXdbyaJFQNztCeB0_ZRpwjRCbStGjE=/aHR0cHM6Ly9yZXMuY2xvdWRpbmFyeS5jb20vbGVzc3dyb25nLTItMC9pbWFnZS91cGxvYWQvZl9hdXRvLHFfYXV0by92MS9taXJyb3JlZEltYWdlcy9uWTVqQXNkQUZTUkZuM1pyZC9tcm1ucWlqd2VkYmdmZWh6bWp0cg==" alt="vl-argmax.png" loading="lazy"/></p><p>The better your value learning framework is, the less explicit and precise you need to be in pinpointing your value function ùòù, and the more you can offload the problem of figuring out what you want to the AI system itself. Value learning, however, raises <a href="https://intelligence.org/files/ValueLearningProblem.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a number of basic difficulties</a> that don‚Äôt crop up in ordinary machine learning tasks.</p></blockquote></li><li id="fnuahq0ks4gl"><sup><strong><a href="#fnrefuahq0ks4gl">^</a></strong></sup><p>This interpretation appears supported by the following quote from <a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Rob Bensinger&#39;s essay</a>,</p><blockquote><p>When you write the seed&#39;s utility function, you, the programmer,¬†don&#39;t understand everything about the nature of human value or meaning. That imperfect understanding remains¬†the causal basis of the fully-grown superintelligence&#39;s actions, long after it&#39;s become smart enough to fully understand our values.</p><p>Why¬†is the superintelligence, if it&#39;s so clever, stuck with whatever meta-ethically dumb-as-dirt utility function we gave it at the outset? Why can&#39;t we just pass the fully-grown superintelligence the buck by instilling in the seed the instruction: &#39;When you&#39;re smart enough to understand Friendliness Theory, ditch the values you started with and just self-modify to become Friendly.&#39;?</p><p>Because that sentence has to actually be coded in¬†to the AI, and when we do so, there&#39;s no <a href="https://www.lesswrong.com/lw/rf/ghosts_in_the_machine/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">ghost in the machine</a> to know exactly what we mean by &#39;frend-lee-ness thee-ree&#39;.</p></blockquote></li><li id="fnaj3oawf86mb"><sup><strong><a href="#fnrefaj3oawf86mb">^</a></strong></sup><p>It&#39;s unclear to me whether MIRI people are claiming that they only ever thought (2) was the hard part of alignment, but <a href="https://www.lesswrong.com/posts/87EzRDAHkQJptLthE/but-why-would-the-ai-kill-us?commentId=ApRHmrMCkkjKuFEnf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here&#39;s a quote from Nate Soares</a> that offers some support for this interpretation IMO,</p><blockquote><p>I&#39;d agree that one leg of possible support for this argument (namely &#34;humanity will be completely foreign to this AI, e.g. because it is a mathematically simple seed AI that has grown with very little exposure to humanity&#34;) won&#39;t apply in the case of LLMs. (I don&#39;t particularly recall past people arguing this; my impression is rather one of past people arguing that of course the AI would be able to read wikipedia and stare at some humans and figure out what it needs to about this &#39;value&#39; concept, but the hard bit is in making it care.</p></blockquote><p>Even if I&#39;m misinterpreting Soares here, I don&#39;t think that would undermine the basic point that MIRI people should probably update in the direction of alignment being easier than they thought.</p></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>