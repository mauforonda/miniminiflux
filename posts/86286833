Published on October 9, 2023 9:54 AM GMT<br/><br/><p>This is a quick response to<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"> Evolution Provides No Evidence For the Sharp Left Turn</a>, due to it winning first prize in<a href="https://www.openphilanthropy.org/research/announcing-the-winners-of-the-2023-open-philanthropy-ai-worldviews-contest/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"> The Open Philanthropy Worldviews contest</a>. I think part of the post is sufficiently misleading about evolutionary history and the OP first prize gives it enough visibility, that it makes sense to write a post-long response.<br/><br/>Central evolutionary biology related claim of the original post is this:</p><blockquote><ul><li>The animals of the generation learn throughout their lifetimes, collectively performing many billions of steps of learning.</li><li>The generation dies, and all of the accumulated products of within lifetime learning are lost.</li><li>Differential reproductive success slightly changes the balance of traits across the species.</li></ul><p>…<br/><br/>The only way to transmit information from one generation to the next is through evolution changing genomic traits, because death wipes out the within lifetime learning of each generation.<br/><br/>…<br/><br/>However, this sharp left turn does not occur because the inner learning processes suddenly become much better / more foomy / more general in a handful of outer optimization steps. It happens because you devoted billions of times more optimization power to the inner learning processes, but then deleted each inner learner shortly thereafter. Once the inner learning processes become able to pass non-trivial amounts of knowledge along to their successors, you get what looks like a sharp left turn. But that sharp left turn only happens because the inner learners have found a kludgy workaround past the crippling flaw where they all get deleted shortly after initialization.</p></blockquote><p>In my view, this interpretation of evolutionary history is something between &#34;speculative&#34; and &#34;wrong&#34;.<br/><br/>Transmitting some of the data gathered during the lifetime of the animal to next generation by some other means is so obviously useful that is it highly<a href="https://www.lesswrong.com/posts/sam4ehxHgnJEGCKed/lessons-from-convergent-evolution-for-ai-alignment" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"> convergent</a>. Non-genetic communication channels to the next generation include epigenetics, parental teaching / imitation learning, vertical transmission of symbionts, parameters of prenatal environment, hormonal and chemical signaling, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048089/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">bio-electric signals</a>, and transmission of environmental resources or modifications created by previous generations, which can shape the conditions experienced by future generations (e.g. beaver dams). <br/><br/>Given the fact overcoming the genetic bottleneck is so highly convergent, it seems a bit surprising if there was a large free lunch on table in exactly this direction, as Quintin assumes:</p><blockquote><p>Evolution&#39;s sharp left turn happened because evolution spent compute in a shockingly inefficient manner for increasing capabilities, leaving vast amounts of free energy on the table for any self-improving process that could work around the evolutionary bottleneck. Once you condition on this specific failure mode of evolution, you can easily predict that humans would undergo a sharp left turn at the point where we could pass significant knowledge across generations. I don&#39;t think there&#39;s anything else to explain here, and no reason to suppose some general tendency towards extreme sharpness in inner capability gains.</p></blockquote><p><br/>It&#39;s probably worth to go a bit into technical details here: evolution did manage to discover evolutionary innovations like mirror neurons: A mirror neuron is a neuron that fires both when an organism acts and when the organism observes the same action performed by another. Thus, the neuron &#34;mirrors&#34; the behavior of the other, as though the observer were itself acting. ... Further experiments confirmed that about 10% of neurons in the monkey inferior frontal and inferior parietal cortex have &#34;mirror&#34; properties and give similar responses to performed hand actions and observed actions.<sup><a href="#fncgipi135eef">[1]</a></sup> <br/><br/>Clearly, mirror neurons are type of an innovation which allows high throughput behavioural cloning / imitation learning. &#34;10% of neurons in the monkey inferior frontal and inferior parietal cortex&#34; is a massive amount of compute. Neurons imitating your parent&#39;s motoric policy based on visual channel information about the behaviour of your parent is a high-throughput channel. (I recommend doing a Fermi estimate of this channel capacity).<br/><br/>The situation where you clearly have a system totally able to eat the free lunch on the table, and supposedly the lunch is still there, makes me suspicious.<br/><br/>At the same time: yes, clearly, nowadays, human culture is a lot of data, and humans learn more than monkeys.</p><h3>Different stories</h3><p>What are some evolutionary plausible alternatives of Quintin&#39;s story? <br/><br/>Alternative stories would usually suggest that ancestral humans had access to channels to overcome the genetic bottleneck, and were using such channels to the extent it was marginally effective. Then, some other major change happened, the marginal fitness advantage of learning more grew, and humans developed to transmit more bits, so, modern humans transmit more.<br/><br/>An example of such major change could be advent of culture. If you look at the past timeline from a replicator dynamics perspective, the next most interesting event after the beginning of life is cultural replicators running on human brains crossing R&gt;1 and starting  the second vast evolutionary search, cultural evolution.<br/><br/>How is the story &#34;cultural evolution is the pivotal event&#34; different? Roughly speaking, culture is a multi-brain parallel immortal evolutionary search computation. Running at higher speed and a layer of abstraction away from physical reality (compared to genes), it was able to discover many pools of advantage, like fire, versatile symbolic communication, or specialise-and-trade superagent organisation.<br/><br/>In this view, there is a type difference between &#39;culture&#39; and &#39;increased channel capacity&#39;. <br/><br/>You can interpret this in multiple ways, but if you want to cast this as a story of a discontinuity, where biological evolution randomly stumbled upon starting a different powerful open-ended misaligned search, it makes sense. The fact that such search finds caches of fitness and negentropy seems not very surprising. <sup><a href="#fn6cir8b7m1yn">[2]</a></sup><br/><br/>Was the &#34;increased capacity to transfer what&#39;s learned in brain&#39;s lifetime to the next generation&#34; at least the most important or notably large direction what to exploit? I&#39;m not a specialist on human evolution, but seems hard to say with confidence: note that &#39;fire&#39; is also a big deal, as it allows you do spend way less on digestion, and cheaper ability to coordinate is a big deal, as illustrated by ants, and symbolic communication is a big deal, as it is digital, robust and and effective compression.<br/><br/>Unfortunately for attempts to figure out what were the precise marginal costs and fitness benefits for ancestral humans, my impression is, ~ten thousand generations of genetic evolution in a fitness landscape shaped by cultural evolution screens a lot of evidence. In particular, from the fact modern humans are outliers in some phenotype characteristic, you can not infer it was the cause of the change to humans. For example, argument like &#39;human kids have unusual capacity to absorb significant knowledge across generations compared to chimps, ergo, the likely cause of human explosive development is ancestral humans having more of this capacity than other species&#39; has very little weight. Modern wolfs are also notably different from modern chihuahuas, but the correct causal story is not &#39;ancestral chihuahuas had an overhang of loyalty and harmlessness&#39;.<br/><br/>Does this partially invalidate the argument toward implications for AI in the original post? In my view yes; if, following Quintin, we translate the actual situation  into quantities and narratives that drive AI progress rates<br/><br/>- the &#34;specific failure mode&#34; of not transmitting what brains learn to the next generation is not there<br/>- the marginal fitness advantage of transmitting more bits to the next generation brains is unclear, similarly to an unclear marginal advantage of e.g. spending more on LLMs curating data for the next gen LLM training<br/>- because we don&#39;t really understand what happened, the metaphorical map to AI progress mostly maps this lack of understanding to lack of clear insights for AI<br/>- it seems likely culture is somehow big deal, but it is not clear how you would translate what happened to AI domain; if such thing can happen with AIs, if anything, it seems pushing more toward the discontinuity side, as the cultural search uncovered relatively fast multiple to many caches of negentropy<br/>(- yes, obviously, given culture, it is important that you can transmit it to next generation, but it seems quite possible that for transferring seed culture  the capacity channel you have via mirror neurons is more than enough)<br/> </p><h3>Not even approximately true</h3><p>In case you still believe the original post is still somehow approximately true, and the implications for AI progress still somehow approximately hold, I think it&#39;s important to basically un-learn that update. Quoting the original post:</p><blockquote><p>This last paragraph makes an extremely important claim that I want to ensure I convey fully:</p><p>- IF we understand the mechanism behind humanity&#39;s sharp left turn with respect to evolution</p><p>- AND that mechanism is inapplicable to AI development</p><p>- THEN, there&#39;s no reason to reference evolution at all when forecasting AI development rates, not as evidence for a sharp left turn, not as an &#34;illustrative example&#34; of some mechanism / intuition which might supposedly lead to a sharp left turn in AI development, not for anything.<br/> </p></blockquote><p>The conjunctive IF is a crux, and because <strong>we don&#39;t understand what happened with culture enough, the rest of the implication does not hold</strong>.<br/><br/>Consider a toy model counterfactual story: in a fantasy word, exactly repeating 128 bits of the first cultural replicator gives the human ancestor the power to cast a spell and gain +50% fitness advantage.  Notice that this is a different story from &#34;overcoming channel to offspring capacity&#34; - you may be in the situation you have plenty of capacity, but don&#39;t have the 128 bits, and this is a situation much more prone to discontinuities.<br/><br/>Because it is not clear if reality was more like stumbling upon a specific string, or piece of code, or evolutionary ratchet, or something else, we don&#39;t know enough to rule out a metaphor suggesting discontinuities.  </p><h3>Conclusion</h3><p>Where I do agree with Quintin is scepticism toward some other stories attempting to draw some strong conclusion from human evolution, including strong conclusions about discontinuities.<br/><br/>I do think there is a reasonably good metaphor genetic evolution : brains ~ base optimiser : mesa-optimiser, but notice that evolution was able to keep brains mostly aligned for all other species except humans.  Relation human brain : cultural evolution is very unlike base optimiser : mesa-optimiser. </p><h3>(Note on AI)</h3><p>While I mostly wanted to focus on the evolutionary part of the OP, I&#39;m sceptical about the AI claims too. (Paraphrasing: While the current process of AI training is not perfectly efficient, I don&#39;t think it has comparably sized overhangs which can be exploited easily.)</p><p>In contrast, to me, it seems current way how AIs learn is very obviously inefficient, compared to what&#39;s possible. For example, explain to GPT4 something new, or make it derive something new. Open a new chat window, and probe if it now knows it. Compare with a human.<br/> </p><ol><li id="fncgipi135eef"><sup><strong><a href="#fnrefcgipi135eef">^</a></strong></sup><p><a href="https://en.wikipedia.org/wiki/Mirror_neuron" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">from Wikipedia</a></p></li><li id="fn6cir8b7m1yn"><sup><strong><a href="#fnref6cir8b7m1yn">^</a></strong></sup><p>This does not imply the genetic evolutionary search is a particularly bad optimiser - instead, the landscape is such that there are many sources of negentropy available.</p></li></ol><br/><br/><a href="https://www.alignmentforum.org/posts/wCtegGaWxttfKZsfx/we-don-t-understand-what-happened-with-culture-enough#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>