Published on October 7, 2023 12:35 AM GMT<br/><br/><p>I remember that when I first started in alignment there was this belief that in order to make alignment progress, you needed to be a genius. With the rise of interpretability, this expectation has moderated somewhat, but I still think it has influence and often leads people to overcomplicate.</p><p>Many recent alignment breakthroughs have been found by keeping it simple<sup><a href="#fn1v3hwr53kci">[1]</a></sup>:</p><p><strong>Lie Detection</strong>:<br/>• <a href="https://arxiv.org/abs/2212.03827" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Contrast-consistent knowledge</a> (better known as discovering latent knowledge) reads the truth out of a model without looking at the outputs, simply by using a linear probe with only two elements in its activation function<sup><a href="#fn2v4ey0p83c9">[2]</a></sup>.<br/>• <a href="https://arxiv.org/abs/2309.15840" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Pacchiari, Owain and others</a> use a logistic regression classifier<sup><a href="#fn2dyeooknd0h">[3]</a></sup> to predict whether a model was truthful from its answers to further questions. They were able to demonstrate this using three different kinds of questions and the results even generalised across models. This technique seems like it could be useful in practise, even if it doesn&#39;t scale all the way to superintelligence. <br/><br/><strong>Superposition</strong>: <a href="https://transformer-circuits.pub/2023/monosemantic-features/#why-not-architectures" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Dictionary learning</a> makes substantial progress on superposition by simply using a one hidden layer multi-layer perceptron as a sparse auto-encoder with L2 reconstruction loss and a L1 loss on hidden layer activations. Anthropic actually chose to avoid more sophisticated methods, as they were worried they might recover features that the model doesn&#39;t actually utilise.<br/><br/><strong>Model-steering</strong>: Activation Engineering: <a href="https://www.lesswrong.com/posts/HWxLQvzJGeXoLPJWd/actadd-steering-language-models-without-optimization" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Activation addition</a> allows us to steer models by simply performing a forward pass for the concept you want to activate, saving the activations of a particular layer as a vector and adding them to future forward pass. See also Turntrout&#39;s prior work on steering a mouse by <a href="https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">subtracting a cheese vector</a><sup><a href="#fnvt1x0nq32oh">[4]</a></sup> and <a href="https://arxiv.org/abs/2309.15840" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">inference-time intervention</a><sup><a href="#fngwv0u8dqysq">[5]</a></sup>.<sup><a href="#fncv4l100bvy">[6]</a></sup><br/><br/><strong>World model location</strong>: Neel Nanda found a linear representation in <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Othello-GPT</a>. This one was a bit trickier: a previous paper had failed to recover it using <a href="https://thegradient.pub/othello/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">linear probes</a>. However, Neel managed to do this by searching for a representation of &#34;my color&#34; and by applying a bunch of other tweaks. See also <a href="https://twitter.com/wesg52/status/1709551516577902782" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Wes Gurnee&#39;s work with Max Tegmark</a>, which used linear probes to find linear representations of latitude and longitude; in addition to one representing time<sup><a href="#fnzynaaklp5vl">[7]</a></sup>.<br/><br/><strong>Scalable Interpretability: </strong>OpenAI used a language model in order to label neurons based on the activations for some relevant examples using <a href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">few-shot prompting</a>.</p><p><strong>Understanding Implications</strong><sup><a href="#fnky6wdd05sjn">[8]</a></sup>: For a long time it appeared as though it would be <a href="https://medium.com/nautilus-magazine/scary-ai-is-more-fantasia-than-terminator-ac1546b35bfc" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">challenging to prevent an AI from taking our instructions too literally</a>. Essentially what seems to have happened is that they decided to just use RL on a generative base model and the problem solved itself. <a href="https://huggingface.co/blog/rlhf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Reinforcement Learning from Human Feedback</a> has some complexity, but I still think it&#39;s quite notable than OpenAI, as far as I can tell, basically one-shotted this with InstructGPT<sup><a href="#fneduvymbukau">[9]</a></sup>. It has some complexity - proximal policy optimization isn&#39;t simple - but it wasn&#39;t a <a href="https://arxiv.org/abs/1707.06347" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new algorithm</a> and even if they hadn&#39;t gotten it first time, I&#39;d bet that we were always going to end up there by iterating on a baseline.<br/><br/>I don&#39;t want to dismiss the challenges of such research. Producing a result requires getting a lot of details right. Actually carrying out one of these projects would involve a massive amount of work. Even picking the right question to investigate requires a lot of skill. However, simpler techniques have gone further for these problems than I would have originally expected.</p><p>I think it&#39;s worthwhile speculating why reasonably simple solutions have worked for these problems, and checking whether there are any similarities between them.</p><p>Firstly, I think it&#39;s worth noting that all of these techniques except for the last two look for linear representations in neural networks. Beren has made a <a href="https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">strong case</a> that there is strong evidence of deep learning models being almost linear. In retrospect, it seems intuitive to me that neural networks would store sparse features in linear combinations, rather than with some more complicated form of compression, as this makes it easy to write to and read from these features<sup><a href="#fn6lkqz2r1t9n">[10]</a></sup>.</p><p>In contrast, the last two techniques listed rely on current AI models being very powerful and quite steerable. I admit that two examples aren&#39;t very many, but I expect we&#39;ll see more in this vein soon.</p><p>Looking more broadly, I think it&#39;s worth noting that all of these results are in empirical alignment research. I don&#39;t think this is a coincidence. Whilst there may be simple solutions to some of the problems of agent foundations, if they were easy to find, they probably would have been found before. In contrast, the last two directly rely on a certain level of capabilities. It&#39;s arguable that linear representation techniques indirectly rely upon a certain level of capabilities: insofar as it is beneficial for a model to have a linear representation of particular features, the more powerful our optimization techniques, the more likely our trained models are to produce such a representation.</p><p><strong>An Aside on Contrast-Consistent Search</strong>:</p><p>When I first encountered this technique, I was awed by it and I felt like I had no conception of how anyone would ever think of something like this. While I still acknowledge the brilliance of this work, it no longer feels completely unthinkable.</p><p>I assume Collin already had some intuitions that neural networks were vastly more linear than you might expect. If you buy this and you&#39;re trying to read the truth directly out of the network weights, it&#39;s then entirely natural to look for a linear direction.</p><p>Linear probes were an <a href="https://arxiv.org/abs/1610.01644" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">already existing technique</a>, so if you knew of this technique and you were making a shortlist of which techniques to investigate, this would make it onto the shortlist.</p><p>Suppose you end up in the position of asking &#34;how can I use a linear probe to find a direction corresponding to truth?&#34; in a neural network. It&#39;d then be natural to ask what some of the properties of truth are, which would then lead directly to the consistency property Collin leveraged.</p><p>So, smart, brilliant even, but not the kind of thing that is completely unthinkable.</p><p>And even just knowing to look for linearities would be a massive headstart.</p><p><strong>Final thoughts</strong>:</p><p>I suspect that all of these approaches are still very far away from where we need to be. I consider them substantial advances nonetheless for two key reasons: having a baseline helps people choose an appropriate level of ambition, and also makes it easier to empirically discover the key issues in solving a problem more fully.</p><p>I&#39;m hoping that increasing awareness that relatively simple techniques have been successful spurs more research progress, by giving people the confidence to actually try to make progress and increasing people&#39;s motivation to explore these kinds of approaches for longer before they toss in the towel.<br/><br/>It may very well be that we&#39;re just in a particular stage of the field where there&#39;s all this low-hanging fruit to pick. Perhaps we&#39;ll soon end up in a situation where we need people to look for brilliant conceptual breakthroughs in order to make further progress. However, I think it would be a mistake to just assume people have already looked very hard for simple paths forward. </p><ol><li id="fn1v3hwr53kci"><sup><strong><a href="#fnref1v3hwr53kci">^</a></strong></sup><p>I thought about also including this paper on <a href="https://twitter.com/arankomatsuzaki/status/1709030049758789924" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">representational engineering</a> which describes, among other things, Linear Artificial Tomograph. However, I haven&#39;t had time to finish skimming it yet.</p></li><li id="fn2v4ey0p83c9"><sup><strong><a href="#fnref2v4ey0p83c9">^</a></strong></sup><p>One component pushes the probability of a statement being true and a statement being false toward adding up to one. A second component pushes the model away from producing probabilities close to 0.5.</p></li><li id="fn2dyeooknd0h"><sup><strong><a href="#fnref2dyeooknd0h">^</a></strong></sup><p>While these are non-linear in their inputs, they assume that the log probability is linear in the inputs.</p></li><li id="fnvt1x0nq32oh"><sup><strong><a href="#fnrefvt1x0nq32oh">^</a></strong></sup><p>They generate two observations that are the same except for one possessing the cheese. </p></li><li id="fngwv0u8dqysq"><sup><strong><a href="#fnrefgwv0u8dqysq">^</a></strong></sup><p>They train a linear probe with sigmoid activation to discover the truthful direction. Then they intervene by adding a multiple of this to the activation of the layer, scaling by a constant and the standard deviation.</p></li><li id="fncv4l100bvy"><sup><strong><a href="#fnrefcv4l100bvy">^</a></strong></sup><p>For some useful potential applications, see Nina&#39;s work on using activation addition to <a href="https://www.lesswrong.com/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation#Do_AIs_internally_represent_the_truth_" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">reduce sycophancy and improving honesty</a> and <a href="https://www.lesswrong.com/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">red-teaming</a>.</p></li><li id="fnzynaaklp5vl"><sup><strong><a href="#fnrefzynaaklp5vl">^</a></strong></sup><p>This paper seems like it was mostly intended for outreach purposes. Unfortunately, the framing was a tiny bit off, in that some people felt that this paper/the publicity around it was overclaiming. That said, if this had been handled a bit better, it could have led to significant improvement in the debate. One point I want to emphasize is that you can spend forever debating whether current neural networks have world models and what a world model even means. Or you can just go and make direct empirical progress, and maybe that doesn&#39;t convince people, but it will at least push them to clarify what they&#39;re looking for.</p></li><li id="fnky6wdd05sjn"><sup><strong><a href="#fnrefky6wdd05sjn">^</a></strong></sup><p>You may object that RLHF is mostly capabilities. I also tend to think about it as being primarily a capabilities advance, but it is an advance in alignment as well.</p></li><li id="fneduvymbukau"><sup><strong><a href="#fnrefeduvymbukau">^</a></strong></sup><p>I&#39;ve heard that the innovation in ChatGPT was more about the user interface and finetuning for that than producing a more powerful model.</p></li><li id="fn6lkqz2r1t9n"><sup><strong><a href="#fnref6lkqz2r1t9n">^</a></strong></sup><p>While orthogonal features are the simplest, you can only include one feature per dimension, whilst you can fit many more near orthogonal features.</p></li></ol><br/><br/><a href="https://www.alignmentforum.org/posts/ziNCZEm7FE9LHxLai/don-t-dismiss-simple-alignment-approaches#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>