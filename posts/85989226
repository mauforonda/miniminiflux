<p><a href="https://arxiv.org/abs/2310.02226" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Think before you speak: Training Language Models With Pause Tokens</a></p>
<p>Another example of how much low hanging fruit remains to be discovered in basic Large Language Model research: this team from Carnegie Mellon and Google Research note that, since LLMs get to run their neural networks once for each token of input and output, inserting &#34;pause&#34; tokens that don&#39;t output anything at all actually gives them extra opportunities to &#34;think&#34; about their output.</p>