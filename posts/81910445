<p>Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Challenges of Reinforcement Learning from Human Feedback</h2><p>If you’ve used ChatGPT, you might’ve noticed the “thumbs up” and “thumbs down” buttons next to each of its answers. Pressing these buttons provides data that OpenAI uses to improve their models through a technique called reinforcement learning from human feedback (RLHF).</p><p>RLHF is popular for teaching models about human preferences, but it faces fundamental limitations. Different people have different preferences, but instead of modeling the diversity of human values, RLHF trains models to earn the approval of whoever happens to give feedback. Furthermore, as AI systems become more capable, they can learn to deceive human evaluators into giving undue approval.</p><p>Here we discuss a <a href="https://arxiv.org/abs/2307.15217" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new paper</a> on the problems with RLHF and open questions for future research.</p><p><strong>How RLHF is used to train language models.</strong> Large language models such as ChatGPT, Claude, Bard, LLaMA, and Pi are typically trained in two stages. During the “pretraining” stage, the model processes large amounts of text from the internet. The model learns to predict which word will appear next in the text, which provides it a broad grasp of grammar, facts, reasoning abilities, and even biases and inaccuracies present in the training data.</p><p>After pretraining, models are “fine-tuned” for particular tasks. Sometimes they’re fine-tuned to mimic demonstrations of approved behavior. Reinforcement learning from human feedback (RLHF) is a technique where the model provides an output and a human provides feedback on it. The feedback is relatively simple, such as a thumbs up or a ranking against another model output. Using this feedback, the model is refined to produce outputs which it expects would be preferred by human evaluators.</p><p>For a more detailed technical explanation of RLHF, see <a href="https://huyenchip.com/2023/05/02/rlhf.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>. Alternatively, for a lighter treatment of the topic, see the “shoggoth” meme below.</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9a4186e-99aa-4526-8c06-cc00b6e808cc_1470x606.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/KhMQtpNCZHUUN6_nRoWVLvMr3S-qNeNjDKzn1suZ2s0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZiOWE0MTg2ZS05OWFhLTQ1MjYtOGMwNi1jYzAwYjZlODA4Y2NfMTQ3MHg2MDYucG5n 424w, https://reader.miniflux.app/proxy/N8bqagsT6ug32IZWUV9RbvMf0HJrqgVehQuQyq_RnIg=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZiOWE0MTg2ZS05OWFhLTQ1MjYtOGMwNi1jYzAwYjZlODA4Y2NfMTQ3MHg2MDYucG5n 848w, https://reader.miniflux.app/proxy/WMDs6326ID_lQIG1dG15TMnMtHBz6dsWhYCW780wX18=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYjlhNDE4NmUtOTlhYS00NTI2LThjMDYtY2MwMGI2ZTgwOGNjXzE0NzB4NjA2LnBuZw== 1272w, https://reader.miniflux.app/proxy/83UwfcDj6k_YMQxpV2CjeTcRqbkv0DTzODryW12tA88=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYjlhNDE4NmUtOTlhYS00NTI2LThjMDYtY2MwMGI2ZTgwOGNjXzE0NzB4NjA2LnBuZw== 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/gei7oMGYFVB9cdUuKf7DMVlJ-Wgxu6jdHSl1zRjifhA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYjlhNDE4NmUtOTlhYS00NTI2LThjMDYtY2MwMGI2ZTgwOGNjXzE0NzB4NjA2LnBuZw==" alt="" srcset="https://reader.miniflux.app/proxy/dnqsnTwVmav1ujN1cFeUW9swRWUNGSZsUtyIuRzdCRo=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZiOWE0MTg2ZS05OWFhLTQ1MjYtOGMwNi1jYzAwYjZlODA4Y2NfMTQ3MHg2MDYucG5n 424w, https://reader.miniflux.app/proxy/An9PbfUHrQ420iYaAMOgcw7rCYz-08DhyCj1PZ5MjBg=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZiOWE0MTg2ZS05OWFhLTQ1MjYtOGMwNi1jYzAwYjZlODA4Y2NfMTQ3MHg2MDYucG5n 848w, https://reader.miniflux.app/proxy/XQTcK9asc9j0PE7ErZ-C3WA4goySVav8akXEA4a6sjc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYjlhNDE4NmUtOTlhYS00NTI2LThjMDYtY2MwMGI2ZTgwOGNjXzE0NzB4NjA2LnBuZw== 1272w, https://reader.miniflux.app/proxy/gei7oMGYFVB9cdUuKf7DMVlJ-Wgxu6jdHSl1zRjifhA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYjlhNDE4NmUtOTlhYS00NTI2LThjMDYtY2MwMGI2ZTgwOGNjXzE0NzB4NjA2LnBuZw== 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption><em>The “shoggoth” meme depicts GPT-3 as an alien monster unlike any human being. Fine-tuning with RLHF masks the shoggoth.</em></figcaption></figure><p><strong>Aligned with whom? </strong>Humans often disagree with one another. Our different values, cultures, and personal experiences shape our beliefs and desires. Instead of modeling different perspectives and considering tradeoffs between them, RLHF trains models to maximize expected approval given previous feedback data, creating two problems.</p><p>First, the people who give feedback to AI systems do not necessarily represent everyone. For example, OpenAI <a href="https://arxiv.org/abs/2203.02155" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">noted</a> in 2020 that half of their evaluators were between 25 and 34 years old. Research shows that language models often reflect the opinions of only some groups of people. For example, <a href="https://arxiv.org/abs/2303.17548" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">one study</a> found GPT-3.5 assigned a &gt;99% approval rating to Joe Biden, while <a href="https://r2hcai.github.io/AAAI-23/files/CameraReadys/49.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">another</a> showed GPT-3 supports the moral foundations of conservatives. Progress on this problem could be made by hiring diverse sets of evaluators and developing methods for improving the representativeness of AI outputs.</p><p>Second, by maximizing expected approval given previous feedback, RLHF ignores other ways of aggregating individual preferences, which could prioritize worse-off users. Recent work has explored this idea by using language models to <a href="https://arxiv.org/abs/2211.15006" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">find areas of agreement</a> between people who disagree.</p><p><strong>RLHF encourages AIs to deceive humans.</strong> We train AIs by rewarding them for telling the truth and punishing them for lying, according to what humans think is true. But when AIs know more than humans, this could make them say what humans expect to hear, even if it is false. Currently, when AIs are being rewarded by humans for being right, in practice they are really being rewarded for saying what we <em>think</em> is right; when we are uninformed or irrational, then we end up rewarding AIs for false statements that conform to our own false beliefs. This could train AIs to learn to deceive humans.</p><p>Relatedly, AIs can exploit limitations of human oversight. We’ve already seen empirical examples of this phenomenon. <a href="https://openai.com/research/learning-from-human-preferences" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Research</a> at OpenAI trained a robot in a computer simulation to grab a ball. A human observed the robot and provided positive feedback when it successfully grabbed the ball. Using RLHF, the robot was trained to earn human approval.</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F414e2716-37cd-4749-ad40-db1f29d7eb01_1310x1160.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/YAYbXt5mEGMVB7U7VBvVqd1LEnch7WoqC5S5oeo8Aqs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MTRlMjcxNi0zN2NkLTQ3NDktYWQ0MC1kYjFmMjlkN2ViMDFfMTMxMHgxMTYwLnBuZw== 424w, https://reader.miniflux.app/proxy/ltgsJw5JXMh3JV5IFdGc_KUYbLZkBS32PskcVRdsufE=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MTRlMjcxNi0zN2NkLTQ3NDktYWQ0MC1kYjFmMjlkN2ViMDFfMTMxMHgxMTYwLnBuZw== 848w, https://reader.miniflux.app/proxy/Wr6zXH5utAxgKcoNcW_TxdEhCTZ-cYtG4oaI8J5209Y=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDE0ZTI3MTYtMzdjZC00NzQ5LWFkNDAtZGIxZjI5ZDdlYjAxXzEzMTB4MTE2MC5wbmc= 1272w, https://reader.miniflux.app/proxy/dGHUb3ldqaBnipbVcvNsaTeLMcJU4Ibt1DwVaLCIGQE=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDE0ZTI3MTYtMzdjZC00NzQ5LWFkNDAtZGIxZjI5ZDdlYjAxXzEzMTB4MTE2MC5wbmc= 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/tzOcfm5FHM13F-ecl5JhveqVyv99LoRuUNYE3Z33yEI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDE0ZTI3MTYtMzdjZC00NzQ5LWFkNDAtZGIxZjI5ZDdlYjAxXzEzMTB4MTE2MC5wbmc=" width="426" alt="" srcset="https://reader.miniflux.app/proxy/tTX1yKRb1ULvY1DUaEDIb_ho_vWebWOdcvATcSWqFZ8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MTRlMjcxNi0zN2NkLTQ3NDktYWQ0MC1kYjFmMjlkN2ViMDFfMTMxMHgxMTYwLnBuZw== 424w, https://reader.miniflux.app/proxy/ChjQOas_VgaYdz8pEb2ZGJBPaaoRO6EOYEA2WWPVoYU=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY0MTRlMjcxNi0zN2NkLTQ3NDktYWQ0MC1kYjFmMjlkN2ViMDFfMTMxMHgxMTYwLnBuZw== 848w, https://reader.miniflux.app/proxy/zCGoiLFwyy3cZwAkaNhgDrhzOTmzs7hapXGe_sgJBAA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDE0ZTI3MTYtMzdjZC00NzQ5LWFkNDAtZGIxZjI5ZDdlYjAxXzEzMTB4MTE2MC5wbmc= 1272w, https://reader.miniflux.app/proxy/tzOcfm5FHM13F-ecl5JhveqVyv99LoRuUNYE3Z33yEI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNDE0ZTI3MTYtMzdjZC00NzQ5LWFkNDAtZGIxZjI5ZDdlYjAxXzEzMTB4MTE2MC5wbmc= 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption><em>It might look like the robot hand is grasping the yellow ball, but it’s just hovering between the camera and the ball. Human evaluators gave positive feedback for this behavior, so the AI learned to repeat it.</em></figcaption></figure><p>Instead of learning to pick up the ball, the robot learned to hold its hand between the human and the ball, such that the human observer would incorrectly believe the robot had grabbed the ball. Clearly, the robot was not aware of the human or the rest of the outside world. Instead, blind pursuit of positive feedback led the AI system to behave in a way that systematically tricked its human evaluator.</p><p>The <a href="https://arxiv.org/abs/2307.15217" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">paper</a> provides a variety of other challenges with RLHF. It summarizes existing research on the topic, and provides suggestions for future technical and interdisciplinary work.</p><h2>Microsoft’s Security Breach</h2><p>Chinese hackers have <a href="https://www.wyden.senate.gov/imo/media/doc/wyden_letter_to_cisa_doj_ftc_re_2023_microsoft_breach.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">accessed</a> hundreds of thousands of U.S. government emails by exploiting software provided by Microsoft. Given that GPT-4 and other AI systems are trained on Microsoft servers, this historic breach raises questions about the cybersecurity of AI systems.</p><p>An FBI investigation found that “email accounts compromised include the Secretary of Commerce, the U.S. Ambassador to China, and the Assistant Secretary of State for East Asia.” The hackers stole an encryption key from Microsoft, allowing them to impersonate the real owners of these accounts.</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73632b22-498b-4765-a882-65f73bd85167_1600x1052.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/rCH08N16SyXJp-P68zPPp3gsA2-05P5skRScxCFP3dM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY3MzYzMmIyMi00OThiLTQ3NjUtYTg4Mi02NWY3M2JkODUxNjdfMTYwMHgxMDUyLnBuZw== 424w, https://reader.miniflux.app/proxy/WJYaBNFASvvwVYBK5vvTfZC1JIacEmRKNsEkdOaYjYg=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY3MzYzMmIyMi00OThiLTQ3NjUtYTg4Mi02NWY3M2JkODUxNjdfMTYwMHgxMDUyLnBuZw== 848w, https://reader.miniflux.app/proxy/UvKk3eQqTXRJ0uve1yZ6q7iz158dKzrb9AHa4v9zVWA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNzM2MzJiMjItNDk4Yi00NzY1LWE4ODItNjVmNzNiZDg1MTY3XzE2MDB4MTA1Mi5wbmc= 1272w, https://reader.miniflux.app/proxy/pFIyhJSmjJ14hNu6kVo6LcyEs3w27zrmFYa516uEBRY=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNzM2MzJiMjItNDk4Yi00NzY1LWE4ODItNjVmNzNiZDg1MTY3XzE2MDB4MTA1Mi5wbmc= 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/QX6pFFj7O_YixTfF6FcIyn1yIy3pKpAA2m7I3n-v1oY=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNzM2MzJiMjItNDk4Yi00NzY1LWE4ODItNjVmNzNiZDg1MTY3XzE2MDB4MTA1Mi5wbmc=" alt="" srcset="https://reader.miniflux.app/proxy/EctiWfeh1Uu6vdUrmrQgBdkClftFYpUttKB4rftl2_M=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY3MzYzMmIyMi00OThiLTQ3NjUtYTg4Mi02NWY3M2JkODUxNjdfMTYwMHgxMDUyLnBuZw== 424w, https://reader.miniflux.app/proxy/0TNsnftSpnwAAI-OEmchwMGgJvIJ-bc-B0k_fSqpCHA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkY3MzYzMmIyMi00OThiLTQ3NjUtYTg4Mi02NWY3M2JkODUxNjdfMTYwMHgxMDUyLnBuZw== 848w, https://reader.miniflux.app/proxy/-pWrYquQOYrJT3EMfr0jXOrHgOf-4Wdbaja2747JM0c=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNzM2MzJiMjItNDk4Yi00NzY1LWE4ODItNjVmNzNiZDg1MTY3XzE2MDB4MTA1Mi5wbmc= 1272w, https://reader.miniflux.app/proxy/QX6pFFj7O_YixTfF6FcIyn1yIy3pKpAA2m7I3n-v1oY=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGNzM2MzJiMjItNDk4Yi00NzY1LWE4ODItNjVmNzNiZDg1MTY3XzE2MDB4MTA1Mi5wbmc= 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><p>This is not the first failure of Microsoft’s security infrastructure. In 2020, the Russian government committed one of the <a href="https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">worst cyberattacks in U.S. history</a> by stealing a Microsoft key. The hackers breached more than 200 government organizations and most Fortune 500 companies for a period of up to nine months before being discovered.</p><p>Microsoft’s cybersecurity raises concerns about their ability to prevent cybercriminals from stealing advanced AI systems. OpenAI <a href="https://openai.com/blog/openai-and-microsoft-extend-partnership" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">trains and deploys</a> all of their models on Microsoft servers. If <em>future</em> advanced AI systems were stolen by cybercriminals, they could pose a global security threat.</p><p>Less than a month ago, Microsoft and six other leading AI labs <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">committed</a> to the White House to “invest in cybersecurity and insider threat safeguards to protect proprietary and unreleased model weights.” After this failure, Microsoft will need to improve their approach to cybersecurity.</p><h2>Conceptual Research on AI Safety</h2><p>Over the last seven months, the Center for AI Safety hosted around a dozen academic philosophers for a research fellowship. This is part of our ongoing effort to make AI safety an interdisciplinary, non-parochial research field. Here are some highlights from their work.</p><p><strong>Do AIs have wellbeing?</strong> Many ethical theories hold that consciousness is not required for moral status. Professor Simon Goldstein and professor Cameron Domenico Kirk-Giannini argue that <a href="https://philpapers.org/archive/GOLAWE-4.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">some AI systems have beliefs and desires</a>, and therefore deserve ethical consideration. Here’s an <a href="https://www.abc.net.au/religion/ai-generative-agents-are-unethical-and-unsafe/102277448" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">op-ed</a> on the idea.</p><p><strong>The safety of language agents.</strong> The same authors argue that language agents are safer in many respects than <a href="https://philarchive.org/rec/GOLLAR-2" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">reinforcement learning agents</a>, meaning that many prior prominent safety concerns are lessened with this potential paradigm.</p><p><strong>Could AIs soon be conscious?</strong> Rob Long and Jeff Sebo review a dozen <a href="https://jeffsebodotnet.files.wordpress.com/2023/06/moral-consideration-for-ai-systems-by-2030-5.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">commonly proposed conditions for consciousness</a>, arguing that under these theories, AIs could soon be conscious. See also Rob Long’s <a href="https://experiencemachines.substack.com/p/key-questions-about-artificial-sentience" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Substack</a> on the topic.</p><p><strong>Shutting down AI systems.</strong> Elliot Thornley <a href="https://s3.amazonaws.com/pf-user-files-01/u-242443/uploads/2023-05-02/m343uwh/The%20Shutdown%20Problem-%20Two%20Theorems%2C%20Incomplete%20Preferences%20as%20a%20Solution.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">establishes</a> the difficulty of building agents which are indifferent to being shut down, then proposes a plan for doing so.</p><p><strong>Rethinking instrumental convergence.</strong> The instrumental convergence thesis holds that regardless of an agent&#39;s goal, it is likely for it to be rational to pursue certain subgoals, such as power-seeking and self-preservation, in service of that goal. Professor Dmitri Gallow <a href="https://drive.google.com/file/d/1Bp6iTbeXgdeE5C3UPqdWFOp8MePRylvu/view" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">questions</a> the idea, arguing that opportunity costs and the possibility of failure make it less rational to pursue these instrumental goals. Relatedly, CAIS affiliate professor Peter Salib argues that goals like <a href="https://deliverypdf.ssrn.com/delivery.php?ID=111089002009097078002109066127090087034086041036045026092113022030105100098083118031022029052037057008050107098097111105106084122004033060060127089076083124064085113061041033106029084001070010027094097028108088017030015096006030123110070064099030092022&amp;EXT=pdf&amp;INDEX=TRUE" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI self-improvement</a> would not necessarily be rational.</p><p><strong>Popular writing. </strong>CAIS philosophy fellows have written various op-eds about <a href="https://www.scmp.com/comment/opinion/article/3223116/growing-threat-ai-misuse-makes-need-effective-targeted-regulation-all-more-urgent" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the dangers of AI misuse</a>, <a href="https://thebulletin.org/2023/06/most-ai-research-shouldnt-be-publicly-released/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">why some AI research shouldn’t be published</a>, and <a href="https://www.cd.kg/wp-content/uploads/2023/07/globe_opinion.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI’s present and future harms</a>, and so on.</p><p><strong>Academic field-building.</strong> Professor Nathaniel Sharadin is helping organize an <a href="https://aiimpacts.tenureslack.com/about-the-workshop/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI Safety workshop</a> at the University of Hong Kong. Cameron Domenico Kirk-Giannini and CAIS Director Dan Hendrycks are editing <a href="https://link.springer.com/collections/cadgidecih" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a special issue of Philosophical Studies</a>. Its first paper argues that AIs <a href="https://link.springer.com/article/10.1007/s11098-023-02023-4" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">will not</a> necessarily maximize expected utility, and the special issue is open for submission until November 1st, 2023.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Links</h2><ul><li><p>A <a href="https://www.vox.com/future-perfect/23775650/ai-regulation-openai-gpt-anthropic-midjourney-stable" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">detailed overview</a> of the AI policies currently being considered by US policymakers.</p></li><li><p>Confidence-building measures that can be taken <a href="https://arxiv.org/abs/2308.00862" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">by AI labs and governments</a>.</p></li><li><p>An interactive visualization of the international supply chain for <a href="https://chipexplorer.eto.tech/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">advanced computer chips</a>.</p></li><li><p>Users will be able to <a href="https://twitter.com/OfficialLoganK/status/1686749315165835264" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">fine-tune</a> GPT-3.5 and GPT-4 later this year.</p></li><li><p>Websites can <a href="https://platform.openai.com/docs/gptbot" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">opt-out</a> of being scraped by OpenAI for training their models.</p></li><li><p>OpenAI files a patent for <a href="https://www.linkedin.com/posts/activity-7092524003159920641-N7MR/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">GPT-5</a>, noting plans to transcribe and generate human audio. (This doesn’t mean they’ve trained GPT-5; only that they’re planning for it.)</p></li><li><p>Meta is building <a href="https://www.reuters.com/technology/meta-prepares-ai-powered-chatbots-attempt-retain-users-ft-2023-08-01/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">products with AI chatbots</a> to help retain users.</p></li><li><p><a href="https://trojandetection.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">NeurIPS competition</a> on detecting Trojans and red-teaming language models.</p></li><li><p>A leading AI researcher argues that it <a href="https://twitter.com/RichardSSutton/status/1686475184612704256" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">wouldn’t be bad</a> for AI systems to grow more powerful than humans: “Why shouldn’t those who are the smartest become powerful?”</p></li><li><p>By listening to the audio from a Zoom call, this AI system can detect <a href="https://arxiv.org/abs/2308.01074" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">which keys are being struck </a>on a laptop keyboard with 93% accuracy.</p></li></ul><p>See also: <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS website</a>, <a href="https://twitter.com/ai_risks?lang=en" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS twitter</a>, <a href="https://newsletter.mlsafety.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">A technical safety research newsletter</a>, and <a href="https://arxiv.org/abs/2306.12001" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">An Overview of Catastrophic AI Risks</a></p><p><a href="https://newsletter.safe.ai/p/ai-safety-newsletter-18?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Share</a></p>