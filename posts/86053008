<p><em>Involving individuals that have expertise working with at-risk communities in the design process will allow companies to address online harms in a scalable and sustainable way, say Nadah Feteih and Elodie Vialle.</em></p>



<figure><img src="https://reader.miniflux.app/proxy/SzzgUD1TE1bU1k3gxTU1SnsgBRFEGysLOiftyrnPu7I=/aHR0cHM6Ly90ZWNocG9saWN5LnByZXNzL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIzLzEwL0ludGVyY29ubmVjdGVkLTEwMjR4NTc2LnBuZw==" alt="" srcset="https://reader.miniflux.app/proxy/SzzgUD1TE1bU1k3gxTU1SnsgBRFEGysLOiftyrnPu7I=/aHR0cHM6Ly90ZWNocG9saWN5LnByZXNzL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIzLzEwL0ludGVyY29ubmVjdGVkLTEwMjR4NTc2LnBuZw== 1024w, https://reader.miniflux.app/proxy/z4zjJmUKW4e2I-1UAorcG52zCQ7ua1l6us7RB-v7lPo=/aHR0cHM6Ly90ZWNocG9saWN5LnByZXNzL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIzLzEwL0ludGVyY29ubmVjdGVkLTMwMHgxNjkucG5n 300w, https://reader.miniflux.app/proxy/7OBKy0Jccv6Rn8mM7MdLaE8_NlXYTplPjl4OzHlj8y0=/aHR0cHM6Ly90ZWNocG9saWN5LnByZXNzL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIzLzEwL0ludGVyY29ubmVjdGVkLTc2OHg0MzIucG5n 768w, https://reader.miniflux.app/proxy/8nT_M6dNNQA76JwodGOEq-Pq33KCz88nds7VDkBPUS4=/aHR0cHM6Ly90ZWNocG9saWN5LnByZXNzL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDIzLzEwL0ludGVyY29ubmVjdGVkLnBuZw== 1200w" sizes="(max-width: 1024px) 100vw, 1024px" loading="lazy"/><figcaption><a href="https://www.shutterstock.com/image-photo/background-abstract-concept-network-social-media-712526989" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Shutterstock</a></figcaption></figure>


<p>Social media platforms have helped a virtual world flourish where people are connected now more than ever. But these platforms have also been abused and misused to amplify hate and harassment and thus harm <a href="https://www.adl.org/resources/report/online-hate-and-harassment-american-experience-2023" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">marginalized and at-risk communities</a>.   </p>



<p>Activists are threatened when <a href="https://www.aljazeera.com/opinions/2011/3/9/the-dangers-of-social-media-revolt" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">speaking up against oppressive governments</a>. Human rights defenders and journalists are targeted ahead of elections. Social media has been used to influence the outcome of democratic processes, and with over 75 elections coming in 2024 there is more pressure to tackle issues like misinformation, online abuse, and <a href="https://policyreview.info/articles/analysis/political-microtargeting-towards-pragmatic-approach" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">microtargeting</a>. </p>



<p>We are two individuals that come from different backgrounds—a software engineer from Egypt that has worked in Big Tech in the US and a journalist from France working with civil society groups internationally—yet we see the same problems. As human rights defenders, over the last few years we have <a href="https://rsf.org/en/rsf-publishes-report-online-harassment-journalists" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">documented</a> examples of abuse where social media platforms have a direct responsibility in spreading hate and suppressing the voices of marginalized individuals (i.e. people of color, <a href="https://www.amnesty.org/en/latest/research/2018/03/online-violence-against-women-chapter-1-1/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">women</a>, etc.). As tech workers and advocates, we’ve noticed how Big Tech’s business model often incentivizes growth, profit, and “innovation” over addressing human rights and <a href="https://hbr.org/2019/11/the-ethical-dilemma-at-the-heart-of-big-tech-companies" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">ethical concerns</a><a href="https://she-persisted.org/wp-content/uploads/2023/02/ShePersisted_MonetizingMisogyny.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">.</a></p>



<p>There is an urgent need to increase collaboration between companies and communities. Tech companies must center marginalized individuals from ideating features to launching them into production, along the lines of the framework proposed in “<a href="https://www.belfercenter.org/publication/design-margins" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Design from the Margins</a>”. We’ve been involved in efforts with the Berkman Klein Center at Harvard to<a href="https://rebootingsocialmedia.org/2022/12/01/from-emergency-to-prevention-protecting-journalists-from-online-abuse/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"> codesign solutions with tech platforms representatives</a> through a multistakeholder approach that is intended to better protect journalists online and <a href="https://www.belfercenter.org/publication/design-margins" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">center</a> vulnerable groups in both the product design and development process. We believe that instead of endlessly triaging products and features that have been flawed and historically broken, it is time to focus on reforming product design processes.</p>



<h2 id="h-profit-vs-trust-and-safety-tech-companies-have-fallen-short-in-prioritization-nbsp">Profit vs Trust and Safety: Tech companies have fallen short in prioritization </h2>



<p>Whistleblower and former Facebook (now Meta) employee Frances Haugen claims the company still <a href="https://www.theverge.com/2021/10/3/22707860/facebook-whistleblower-leaked-documents-files-regulation" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">prioritizes profit</a> and growth over tackling the major harms its platforms have caused. We argue that there need to be actions taken to fix biased enforcement systems, <a href="https://aiforthepeopleus.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">address </a>algorithmic bias, invest engineering resources to build tools to help users affected by <a href="https://pen.org/issue/online-abuse/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">online abuse</a> escalate concerns, and improve content moderation in various languages (i.e. having content moderation teams based in more countries would help with understanding local context and dialects).</p>



<p>Over the past few years, tech companies have been forced to mitigate harms by establishing <a href="https://techpolicy.press/evaluating-the-forces-shaping-the-trust-safety-industry/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Trust &amp; Safety policies and teams</a> focused on content moderation, privacy, and security. Independent and semi-independent external entities (such as the <a href="https://www.oversightboard.com/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Oversight Board</a>) have been created to hold platforms like Meta accountable and require them to review policy and content decisions. While the ability to reverse biased and unfair decisions is valuable, it is not, in and of itself, a scalable or adequate solution. Bringing back a post that was incorrectly removed (whether it through a human or an automated review system) isn’t fixing the underlying algorithms that caused that decision to be made or fundamentally addressing the mistakes that are made during the human review process. We can’t conflate short-term band-aid solutions with comprehensive solutions to rebuild systems to center accuracy and fairness. </p>



<p>Unfortunately, many teams at these companies find themselves trying to patch infrastructure that was not initially built or designed with privacy and safety in mind. The communities most affected by bias are often those at the margins and who <a href="https://www.latimes.com/business/technology/story/2020-06-24/tech-started-publicly-taking-lack-of-diversity-seriously-in-2014-why-has-so-little-changed-for-black-workers" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">do not have a seat at the table</a> to influence product decisions. In recent years, many individuals are taking a stand and establishing external organizations, such as the <a href="https://integrityinstitute.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Integrity Institute</a> and <a href="https://alltechishuman.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">All Tech is Human</a>, to bring together professionals at the forefront of public interest tech to develop ideas, write papers, and imagine solutions to solve the most pressing issues that threaten safety, online and off. It is important to include a diverse set of users in the design process to ensure fairness and minimize bias from the outset, not only when responding to a <a href="https://www.pbs.org/newshour/world/amnesty-report-finds-facebook-amplified-hate-ahead-of-rohingya-massacre-in-myanmar" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">disaster, crisis, or appeal</a>. </p>



<h2 id="h-interventions-should-focus-more-on-process-over-product-decisions">Interventions should focus more on process over product decisions</h2>



<p>In 2021, a change to the Instagram ranking algorithm demoted reshared content on stories. It was aligned with the goal of prioritizing original content but unfortunately <a href="https://www.theverge.com/2021/5/30/22460946/instagram-making-changes-algorithm-censoring-pro-palestinian-content" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">stifled a large audience</a> of users (i.e. activists and community organizers) that leverage stories to bring awareness to certain causes and crises. In an ideal scenario, before an algorithm or ranking change like this is rolled out it should be tested by a wide audience and management should consider various use cases from diverse groups by ensuring that the stakeholders impacted by these changes are consulted.</p>



<p>Community driven design is how we can focus on responsible innovation. As these social media platforms (Meta, TikTok, X, YouTube) feel increased pressure to <a href="https://www.csis.org/analysis/banning-tiktok-will-not-solve-us-online-disinformation-problems" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">address tension</a> and reconcile with their audiences, much can be gained by adopting a different approach. At the <a href="https://www.digitalrights.community/blog/apply-feira-global-gathering" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Global Gathering convening digital rights organizations</a> mid-September in Portugal, several organizations expressed frustration at tech platforms not consulting countries from the Global South before launching <a href="https://www.reuters.com/article/us-facebook-newsfeed-idUSKCN1GD671" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new products. </a> These individuals from the Majority World need to be more involved and considered on decisions that affect them in areas such as content moderation, data privacy, product design, and policy.</p>



<p>Indeed, these companies face even more pressure, not only from individual users but from governments, as they need to comply with regulation from the EU such as the <a href="https://www.accessnow.org/press-release/joint-statement-dsa-human-rights/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Digital Services Act (DSA)</a>. According to Article 26, <a href="https://ec.europa.eu/commission/presscorner/detail/en/IP_23_2413" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">very large online platforms</a> are required to conduct <a href="https://digitalservicesact.cc/dsa/art26.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">risk assessments</a> to minimize negative effects on freedom of expression and inauthentic use or exploitation of the service. It is important for at-risk individuals and civil society groups most familiar with harms perpetuated by the platforms to be involved in consultations during these assessments so that potential negative effects can be mitigated from the outset. Abiding by community driven design will set up a precedent and encourage companies to <a href="https://www.accessnow.org/wp-content/uploads/2022/11/Declaration-of-principles-for-content-and-platform-governance-in-times-of-crisis.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">deploy more responsible design processes</a> and work more with civil society organizations. </p>



<p>Teams have started working with communities on <a href="https://humanrights.fb.com/wp-content/uploads/2023/09/2022-Meta-Human-Rights-Report.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">policies</a>, yet there is more work to be done in the product development process. Partnered with affected individuals, these teams can brainstorm scenarios, situations, and simulate the implications of product and feature decisions through a multistakeholder design process to preemptively mitigate potential harms. As this codesign process is implemented and scaled, these major questions must be considered – how are these individuals and communities picked, how is the agenda of the design process set up, and how will the diversity of voices (ethnic, geographical, background) be represented. It is paramount to acknowledge the power dynamics and knowledge asymmetry between organizations representing users at risk and the platforms to ensure a fair and equitable participation in these processes.</p>



<h2 id="h-providing-a-voice-to-those-affected-by-platform-decisions">Providing a voice to those affected by platform decisions</h2>



<p>Individual advocates and civil society organizations have banded together to push for change, pulling together  global coalitions, such as the <a href="https://onlineviolenceresponsehub.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Coalition Against Online Violence</a>, organizing <a href="https://pen.org/campaign/fight-online-abuse-now/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">campaigns</a> and <a href="https://7amleh.org/2021/05/31/7amleh-jointly-calls-on-facebook-to-stop-censoring-palestinian-content" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">open letters</a>, and<a href="https://www.harpersbazaar.com/culture/features/a42860180/after-the-whistle-blows/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"> blowing the whistle</a>. Many of the individuals that have been at the forefront of online abuse and attacks had no other choice than to <a href="https://peoplevsbig.tech/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">develop their own expertise</a> so as to mitigate the risks, with many survivors becoming <a href="https://yoursosteam.wordpress.com/about/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">pioneers</a> and <a href="https://www.ffwd.org/blog/tech-nonprofits/online-sos-digital-safe-haven-online-harassment/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">experts</a> themselves. When these platforms aren’t answering the needs of these communities, people have resorted to building their own solutions such as <a href="https://www.blockpartyapp.com/landing/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Block Party</a>, a browser extension that reduces exposure to harassment and online attacks on Twitter (now known as X). Unfortunately, with the launch of an expensive Twitter API subscription tier, many smaller developers were left with no choice but to shut down their apps (including Block Party) and a decision like this further illustrates the glaring divide between large tech platforms and individuals as it strains already under-resourced organizations. The onus in solving the problems that these platforms have created should not fall on these individuals and instead time, energy, and people should be allocated at these companies to address these issues. </p>



<p>Meta’s Trusted Partner program was created as a communication channel between the platform and civil society organizations. However, there have been <a href="https://restofworld.org/2023/exporter-meta-trusted-partner-program-slow-decline/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">many points of frustration in the process</a>, with trusted partners, often under-resourced nonprofits in the global majority,  expending time and resources to track, report, and escalate harms  that do not receive a timely, accurate, or relevant response, as <a href="https://internews.org/wp-content/uploads/2023/07/SafetyAtStake_Internews.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">highlighted in a recent Internews report</a>. These platforms need to invest more time and money into building better crisis protocols (as required by <a href="https://digitalservicesact.cc/dsa/art37.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">article 37 of the DSA</a>), including tooling for escalation channels and dashboards to track user and nonprofit reporting. While some social media platforms, like Facebook and TikTok, have rudimentary support inboxes for user reporting of hate, harassment, and harm, <a href="https://pen.org/report/shouting-into-the-void/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">recent research</a> by nonprofits PEN America and Meedan found that user reporting mechanisms need to be overhauled, including the creation of dashboards to “track outcomes and understand why decisions were made.” We propose that platform engineering teams should  collaborate with vulnerable communities to make the reporting and escalation processes easier for individuals and partners to use.</p>



<p>Earlier this month, we reconvened at Harvard University for the 25th anniversary of the Berkman Klein Center for Internet &amp; Society. A common observation in our conversations with others were the unique perspectives that each person brought and the power of inclusivity when brainstorming solutions, involving individuals from marginalized communities and the Global South instead of solely pushing Western and techno-solutionist perspectives. We see the way existing social media platforms can improve and even beyond that we believe it’s time to <a href="https://publicinfrastructure.org/2023/03/29/the-three-legged-stool/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">reimagine the digital public sphere</a>, develop <a href="https://www.mediasupport.org/publication/public-interest-infrastructure/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">digital public infrastructures</a> and social platforms, and build new solutions from the ground up. </p>



<p>The future of tech, democracy, and human rights is intertwined now more than ever, and it is the responsibility of our generation to address these challenges together. </p>



<p><em>Authors would like to thank </em><a href="https://mickens.seas.harvard.edu/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><em>James Mickens</em></a><em> and </em><a href="https://pen.org/user/viktorya-vilk/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><em>Viktorya Vilk</em></a><em> for their support.</em></p>
<p>The post <a href="https://techpolicy.press/centering-community-voices-how-tech-companies-can-better-engage-with-civil-society-organizations/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Centering Community Voices: How Tech Companies Can Better Engage with Civil Society Organizations</a> appeared first on <a href="https://techpolicy.press" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Tech Policy Press</a>.</p>
