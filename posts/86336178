Published on October 10, 2023 12:29 AM GMT<br/><br/><p>The <a href="https://en.wikipedia.org/wiki/Instrumental_convergence" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">paperclip maximizer</a> is a thought experiment about a hypothetical superintelligent AGI that is obsessed with maximizing paperclips. It can be modeled as a utility-theoretic agent whose utility function is proportional to the number of paperclips in the universe. The <a href="https://arbital.com/p/orthogonality/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Orthogonality Thesis</a> argues for the logical possibility of such an agent. It comes in weak and strong forms:</p>
<blockquote>
<p>The weak form of the Orthogonality Thesis says, &#34;Since the goal of making paperclips is tractable, somewhere in the design space is an agent that optimizes that goal.&#34;</p>
<p>The strong form of Orthogonality says, &#34;And this agent doesn&#39;t need to be twisted or complicated or inefficient or have any weird defects of reflectivity; the agent is as tractable as the goal.&#34; That is: When considering the necessary internal cognition of an agent that steers outcomes to achieve high scores in some outcome-scoring function U, there&#39;s no added difficulty in that cognition except whatever difficulty is inherent in the question &#34;What policies would result in consequences with high U-scores?&#34;</p>
</blockquote>
<p>This raises a number of questions:</p>
<ul>
<li>Why would it be likely that the future would be controlled by utility-maximizing agents?</li>
<li>What sorts of utility functions are likely to arise?</li>
</ul>
<p>A basic reason to expect the far future to be controlled by utility-maximizing agents is that utility theory is the theory of making tradeoffs under uncertainty, and agents that make plans far into the future are likely to make tradeoffs, since tradeoffs are necessary for their plans to succeed. They will be motivated to make tradeoffs leading to controlling the universe almost regardless of what U is, as long as U can only be satisfied by pumping the distant future into a specific part of the possibility space. Whether an agents seeks to maximize paperclips, minimize entropy, or maximize the amount of positive conscious experience, it will be motivated to, in the short term, cause agents sharing its values to have more leverage over the far future. This is the basic instrumental convergence thesis.</p>
<p>One example of approximately utility-maximizing agents we know about are biological organisms. Biological organisms model the world and have goals with respect to the world, which are to some degree resistant to wireheading (thus constituting <a href="https://arbital.com/p/environmental_goals/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">environmental goals</a>). They make tradeoffs to achieve these goals, which have correlation with survival and reproduction. The goals that end up likely for biological organisms to have will be (a) somewhat likely to arise from pre-existing processes such as genetic mutation, (b) well-correlated enough with survival and reproduction that an agent optimizing for these goals will be likely to replicate more agents with similar goals. However, these goals need not be <a href="https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">identical with inclusive fitness</a> to be likely goals for biological organisms. Inclusive fitness itself may be too unlikely to arise as a goal from genetic mutation and so on, to be a more popular value function than proxies for it.</p>
<p>However, there are a number of goals and values in the human environment that are not well-correlated with inclusive fitness. These are generally parts of social systems. Some examples include capacity at games such as sports, progress in a research field such as mathematics, and maximization of profit (although, this one is at least related to inclusive fitness in a more direct way than the others). Corresponding institutions which incentivize (generally human) agents to optimize for these goals include gaming/sports leagues, academic departments, and corporations.</p>
<p>It is quite understandable that goals well-correlated with inclusive fitness would be popular, but why would goals that are not well-correlated with inclusive fitness also be popular? Molgbug&#39;s <a href="https://www.unqualified-reservations.org/2007/05/magic-of-symmetric-sovereignty/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Fnargl thought experiment</a> might shed some light on this:</p>
<blockquote>
<p>So let&#39;s modify this slightly and instead look for the worst possible rational result. That is, let&#39;s assume that the dictator is not evil but simply amoral, omnipotent, and avaricious.</p>
<p>One easy way to construct this thought-experiment is to imagine the dictator isn&#39;t even human. He is an alien. His name is Fnargl. Fnargl came to Earth for one thing: gold. His goal is to dominate the planet for a thousand years, the so-called &#34;Thousand-Year Fnarg,&#34; and then depart in his Fnargship with as much gold as possible. Other than this Fnargl has no other feelings. He&#39;s concerned with humans about the way you and I are concerned with bacteria.</p>
<p>You might think we humans, a plucky bunch, would say &#34;screw you, Fnargl!&#34; and not give him any gold at all. But there are two problems with this. One, Fnargl is invulnerable---he cannot be harmed by any human weapon. Two, he has the power to kill any human or humans, anywhere at any time, just by snapping his fingers.</p>
<p>Other than this he has no other powers. He can&#39;t even walk---he needs to be carried, as if he was the Empress of India. (Fnargl actually has a striking physical resemblance to Jabba the Hutt.) But with invulnerability and the power of death, it&#39;s a pretty simple matter for Fnargl to get himself set up as Secretary-General of the United Nations. And in the Thousand-Year Fnarg, the UN is no mere sinecure for alcoholic African kleptocrats. It is an absolute global superstate. Its only purpose is Fnargl&#39;s goal---gold. And lots of it.</p>
<p>In other words, Fnargl is a revenue maximizer. The question is: what are his policies? What does he order us, his loyal subjects, to do?</p>
<p>The obvious option is to make us all slaves in the gold mines. Otherwise---blam. Instant death. Slacking off, I see? That&#39;s a demerit. Another four and you know what happens. Now dig! Dig! (Perhaps some readers have seen Blazing Saddles.)</p>
<p>But wait: this can&#39;t be right. Even mine slaves need to eat. Someone needs to make our porridge. And our shovels. And, actually, we&#39;ll be a lot more productive if instead of shovels, we use backhoes. And who makes those? And...</p>
<p>We quickly realize that the best way for Fnargl to maximize gold production is simply to run a normal human economy, and tax it (in gold, natch). In other words, Fnargl has exactly the same goal as most human governments in history. His prosperity is the amount of gold he collects in tax, which has to be exacted in some way from the human economy. Taxation must depend in some way on the ability to pay, so the more prosperous we are, the more prosperous Fnargl is.</p>
<p>Fnargl&#39;s interests, in fact, turn out to be oddly well-aligned with ours. Anything that makes Fnargl richer has to make us richer, and vice versa.</p>
<p>For example, it&#39;s in Fnargl&#39;s interest to run a fair and effective legal system, because humans are more productive when their energies aren&#39;t going into squabbling with each other. It&#39;s even in Fnargl&#39;s interest to have a fair legal process that defines exactly when he will snap his fingers and stop your heart, because humans are more productive when they&#39;re not worried about dropping dead.</p>
<p>And it is in his interest to run an orderly taxation system in which tax rates are known in advance, and Fnargl doesn&#39;t just seize whatever, whenever, to feed his prodigious gold jones. Because humans are more productive when they can plan for the future, etc. Of course, toward the end of the Thousand-Year Fnarg, this incentive will begin to diminish---ha ha. But let&#39;s assume Fnargl has only just arrived.</p>
<p>Other questions are easy to answer. For example, will Fnargl allow freedom of the press? But why wouldn&#39;t he? What can the press do to Fnargl? As Bismarck put it: &#34;they say what they want, I do what I want.&#34; But Bismarck didn&#39;t really mean it. Fnargl does.</p>
</blockquote>
<p>One issue with the Fnargl thought experiment is that, even with the power of death, Fnargl may lack the power to rule the world, since he relies on humans around him for information, and those humans have incentives to deceive him. However, this is an aside; one could modify the thought experiment to give Fnargl extensive surveillance powers.</p>
<p>The main point is that, by monomaniacally optimizing for gold, Fnargl rationally implements processes for increasing overall resources and efficient conversion between different resources, coherent tradeoffs between different resources, and a coherent system (including legalistic aspects and so on) so as to make these tradeoffs in a rational manner. This leads to a Fnargl-ruled civilization &#34;succeeding&#34; in the sense of having a strong material economy, high population, high ability to win wars, and so on. Molgbug asserts that Fnargl&#39;s interests are well-aligned with ours, which is more speculative; due to convergent instrumentality, Fnargl will implement the sort of infrastructure that rational humans would implement, although the implied power competition would reduce the level of alignment.</p>
<p>By whatever &#34;success&#34; metric for civilizations we select, it is surely possible to do better than optimizing for gold, as it is possible for an organism to gain more inclusive fitness by having values that are more well-aligned with inclusive fitness. But even a goal as orthogonal to civilizational success as gold-maximization leads to a great deal of civilizational success, due to civilizational success being a convergent instrumental goal.</p>
<p>Moreover, the simplicity and legibility of gold-maximization simplifies coordination compared to a more complex proxy for civilizational success. A Fnargl-ocracy can evaluate decisions (such as decisions related to corporate governance) using a uniform gold-maximization standard, leading to a high degree of predictability, and simplicity in prioritization calculations.</p>
<p>What real-world processes resemble Fnargl-ocracy? One example is Bitcoin. Proof-of-work creates incentives for maximizing a certain kind of cryptographic puzzle-solving. The goal itself is rather orthogonal to human values, but Bitcoin nonetheless creates incentives for goals such as creating computing machinery, which are human-aligned due to convergent instrumentality (additional manufacturing of computing infrastructure can be deployed to other tasks that are more directly human-aligned).</p>
<p>As previously mentioned, sports and gaming are popular goals that are fairly orthogonal to human values. Sporting incentivizes humans and groups of humans to become more physically and mentally capable, leading to more generally-useful fitness practices such as weight training, and agency-related mental practices, which people can learn about by listening to sports athletes and coaches. Board games such as chess incentivize practical rationality and general understanding of rationality, including AI-related work such as the <a href="https://en.wikipedia.org/wiki/Minimax" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Minimax algorithm</a>, <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Monte-Carlo Tree Search</a>, and <a href="https://en.wikipedia.org/wiki/AlphaGo" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AlphaGo</a>. Bayesian probability theory was developed in large part to analyze gambling games. Speedrunning has led to quite a lot of analysis of video games and practice at getting better at these games, by setting a uniform standard by which gameplay runs can be judged.</p>
<p>Academic fields, especially STEM-type fields such as mathematics, involve shared, evaluable goals that are not necessarily directly related to human values. For example, number theory is a major subfield of mathematics, and its results are rarely directly useful, though progress in number theory, such as the proof of Fermat&#39;s last theorem, is widely celebrated. Number theory does, along the way, produce more generally-useful work, such as Peano arithmetic (and proof theory more generally), Gödel&#39;s results, and cryptographic algorithms such as RSA.</p>
<p>Corporations are, in general, supposed to maximize profit conditional on legal compliance and so on. While profit-maximization comes apart from human values, corporations are, under conditions of rule of law, generally incentivized to produce valuable goods and services at minimal cost. This example is less like a paperclip maximizer than the previous examples, as the legal and economic system that regulates corporations has been in part designed around human values. The simplicity of the money-maximization goal, however, allows corporations to make internal decisions according to a measurable, legible standard, instead of dealing with more complex tradeoffs that could lead to inconsistent decisions (which may be &#34;money-pumpable&#34; as VNM violations tend to be).</p>
<p>Some systems are relatively more loaded on human values, and less like paperclip maximizers. Legal systems are designed and elaborated on in a way that takes human values into account, in terms of determining which behaviors are generally considered prosocial and antisocial. Legal decisions form precedents that formalize certain commitments including trade-offs between different considerations. Religions are also designed partially around human values, and religious goals tend to be aligned with self-replication, by for example encouraging followers to have children, to follow legalistic norms with respect to each other, and to spread the religion.</p>
<p>The degree to which commonly-shared social goals can be orthogonal to human values is still, however, striking. These goals are a kind of <a href="https://en.wikipedia.org/wiki/MacGuffin" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">MacGuffin</a>, as <a href="https://thezvi.wordpress.com/2017/12/31/book-review-the-elephant-in-the-brain/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Zvi wrote about</a>:</p>
<blockquote>
<p>Everything is, in an important sense, about these games of signaling and status and alliances and norms and cheating. If you don&#39;t have that perspective, you need it.</p>
<p>But let&#39;s not take that too far. That&#39;s not all such things are about.  Y still matters: you need a McGuffin. From that McGuffin can arise all these complex behaviors. If the McGuffin wasn&#39;t important, the fighters would leave the arena and play their games somewhere else. To play these games, one must make a plausible case one cares about the McGuffin, and is helping with the McGuffin.</p>
<p>Otherwise, the other players of the broad game notice that you&#39;re not doing that. Which means you&#39;ve been caught cheating.</p>
<p>Robin&#39;s standard reasoning is to say, suppose X was about Y. But if all we cared about was Y, we&#39;d simply do Z, which is way better at Y. Since we don&#39;t do Z, we must care about something else instead. But there&#39;s no instead; there&#39;s only in addition to.</p>
<p>A fine move in the broad game is to actually move towards accomplishing the McGuffin, or point out others not doing so. It&#39;s far from the only fine move, but it&#39;s usually enough to get some amount of McGuffin produced.</p>
</blockquote>
<p>By organizing around a MacGuffin (such as speedrunning), humans can coordinate around a shared goal, and make uniform decisions around this shared goal, which leads to making consistent tradeoffs in the domain related to this goal. The MacGuffin can, like gold-maximization, be basically orthogonal to human values, and yet incentivize instrumental optimization that is convergent with that of other values, leading to human value satisfaction along the way.</p>
<p>Adopting a shared goal has the benefit of making it easy to share perspective with others. This can make it easier to find other people who think similarly to one&#39;s self, and develop practice coordinating with them, with performance judged on a common standard. Altruism can have this effect, since in being altruistic, individual agents &#34;erase&#34; their own <a href="https://en.wikipedia.org/wiki/Indexicality" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">index</a>, sharing an agentic perspective with others; people meeting friends through effective altruism is an example of this.</p>
<p>It is still important, to human values, that the paperclip-maximizer-like processes are not superintelligent; while they aggregate compute and agency across many humans, they aren&#39;t nearly as strongly superintelligent as a post-takeoff AGI. Such an agent would be able to optimize its goal without the aid of humans, and would be motivated to limit humans&#39; agency so as to avoid humans competing with it for resources. Job automation worries are, accordingly, in part the worry that existing paperclip-maximizer-like processes (such as profit-maximizing corporations) may become misaligned with human welfare as they no longer depend on humans to maximize their respective paperclips.</p>
<p>For superintelligent AGI to be aligned with human values, therefore, it is much more necessary for its goals to be directly aligned with human values, even more than the degree to which human values are aligned with inclusive evolutionary fitness. This requires overcoming <a href="https://unstableontology.com/2021/10/28/selfishness-preference-falsification-and-ai-alignment/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">preference falsification</a>, and taking indexical (including selfish) goals into account.</p>
<p>To conclude, paperclip-maximizer-like processes arise in part because the ability to make consistent, legible tradeoffs is a force multiplier. The paperclip-maximization-like goals (MacGuffins) can come apart from both replicator-type objectives (such as inclusive fitness) and human values, although can be aligned in a non-superintelligent regime due to convergent instrumentality. It is hard to have a great deal of influence over the future without making consistent tradeoffs, and already-existing paperclip-maximizer-like systems provide examples of the power of legible utility functions. As automation becomes more powerful, it becomes more necessary, for human values, to design systems that optimize goals aligned with human values.</p>
<br/><br/><a href="https://www.alignmentforum.org/posts/Z8C29oMAmYjhk2CNN/non-superintelligent-paperclip-maximizers-are-normal#comments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Discuss</a>