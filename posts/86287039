<p>Today’s substack will be buried in most peoples inboxes and attention as it’s Nobel Prize day. But I’m trying to keep to schedule, so I’m going to do three things in this substack. First, I’m going to do my normal thing about upcoming workshops. Then I’m going to discuss a little about what I’m calling the story of synthetic control as a lead in to discussing synthetic diff in diff, a more recent synthetic control estimator published last year by a Dream Team of econometricians.  And then I’m going to say congratulations to Claudia Goldin whose work on economic history, labor markets, technology and women has been some of the most exciting, interesting and influential work of the last 50 years and is this years winner of the Nobel Prize. Before my little ChatGPT-4 experiment, I had picked Goldin and Katz to win so alas I don’t get bragging rights. But what an exciting prize. I am currently teaching “the race between education and technology” for my Econ 1305 class and can’t wait to tell the students about it tomorrow. </p><p>Head over to Marginal Revolution blog where Tyler and Alex will soon write up an excellent breakdown of the winners contributions to their field and the science as a whole, a tradition those two have been doing now for maybe two decades. I can’t do it justice but congratulations Dr Goldin! </p><p><strong>Diff-in-diff workshop announcement</strong></p><p>First let me get my plug out of the way. Starting this coming Saturday, I will be teaching my Causal Inference II workshop at my Mixtape Sessions platform.  It runs four days over two weekends, but this time it is a little different, in that this time I am stripping out the synthetic control material so that it is only diff-in-diff. I did this because I wanted to cover more synthetic control, but that was never possible with the old format, and because of my meandering style, I often would leave myself with only a little bit of time to cover synth anyway.  So now there is a Causal Inference III workshop scheduled for mid November that’s two days long devoted only to synthetic control, and who knows — maybe over time that one will get larger too.  But in the meantime, we have Causal Inference II and it starts on Saturday this week.  Would love to see you there, so sign up <a href="https://www.mixtapesessions.io/session/ci_ii_oct14/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a> if you’re interested. Here’s the pictures I usually post. And tell your family and friends about it. I think people have found the workshop helpful.</p><p>But the second thing I’m going to do today is flesh out a little of the narrative of synthetic control as I move into a week of discussing synthetic diff in diff. So let me explain a little of what I mean by that now. </p><p><strong>Building out a narrative of synthetic control</strong></p><p>I have noticed that I use the word “narrative” and “story” more now than I ever have. It even has crept into my teaching and my writing. I teach econometrics in a kind of sweeping style like I’m telling the story of an procedure’s evolution. Evolution I think when you think of it is a kind of story — it’s a story of change, growth, arcs, history, creation and death. And econometrics has that too or at least the way it exists in my mind it does.</p><p>So I am always trying to figure out the narratives for econometrics because one, I think stories best describe scientific progress anyway; two, I find that it is helpful pedagogically to know where these things came from and why they changed. After all, evolution itself is about adaptation. There are spontaneous changes no doubt but there is also selection and adaptation. So I find it helpful if we can pin these things down. For instance, I tend to emphasize the role that the empirical labor crisis played in the 1970s as well as the availability of rich micro data for the direction that Orley, Card, Lalonde and others took empirical labor inside the Princeton Industrial Relations Section. But third, and maybe most of all, I love a good story. Who doesn’t?</p><p>Finding the narrative for synthetic control after Abadie’s original paper has eluded me somewhat, but part of that has been because of personal and professional focus. I’ve been myopic for several years for really mastering my own narrative of diff in diff, for one. When it suddenly became clear that the empirical practices around diff in diff were being questioned, given it was thought to be the easiest method to grasp and use, I found myself compelled to figure out both what was going on and to develop a narrative eventually that would enable me to teach it. The narrative was the way I get I could best deliver a lot of material to people I think as diff in diff in the end is truthfully a lot of material. </p><p>So after the book came out, the one real investment I made was in difference-in-differences, and then that turned into Mixtape Sessions, and I became very focused on being an ambassador for causal inference methods to anyone who would listen.  That meant, though, I have watched from the sidelines as more advances were made, whether that was in IV or synthetic control or machine learning more generally, and so I just have decided to use this new workshop on synthetic control, <a href="https://www.mixtapesessions.io/session/ci_iii_nov11/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Causal Inference III, scheduled for mid November,</a> to build out, what I call, “the narrative of synthetic control”. And I’ll probably just use the substack to play around with it some. </p><p>To help you understand what I mean, consider the diff-in-diff narrative that I use. It goes a little like this:</p><ol><li><p>Four averages and three subtractions (Orley’s phrase) and references to Princeton Industrial Relations Section with lots of video clips to emphasize its origin, maybe throwing in some John Snow and Ignaz Semmelweis in there too</p></li><li><p>Potential outcomes, ATT and parallel trends</p></li><li><p>OLS specifications, event studies and triple differences for simple 2x2 scenarios </p></li><li><p>Falsifications and other practical advice </p></li><li><p>Compositional change in repeated cross sections  </p></li><li><p>Introducing covariates correctly and incorrectly  </p></li><li><p>Staggered adoption, heterogenous treatment effects, Bacon decomposition and deep dives on all the new estimators</p></li></ol><p>And that’s really the narrative.  It moves forward in time, it emphasizes people (I use videos even), and it unfolds like a flower with increasing complexity and novel insights along the way. I think when one grasps the various Acts, then they can use it more or less to piece the estimators together so that it’s less random, less spotty and just more seamless. The less that estimators just stand independent of one another, the less erratic and random econometrics seems to students, the more it seems like one large tree with many branches, many stems, many leaves and even budding fruit. </p><p>The challenge with synthetic control has been that I have not quite had the same luck find that narrative structure yet.  I have had a few, sort of, but they haven’t quite fully come together into a full story.  The way I’ve been trying has been like this:</p><ol><li><p>Abadie’s synth: non-negative weights and the convex hull, randomization inference and one treatment group </p></li><li><p>Augmented synthetic control: “some negative weighting but the least worst form of it” plus bias adjustment, new forms of inference (e.g., jackknife, conformal inference), but still one treatment group</p></li><li><p>Matrix completion with nuclear norm regularization: introducing vertical versus horizontal regression concepts, introducing the block concepts, multiple treated units, unifying unconfoundedness and synthetic control in a single framework, unifying multiple designs (one period treatment; one unit treatment; staggered adoption), low rank matrices and penalized regressions</p></li><li><p>Augmented synthetic control with staggered adoption: another method for aggregating over multiple synthetic controls</p></li><li><p>Synthetic difference-in-differences: creating a synthetic control that weakens dependence on parallel trends; conduct diff-in-diff to weaken dependence on the convex hull</p></li></ol><p>This narrative has a few flowering directions. First, there is the addressing of the convex hull, the non-negative weighting. What if it is impossible to achieve? Then there seem to be a few contributions — you can use negative weights and extrapolate but that requires some model dependency, and in some respects, model dependency and extrapolation isn’t really in the spirit of the nonparametric matching frameworks that synth seems to belong to. Still, you can do it. As I told my audience in Barcelona, there is no way to use a non-negatively weighted average of donor pool footballers that will ever equal Messi. So if you insist on that requirement, then you cannot study Messi. When faced with that predicament, negative weighting to allow extrapolation from lesser qualified footballers into the heavens where Messi plays may be all we can do. </p><p>But then there were other solutions. Doudchenko and Imbens in what appears to be an influential 2016/2017 working paper that will never be published I guess, begin to fiddle with seemingly small things that allowed for the synthetic control to look like the treatment group but standing a little far away by introducing level shifts. This idea that someone looks almost exactly like me but is one foot shorter does seem like metaphorically one of the directions we saw synth go. Even augmented synth with its focus on negative weighting seemed like that was the spirit too. It found the shortest distance to the edge of the hull that when combined with bias adjustment erased the remaining bias. So the molding of the synthetic control remained but the purity of finding an exact match did seem to weaken due to a bit of accepting the realism that outliers will be definition be hard to exactly match. </p><p>And then there is the obvious direction of expanding the method to more than one unit and more than one treatment date, which was how I have always envisioned matrix completion with nuclear norm regularization. </p><p>But while I like the direction of this narrative, it really leaves out a ton of people.  Bruno Ferman and Christine Pinto, for instance, have not yet made it into this story, and they are arguably two of the more prominent synthetic control econometricians, and have been for quite some time, after Abadie, Athey and Imbens. Then there is also Abadie’s new work with L’Hour which I’ve not read whatsoever.  Plus David Powell has over the years made several of his own contributions. There are various observations made about factor models, interactive fixed effects models, lagged outcome unconfoundedness, the asymptotics in time and the technical data requirements for the length of the pre treatment time series, as well as numerous suggestions for how to develop more convincing robustness measures and even just basic suggestions of data visualization as well as new methods of inference. </p><p> So I am really trying to grasp an overarching narrative that makes sense for me, as the narratives help people and me navigate what is otherwise just a bunch of papers.  Like I said, I think the worst way to teach econometrics is just to treat it like a menu of estimators.  I don’t like it because it just feels like a machine gun of estimators: “and you get a synthetic control estimator, and you get a synthetic control estimator, and you get a synthetic control estimator!”</p><iframe src="https://www.youtube-nocookie.com/embed/pviYWzu0dzk?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" allowfullscreen="true" width="728" height="409" sandbox="allow-scripts allow-same-origin allow-popups" loading="lazy"></iframe><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0ef0231-231a-4faf-afbe-fe4b460f0e44.heic" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/YnhDRYPKN7-KPWhU_td754WTlzjPPJj-JjXoJ_xsHcA=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZjMGVmMDIzMS0yMzFhLTRmYWYtYWZiZS1mZTRiNDYwZjBlNDQuaGVpYw== 424w, https://reader.miniflux.app/proxy/1Dzh4gGSfRGHdiebALxbp03VcMEBYWEWRcIb0oC0k4A=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZjMGVmMDIzMS0yMzFhLTRmYWYtYWZiZS1mZTRiNDYwZjBlNDQuaGVpYw== 848w, https://reader.miniflux.app/proxy/scAwap84RjGf2RgcvoUEi7CWpaUJPpOiVvYmaKO5xA8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYzBlZjAyMzEtMjMxYS00ZmFmLWFmYmUtZmU0YjQ2MGYwZTQ0LmhlaWM= 1272w, https://reader.miniflux.app/proxy/jEeoa_TVRPYAEvbqtImRdKJI0EtnGiqm2SzMtM-whBQ=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYzBlZjAyMzEtMjMxYS00ZmFmLWFmYmUtZmU0YjQ2MGYwZTQ0LmhlaWM= 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/mSGp69Q0OEccWKfhHLIjdy-24cQJGROzGtneTNjbLy8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYzBlZjAyMzEtMjMxYS00ZmFmLWFmYmUtZmU0YjQ2MGYwZTQ0LmhlaWM=" alt="" srcset="https://reader.miniflux.app/proxy/zkvGvEdfE7nPvJtp49PznnNUQvDuk0_ThSF5LdhLq8A=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZjMGVmMDIzMS0yMzFhLTRmYWYtYWZiZS1mZTRiNDYwZjBlNDQuaGVpYw== 424w, https://reader.miniflux.app/proxy/QdWz7SFIGFOW61SUTRrPpAhIjNjpF5Ugi6lBFfZrx9c=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZjMGVmMDIzMS0yMzFhLTRmYWYtYWZiZS1mZTRiNDYwZjBlNDQuaGVpYw== 848w, https://reader.miniflux.app/proxy/1t_1FvZxoTVYilMz9lSVFuitg488Ufya2rTCrr0mjMc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYzBlZjAyMzEtMjMxYS00ZmFmLWFmYmUtZmU0YjQ2MGYwZTQ0LmhlaWM= 1272w, https://reader.miniflux.app/proxy/mSGp69Q0OEccWKfhHLIjdy-24cQJGROzGtneTNjbLy8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYzBlZjAyMzEtMjMxYS00ZmFmLWFmYmUtZmU0YjQ2MGYwZTQ0LmhlaWM= 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p>So, like I said, what I’m wanting to do is start using the substack to help me flesh out that narrative while also discussing these estimators as I continue developing this new workshop on synthetic control with the goal being to be as thorough as respectful of the full story of synthetic control and all it’s people and pieces as I can. </p><p>So to help me, I have some upcoming one-on-one meetings with people I consider to be experts both in synthetic control as a broad literature (I’m meeting with Bruno Ferman soon) as well as applying some of these new estimators.  And one of the very first ones I’ve decided to focus on intently is synthetic difference in differences. </p><p><strong>Synthetic difference in differences</strong></p><p>Synthetic difference-in-differences was published in 2022 in The American Economic Review, our flagship journal in economics, and has five coauthors.  They are <a href="https://sites.google.com/view/dmitry-arkhangelsky/home" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Dmitry Arkhangelsky</a>, <a href="https://www.gsb.stanford.edu/faculty-research/faculty/susan-athey" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Susan Athey</a>, <a href="https://davidahirshberg.bitbucket.io" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">David Hirshberg</a>, <a href="https://www.nobelprize.org/prizes/economic-sciences/2021/imbens/facts/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Guido Imbens</a>, and <a href="https://web.stanford.edu/~swager/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Stefan Wager</a>.  The <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20190159" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AER version is here</a>, but an ungated version is <a href="https://browse.arxiv.org/pdf/1812.09970.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>. I have been slow, maybe even reluctant to be honest, to learn this method because it was going to require updating so much human capital. I hadn’t probably shared that but I’ve gotten exhausted by using my eraser so often. Going back to basics and starting over with diff in diff changed my brain but as with most marathons I also hit a wall around the 18th mile and just started to slow down. Synth did spooked me because I sensed I’d have to do a major rebuild, but with some personal milestones out of the way, and being hyper focused on completing the book, I’m back to thinking that my book needs chapters that have narrative, plot, and build from basics. So this week, my goal is to write something new everyday even if only a little about synth did starting now. </p><p>Let me start using my own words. Synth did is one of these new causal panel estimators born within a potential outcomes framework. The potential outcomes framework frames causality in terms of contrasts between two imaginary worlds — the haves and the have nots — and so the focus becomes very much about how to best impute a missing potential outcome when it’s missing because of some irregular, and yet natural, treatment assignment. </p><p>Ordinarily, the causal panel models born in the inspirational wake of Rubin’s way of framing causality has been the diff in diff estimates. Though diff in diff is not narrowly attached to the panel setup, as it can be used too with the repeated cross section, it is nonetheless one of the most common tools the applied worker looking to estimate the effect of some law or set of laws uses. It fit within a regression framework easily, but over time it became clearer and clearer that the parameter it estimated could only be correctly estimated if the comparison group was a good one despite not being a random one. And the modeling assumption needed to use that control group was one that correctly proxied for what would have been over time has the treatment never happened to the treatment group. The parallel trends assumption, as it became known as, was simply another way of saying that the control group looked like the treatment group in its counterfactual difference over time.</p><p>The new did estimators I think show, maybe even unintentionally; their own weakness when taking in aggregate. The CS estimator for instance uses whichever groups aren’t treated at that moment to estimate small building block parameters called the group time ATT. That could be the never or not yet treated. But the SA used the last cohort or the never treated. All of them use untreated groups for estimating causal effects, as Bacons decomposition showed otherwise the assumptions one needs become too expensive to justify, but which ones — that is sometimes the subtle detail that does not get mentioned as the estimators may actually use slightly different chosen untreated controls like I said.</p><p>Well what if both or neither or only one of those were true in the meta scheme of things? The main criteria used for selecting the control group with contemporary did was just that it isn’t treated when the calculations are made, but beyond that finding the best controls is not done. There is the introduction of covariates but that is primarily just to reset the parallel trends assumption, not so much to find the best set of controls for which the parallel trends assumption might hold. An estimated causal effect is only as good as the comparison group is at proxying for the counterfactual, and the use of parallel trends birthed a new wave of ways of thinking by focusing on imputing missing potential outcomes using control groups enforced to be good stand ins but synthetic control sort of also came in as an alternative paradigm to move away from parallel trends and instead find a kind of nearest neighbor match in the panel context.  They each seemed like different ways to skin the same cat, and one could make arguments that synthetic control, if it could advance enough to be as useful as did, might have some attributes that make it appealing. </p><p>So as I ran out of time, and the Nobel Prize will be announced soon, I am just going to wind down and say that synthetic did seems to be an estimator with an unusual promise, and one I think applied people had been attempting to create on their own for years. What if we created a synthetic control that was similar on the trends — a mashup of players who look like Messi over time but not at the same level as no one is at his level, admittedly — and then once we find that match, use diff in diff to address the level differences between them? While I can’t name the applied papers I’d seen try things like this, I know I’ve seen people try it where they’d first match units using propensity scores, pair them, then use did on the matched pairs. The properties of those methods, as with many gut instinct approaches applied researchers have taken, was not known but it sounds like not surprisingly, it was indeed the right solution. There was a way to create a matched sample to overcome the failure of parallel trends and then use did to overcome the failure of achieving an exact match implied by the convex hull requirement. And that’s synthetic did. It accomplishes this with two sets of estimated weights — the already well documented unit weights which Abadie introduced combined with estimated time weights. The introduction of time weights attempts to bring in more weighting from the pre treatment period which resemble that of the post treatment period. It’s this introduction of the time weights that is the most novel for me so I will be focused on that this week and hope to be in a good place about it before the weeks conclusion.</p><p>The R package for implementing the synth did methods developed here is available at https://github.com/synth-inference/synthdid. The associated vignette is at https://synth- inference.github.io/synthdid/.  The Stata package, sdid, is described <a href="https://www.damianclarke.net/research/papers/SDID.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">here</a>. I’ll try to make up applications this week. This substack was free, but the others this week will be behind a paywall as I continue experimenting with that. Cheers!</p><p>Scott&#39;s Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.</p>