<p>The AI watermarking concept fails an analysis of the relative cost and frequency of <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Type I vs Type II errors</a>.</p><p>Suppose the law required every AI generated image to have a bright red border around it, as Stuart Russell <a href="https://www.youtube.com/watch?v=hm1zexCjELo" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">just proposed</a> before the Senate.</p><p>This could maybe work if there was just one company in control of the technology; if the internet made it easy to trace the provenance of shared images; and if real photos vastly outnumbered AI generated ones. Yet in reality, the technology is largely open source; tracing provenance is nearly impossible; and generated images will soon vastly outnumber real photos.</p><p>Given this context, watermarking laws could easily backfire by <em>enhancing</em> trust in deepfakes that lack said watermark. That is, the rate of false negatives is impractically high.</p><p>There is a <a href="https://en.wikipedia.org/wiki/The_Problem_of_Social_Cost" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Coasian element</a> to this: for whom is the social cost associated with content labelling lowest? There are two options: either label all deepfakes as fake, or label all real images as real.</p><p>The latter option seems clearly lower cost and easier to enforce. Rather than label all deepfakes as fake, digital cameras will label all real photos as real, whether through imperceptible watermarkings or some kind of enhanced exif data. In turn, when presented with a salacious image, we will disregard it by default absent proof of its veracity, rather than believe everything we see by default absent proof of its fakeness.</p><p>Thanks for reading Second Best! Subscribe for free to receive new posts and support my work.</p>