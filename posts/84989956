<p>Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p>This week we’re looking closely at AI legislative efforts in the United States, including:</p><ul><li><p>Senator Schumer’s AI Insight Forum</p></li><li><p>The Blumenthal-Hawley framework for AI governance</p></li><li><p>Agencies proposed to govern digital platforms</p></li><li><p>State and local laws against AI surveillance</p></li><li><p>The National AI Research Resource (NAIRR)</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p></li></ul><h2>Senator Schumer’s AI Insight Forum</h2><p>The CEOs of more than a dozen major AI companies gathered in Washington on Wednesday for a hearing with the Senate. Organized by Democratic Majority Leader Chuck Schumer and a bipartisan group of Senators, this was the first of many hearings in their AI Insight Forum. </p><p>After the hearing, Senator Schumer <a href="https://rollcall.com/2023/09/13/theres-consensus-not-details-on-ai-regulation-schumer-says/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">said</a>, “I asked everyone in the room, ‘Is government needed to play a role in regulating AI?’ and every single person raised their hands.” Elon Musk, CEO of xAI, <a href="https://www.youtube.com/watch?v=Z5fy4S3VxNk" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">called</a> the hearings “a great service to humanity.” </p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1e550ba-53e1-4ea0-b51b-e690a56e03fd_2754x1134.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/3KMLKFU5JmhLThFksH9xYHccx028kXAqM7lOGGxqP3g=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZhMWU1NTBiYS01M2UxLTRlYTAtYjUxYi1lNjkwYTU2ZTAzZmRfMjc1NHgxMTM0LnBuZw== 424w, https://reader.miniflux.app/proxy/xQCN1Zr1zWuE504qyC1JDSNsrpG96t12Ywrmv_1zQTw=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZhMWU1NTBiYS01M2UxLTRlYTAtYjUxYi1lNjkwYTU2ZTAzZmRfMjc1NHgxMTM0LnBuZw== 848w, https://reader.miniflux.app/proxy/k7M6sSDWoOwbPJjTkNMm__aAxiAJ19imeJY53qpPnKQ=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYTFlNTUwYmEtNTNlMS00ZWEwLWI1MWItZTY5MGE1NmUwM2ZkXzI3NTR4MTEzNC5wbmc= 1272w, https://reader.miniflux.app/proxy/h_yPm9XOrA8PZ6RJD8mVVjVYI5V4AFsldyqFSBXzqY4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYTFlNTUwYmEtNTNlMS00ZWEwLWI1MWItZTY5MGE1NmUwM2ZkXzI3NTR4MTEzNC5wbmc= 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/rBY93d8RfZB-zVZdY0_M2lLOJrHLtfVPo1lPLzVr1Sk=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYTFlNTUwYmEtNTNlMS00ZWEwLWI1MWItZTY5MGE1NmUwM2ZkXzI3NTR4MTEzNC5wbmc=" alt="" srcset="https://reader.miniflux.app/proxy/QnR-uIezCsSk1oTgLL5uN8Naq59erCyhOCsBYNx9C6o=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZhMWU1NTBiYS01M2UxLTRlYTAtYjUxYi1lNjkwYTU2ZTAzZmRfMjc1NHgxMTM0LnBuZw== 424w, https://reader.miniflux.app/proxy/yb1GEMyNqiUyjdRRSpZlrDNO2Qx4Zmz5eDNlsb4aes4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkZhMWU1NTBiYS01M2UxLTRlYTAtYjUxYi1lNjkwYTU2ZTAzZmRfMjc1NHgxMTM0LnBuZw== 848w, https://reader.miniflux.app/proxy/v0-1Y-C5c_yJi_3HxBomf33l7WZrU1wfQRDcVcq7i38=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYTFlNTUwYmEtNTNlMS00ZWEwLWI1MWItZTY5MGE1NmUwM2ZkXzI3NTR4MTEzNC5wbmc= 1272w, https://reader.miniflux.app/proxy/rBY93d8RfZB-zVZdY0_M2lLOJrHLtfVPo1lPLzVr1Sk=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGYTFlNTUwYmEtNTNlMS00ZWEwLWI1MWItZTY5MGE1NmUwM2ZkXzI3NTR4MTEzNC5wbmc= 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p>Senator Josh Hawley raised <a href="https://www.cnn.com/2023/09/13/tech/schumer-tech-companies-ai-regulations/index.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">concerns</a> that despite the hearings, “nothing is advancing” in terms of legislation. Below, we’ll discuss several bills on AI policy which have been introduced to Congress, none of which have come to a vote. </p><h2>The Blumenthal-Hawley Framework</h2><p>Senator Hawley recently introduced a <a href="https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanaiframework.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">framework for AI legislation</a> alongside Senator Richard Blumenthal. The pair lead the Senate Judiciary Subcommittee on Privacy, Technology and the Law, and have <a href="https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">hosted</a> <a href="https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-principles-for-regulation" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">three</a> <a href="https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-legislating-on-artificial-intelligence" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">hearings</a> on AI policy over the last five months. </p><p>The Blumenthal-Hawley framework recommends:</p><ul><li><p><strong>Licensing. </strong>General purpose AI systems and AIs used in high-risk situations should be required to obtain a license from an independent oversight body. There should be strong rules against conflicts of interest to prevent regulatory capture. </p></li><li><p><strong>Legal Liability. </strong>Congress should clarify that <a href="https://www.vox.com/recode/2020/5/28/21273241/section-230-explained-supreme-court-social-media" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Section 230</a> does not apply to AI (Sens. Blumenthal and Hawley have introduced a <a href="https://www.congress.gov/bill/118th-congress/senate-bill/1993" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">bill</a> that would accomplish this), and hold AI companies liable for emerging harms such as deepfakes and election interference. </p></li><li><p><strong>Transparency. </strong>Companies should be required to publicly report details about their training data, and label AI outputs with easily identifiable watermarks. The federal AI oversight body should maintain a database of AI harms. </p></li><li><p><strong>Human in the loop. </strong>Users should be notified when they are interacting with an AI system, and should have the right to a human review of AI decisions. </p></li><li><p><strong>AI Proliferation. </strong>To keep rogue actors and adversary nations from obtaining frontier AI systems, the US should utilize export controls, sanctions, and other restrictions.</p></li></ul><p>This framework was <a href="https://www.judiciary.senate.gov/imo/media/doc/2023-09-12_pm_-_testimony_-_smith.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">endorsed</a> by Microsoft President Brad Smith in a hearing last week. In California, state Senator Scott Wiener introduced a similar <a href="https://legiscan.com/CA/text/SB294/id/2840929" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">bill of intent</a> for state legislation on AI. The details of both proposals still need to be fleshed out into concrete policies for AI governance.</p><h2>Agencies Proposed to Govern Digital Platforms</h2><p>Whereas AI policy discussions are often conceptual and lack concrete legislative proposals, there are many concrete proposals to regulate digital platforms including social media. These bills could affect AI developers, and offer insights for those crafting AI legislation. </p><p>In June, a bill that would create a federal agency to govern digital platforms was introduced by Democratic Senators <a href="https://www.bennet.senate.gov/public/_cache/files/7/9/791b3c4d-a3ee-471e-9a5b-b33856ba4d7e/2138DDEAE263E62B8005385899B6C3CB.2023-dpca-section-by-section.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Michael Bennet and Peter Welch</a>. The agency would have “a broad mandate to promote the public interest” via methods including rules, civil penalties, hearings, investigations, and research. Senators <a href="https://www.warren.senate.gov/newsroom/press-releases/warren-graham-unveil-bipartisan-bill-to-rein-in-big-tech" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Elizabeth Warren and Lindsey Graham</a> introduced a similar bill in July requiring digital platforms to obtain a federal license each year. </p><p>These bills would <a href="https://techpolicy.press/senators-propose-a-licensing-agency-for-ai-and-other-digital-things/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">likely apply to AI companies</a>, but many AI-specific policies are not included in these bills. For example, they do not require companies to disclose their training data, nor mandate red-teaming and evaluations before the release of new AI models. The federal agency could work on making rules about AI systems, but amending the bill itself to address AI-specific concerns could provide a clearer mandate for the new agency.</p><p>Despite <a href="https://theaipi.org/poll-shows-overwhelming-concern-about-risks-from-ai-as-new-institute-launches-to-understand-public-opinion-and-advocate-for-responsible-ai-policies/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">public support for regulating AI</a> and <a href="https://www.axios.com/2021/12/15/social-media-regulation-poll" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">social media</a>, there is no guarantee that these bills will become law. Senator <a href="https://www.cnbc.com/video/2023/09/13/senator-ted-cruz-federal-regulation-of-ai-makes-no-sense-at-all.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Ted Cruz</a> recently came out against AI regulation, and others may follow. Last year, no vote was held on <a href="https://time.com/6243256/schumer-kills-antitrust-big-tech-bills/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">two bills with broad bipartisan support</a> to regulate technology companies after <a href="https://www.wsj.com/articles/big-tech-has-spent-36-million-on-ads-to-torpedo-antitrust-bill-11654767000" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">the industry spent $37 million lobbying against them</a>.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Deepfakes and Watermarking Legislation</h2><p>An AI-specific proposal put forth in several recent bills intends to combat deepfakes and AI-generated misinformation. By using AI to fabricate text, images, videos, and audio, scammers have run <a href="https://www.theguardian.com/us-news/2023/jun/14/ai-kidnapping-scam-senate-hearing-jennifer-destefano" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">fake kidnapping scams</a> and even <a href="https://insiderpaper.com/china-scammer-uses-ai-to-pose-as-mans-friend-steal-millions/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">stole $600,000 from a Chinese businessman</a>. More powerful AI systems could soon be used for widespread misinformation campaigns of <a href="https://arxiv.org/abs/2303.08721" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">persuasion</a> and <a href="https://arxiv.org/abs/2308.14752" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">deception</a>.</p><p>Several recent bills intend to combat AI deepfakes with clearly visible notices of AI-generated content. The <a href="https://www.congress.gov/bill/116th-congress/house-bill/3230/amendments" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">DEEP FAKES Accountability Act of 2019</a> first proposed the idea, and the <a href="https://www.congress.gov/bill/118th-congress/senate-bill/2691" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI Labeling Act of 2023</a> proposes a similar requirement where the primary responsibility for providing benchmarks would be placed on AI developers rather than downstream users. The <a href="https://www.klobuchar.senate.gov/public/index.cfm/2023/5/klobuchar-booker-bennet-introduce-legislation-to-regulate-ai-generated-content-in-political-ads" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">REAL Political Advertisements Act</a> is more narrowly targeted, mandating disclosure of AI-generated content only in political advertisements. </p><p>Clearly visible notices of AI-generated content might be intrusive or unpleasing to viewers. Another potential solution is <a href="https://arxiv.org/abs/2301.10226" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">watermarking</a>, the practice of embedding statistical patterns in AI outputs which are typically unnoticed by a consumer, but which can be detected using a watermark detection tool. The <a href="https://defensescoop.com/2023/07/12/senators-propose-new-dod-led-prize-competition-for-tech-to-detect-and-watermark-generative-ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">National Defense Authorization Act for 2024</a> directed DARPA to hold a prize competition for the development of AI watermarking techniques. </p><h2>State and Local Laws Against AI Surveillance</h2><p>While federal laws tend to get the most attention, it&#39;s important to note that <a href="https://epic.org/the-state-of-state-ai-laws-2023/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">states and local governments are also playing a role in AI regulation</a>, particularly against AI surveillance.</p><p>Alabama, Colorado, Washington, and Baltimore, Maryland have <a href="https://epic.org/the-state-of-ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">all passed laws</a> restricting police use of facial recognition technology. Some have banned it outright, while others require a search warrant and prohibit its use in sensitive locations such as schools. </p><p>These state and local initiatives highlight concerns about civil liberties and the willingness of states and cities to check the unbridled use of AI by law enforcement agencies.</p><h2>National AI Research Resource (NAIRR)</h2><p>The <a href="https://www.heinrich.senate.gov/imo/media/doc/create_ai_act_fact_sheet1.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CREATE AI Act</a> is a bipartisan bill which has been designed and developed with strong support over the last few years. It would establish a National AI Research Resource (NAIRR) to provide compute and data to AI researchers outside of industry.</p><p>The <a href="https://eshoo.house.gov/sites/evo-subsites/eshoo.house.gov/files/evo-media-document/eshoo_043_xml.pdf" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">bill</a> states that priority access will be given to research projects focused on “privacy, ethics, civil rights and civil liberties, safety, security, risk mitigation, and trustworthiness.” Today, <a href="https://almanac.eto.tech/topics/ai-safety/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">only 2% of AI research</a> focuses on key topics relevant for safety. But by targeting these neglected topics in AI safety, NAIRR could substantially boost the amount of research in the field.</p><p>Creating “testbeds&#39;&#39; for evaluating AI systems is another statutory requirement for NAIRR. They would be tasked with working with NIST to “develop a comprehensive catalog of open AI testbeds.” These could include <a href="https://arxiv.org/abs/2305.15324" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">evaluations of extreme risks</a> such as dangerous capabilities and misalignment. Perhaps a future federal regulator could also benefit from access to a suite of AI evaluation testbeds. </p><p>Decisions about which projects to support would be made by federal agencies, as well as an “operating entity” – likely a university or federally funded research and development center – chosen to run the day-to-day operations of the cluster. The work would be overseen by the National Science Foundation and expert advisory committees. </p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>Links</h2><ul><li><p>The President of the European Union <a href="https://twitter.com/EU_Commission/status/1702295053668946148" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">says</a> that “mitigating the risk of extinction from AI should be a global priority.”</p></li><li><p>The <a href="https://time.com/collection/time100-ai/6309050/dan-hendrycks/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">TIME100 list on AI</a> included Dan Hendrycks, Executive Director of the Center for AI Safety, alongside others including Sam Altman, Eric Schmidt, and Ilya Sutskever. </p></li><li><p>AI scientist Richard Sutton alarmingly <a href="https://twitter.com/DanHendrycks/status/1700614592035262968" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">says</a> that AIs “could displace us from existence” and that  “we should not resist succession.” </p></li><li><p>Chinese companies have <a href="https://www.reuters.com/technology/teardown-huaweis-new-phone-shows-chinas-chip-breakthrough-2023-09-04/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">designed and manufactured</a> a new 7nm computer chip. </p></li><li><p><a href="https://www.nytimes.com/2023/09/11/us/politics/china-disinformation-ai.html" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">China spreads AI-generated images</a> of the fires in Hawaii, falsely claiming they were caused by an American “weather weapon.” </p></li><li><p>Israeli Prime Minister <a href="https://twitter.com/netanyahu/status/1703823935060451628" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Benjamin Netanyahu</a> holds a roundtable discussion on AI safety with Elon Musk, Max Tegmark, and Greg Brockman. </p></li><li><p>The Atlantic <a href="https://www.theatlantic.com/magazine/archive/2023/09/sam-altman-openai-chatgpt-gpt-4/674764/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">interviews Sam Altman</a> on his work at OpenAI. </p></li><li><p>An autonomous AI system defeated human pilots in <a href="https://www.nature.com/articles/s41586-023-06419-4" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">racing drones</a>. </p></li><li><p><a href="https://fas.org/publication/trust-issues/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Less than 15%</a> of AI grants from the National Science Foundation support trustworthy AI topics including robustness, interpretability, and fairness, finds a recent analysis. </p></li></ul><p>See also: <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS website</a>, <a href="https://twitter.com/ai_risks?lang=en" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS twitter</a>, <a href="https://newsletter.mlsafety.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">A technical safety research newsletter</a>, <a href="https://arxiv.org/abs/2306.12001" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">An Overview of Catastrophic AI Risks</a>, and our <a href="https://forms.gle/EU3jfTkxfFgyWVmV7" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">feedback form</a></p><p><a href="https://newsletter.safe.ai/p/the-landscape-of-us-ai-legislation?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Share</a></p>