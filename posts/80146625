<p>Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Center for AI Safety</a>. We discuss developments in AI and AI safety. No technical background required.</p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p><h2>OpenAI announces a ‘superalignment’ team</h2><p>On July 5th, <a href="https://openai.com/blog/introducing-superalignment" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">OpenAI announced the ‘Superalignment’ team</a>: a new research team given the goal of aligning superintelligence, and armed with 20% of OpenAI’s compute. In this story, we’ll explain and discuss the team’s strategy.</p><p><strong>What is superintelligence?</strong> In their announcement, OpenAI distinguishes between ‘artificial general intelligence’ and ‘superintelligence.’ Briefly, ‘artificial general intelligence’ (AGI) is about breadth of performance. Generally intelligent systems perform well on a wide range of cognitive tasks. For example, humans are in many senses generally intelligent: we can learn how to drive a car, take a derivative, or play piano, even though evolution didn’t train us for those tasks. A superintelligent system would not only be generally intelligent, but also <em>much more intelligent</em> than humans. Conservatively, a superintelligence might be to humanity as humanity is to chimps.</p><p><strong>‘Solving’ ‘superalignment’ in four years.</strong> OpenAI believes that superintelligence could arrive this decade. They also believe that it could cause human extinction if it isn’t aligned. By ‘alignment,’ OpenAI means making sure AI systems act according to human intent. So, ‘superalignment’ means making sure <em>superintelligent</em> AI systems act according to human intent (as opposed to doing alignment <em>super </em>well). The Superalignment team’s stated goal is to “solve the core technical challenges of superintelligence alignment in four years.” </p><p>There are two important caveats to this goal. The first is that “human intent” isn’t monolithic. AI safety will have to involve compromise between different human intents. OpenAI knows that their technical work will need to be complemented by <a href="https://openai.com/blog/democratic-inputs-to-ai" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI governance</a>. The second is that alignment may not be a problem able to be conclusively solved once and for all. It might instead be a <a href="https://en.wikipedia.org/wiki/Wicked_problem" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">wicked problem</a> that must be met with varied interventions and ongoing vigilance.</p><p><strong>Current alignment techniques don’t scale to superintelligence. </strong>OpenAI’s current alignment techniques rely on humans to supervise AI. In one technique, “reinforcement learning from human feedback” (RLHF), humans train AI systems to act well by giving them feedback. RLHF is how OpenAI trained ChatGPT to (usually) avoid generating harmful content. </p><p>Humans can generally tell when a less intelligent system is misbehaving. The problem is that humans won’t be able to tell when a superintelligent AI system misbehaves. For example, a superintelligent system might deceive or manipulate human supervisors into giving it positive feedback.</p><p><strong>OpenAI’s approach to alignment is to build and scale an automated alignment researcher.</strong> OpenAI proposes to avoid the problem of human supervision by automating supervision. Once they have built a “roughly human-level” alignment researcher, OpenAI plans to “iteratively align superintelligence” using vast amounts of compute. By “iterative,” OpenAI means that their first automated alignment researcher could align a relatively more capable system, which could then align an <em>even more</em> capable system, and so on. </p><p><strong>OpenAI dedicated 20% of their compute to alignment. </strong>OpenAI’s Superalignment team represents the single largest commitment a leading AI lab — or government, for that matter — has made to AI safety research. Still, it may not be enough. For example, Geoffrey Hinton has suggested that AI labs should contribute about <a href="https://youtu.be/Y6Sgp7y178k" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">50% of their resources to safety research</a>.</p><h2>Musk launches xAI</h2><p>Elon Musk has launched <a href="http://x.ai" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">xAI</a>, a new AI company that aims to compete with OpenAI and DeepMind. In this story, we discuss the implications of the launch.</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cb80b5e-96f7-4522-936a-fe3962bb23f5_292x296.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/tdmuiR9k1I1qen24NQWu73HhJYFp0gpKs5uQdHScnoc=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyY2I4MGI1ZS05NmY3LTQ1MjItOTM2YS1mZTM5NjJiYjIzZjVfMjkyeDI5Ni5wbmc= 424w, https://reader.miniflux.app/proxy/4-KTjQ5HF1qnzcgqhu7lIb7l78w4TMULsQnQoyj437o=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyY2I4MGI1ZS05NmY3LTQ1MjItOTM2YS1mZTM5NjJiYjIzZjVfMjkyeDI5Ni5wbmc= 848w, https://reader.miniflux.app/proxy/YiV6V9RDq7yFpPtqqgScTgJr4u6qIvMh39Qmb4S6DAs=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMmNiODBiNWUtOTZmNy00NTIyLTkzNmEtZmUzOTYyYmIyM2Y1XzI5MngyOTYucG5n 1272w, https://reader.miniflux.app/proxy/xl8fKlP-bSihepBAzjd3gVQPCPJGcfQRK2-44XyKAP0=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMmNiODBiNWUtOTZmNy00NTIyLTkzNmEtZmUzOTYyYmIyM2Y1XzI5MngyOTYucG5n 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/-GEvn7AiZv8mIEWyyXo8gn4NNbdIOJ4zIq9AMWU6SQk=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMmNiODBiNWUtOTZmNy00NTIyLTkzNmEtZmUzOTYyYmIyM2Y1XzI5MngyOTYucG5n" width="292" height="296" alt="" srcset="https://reader.miniflux.app/proxy/OuSRjl8tHguv4h6oJ2YbKH7oeE_f4zKhxXjelt9iV9U=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyY2I4MGI1ZS05NmY3LTQ1MjItOTM2YS1mZTM5NjJiYjIzZjVfMjkyeDI5Ni5wbmc= 424w, https://reader.miniflux.app/proxy/ZxKkpExFwkMI8kymgIqyJmH8iLJvPGhLXv7-M4uC9VM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyY2I4MGI1ZS05NmY3LTQ1MjItOTM2YS1mZTM5NjJiYjIzZjVfMjkyeDI5Ni5wbmc= 848w, https://reader.miniflux.app/proxy/zwlaCGvT5WibEwt9QxkvOfyOYDv-5sXtYhqIU1XfecI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMmNiODBiNWUtOTZmNy00NTIyLTkzNmEtZmUzOTYyYmIyM2Y1XzI5MngyOTYucG5n 1272w, https://reader.miniflux.app/proxy/-GEvn7AiZv8mIEWyyXo8gn4NNbdIOJ4zIq9AMWU6SQk=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMmNiODBiNWUtOTZmNy00NTIyLTkzNmEtZmUzOTYyYmIyM2Y1XzI5MngyOTYucG5n 1456w" sizes="100vw" loading="lazy"/></picture></a></figure><p><strong>What are xAI’s prospects?</strong> Given Musk’s resources, xAI has the potential to challenge OpenAI and DeepMind for a position as a top AI lab. In particular, xAI might be able to draw on the <a href="https://www.tesla.com/AI" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">AI infrastructure at Tesla.</a> Tesla is building what it projects to be <a href="https://electrek.co/2023/06/21/tesla-dojo-supercomputer-is-finally-coming-next-month/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">one of the largest supercomputers in the world by early 2024</a>, and Musk has said that Tesla <a href="https://electrek.co/2022/10/01/tesla-dojo-supercomputer-tripped-power-grid/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">might offer a cloud computing service</a>.</p><p><strong>How will xAI affect AI risk?</strong> It’s unclear how the entrance of xAI will affect AI risk. On one hand, the entrance of another top AI lab might exacerbate the competitive pressures. On the other hand, Musk has been <a href="https://www.theguardian.com/technology/2014/oct/27/elon-musk-artificial-intelligence-ai-biggest-existential-threat" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">one of the earliest public proponents of AI safety</a>. xAI has also <a href="http://x.ai" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">listed</a> Dan Hendrycks, the director of CAIS, as an advisor to xAI. (Note: Hendrycks does not have any financial stake in xAI and chose to receive a token $1 salary for his consulting.) xAI has the potential to direct Musk’s resources towards mitigating AI risk. More information about the organization will come out during this Friday’s Twitter spaces with the xAI team.</p><h2>Developments in Military AI Use</h2><p>According to a recent <a href="https://www.bloomberg.com/news/newsletters/2023-07-05/the-us-military-is-taking-generative-ai-out-for-a-spin" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Bloomberg</a> article, the Pentagon is testing five large language model (LLM) platforms in military applications. One of these platforms is Scale AI’s Donovan. Also, defense companies are advertising AI-powered drones that can autonomously identify and attack targets.</p><p><strong>AI and defense companies are developing LLMs for military use.</strong> Several companies, including Palantir Technologies, Anduril Industries, and Scale AI, are developing LLM-based military decision platforms. The Pentagon is currently testing five of these platforms. Scale AI says its new product, <a href="https://scale.com/donovan" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Donovan</a>, is one of them.</p><p><strong>What are the military applications of LLMs? </strong>The Pentagon is testing the LLM platforms for their ability to analyze and present data in natural language. Military decision-makers could make information requests directly through LLM platforms with access to confidential data. Currently, the military relies on much slower processes. Bloomberg reports that one platform took 10 minutes to complete an information request that would have otherwise taken several days.</p><p>The Pentagon is also testing the platforms for their ability to propose its own courses of action. Bloomberg was allowed to ask Donovan about a US response to a Chinese invasion of Taiwan. It responded: “Direct US intervention with ground, air and naval forces would probably be necessary.&#34;</p><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F293751e3-6de4-4a4c-9cb7-b18c5dd88e82_1600x764.png" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer"><picture><source type="image/webp" srcset="https://reader.miniflux.app/proxy/Fgp-AXSMZZMrIAsMerSBDsyUCxcv2BFl8iuWtR1yXwQ=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyOTM3NTFlMy02ZGU0LTRhNGMtOWNiNy1iMThjNWRkODhlODJfMTYwMHg3NjQucG5n 424w, https://reader.miniflux.app/proxy/lmbuM6b93F0-bwBDcy9AVMGQqXE_97QgF4AvGRzjcA8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX3dlYnAscV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyOTM3NTFlMy02ZGU0LTRhNGMtOWNiNy1iMThjNWRkODhlODJfMTYwMHg3NjQucG5n 848w, https://reader.miniflux.app/proxy/zBLH4W2Holu37y3xjwaCRJmASfLZWYsb7IyBqFQbDgM=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMjkzNzUxZTMtNmRlNC00YTRjLTljYjctYjE4YzVkZDg4ZTgyXzE2MDB4NzY0LnBuZw== 1272w, https://reader.miniflux.app/proxy/63gpxvmTl0HTscYHzMegSmy1zEGehF8rRrXYHFzM2E4=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl93ZWJwLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMjkzNzUxZTMtNmRlNC00YTRjLTljYjctYjE4YzVkZDg4ZTgyXzE2MDB4NzY0LnBuZw== 1456w" sizes="100vw"/><img src="https://reader.miniflux.app/proxy/Q6xycvg1U_RayFd4bhJ-BSw9HDfEAbkdSi9CLP2HZ7g=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMjkzNzUxZTMtNmRlNC00YTRjLTljYjctYjE4YzVkZDg4ZTgyXzE2MDB4NzY0LnBuZw==" alt="" srcset="https://reader.miniflux.app/proxy/SUuuO1ITf8nt5jrCtFxxgeP84jQ_J5-41F4BZZW_ZP8=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd180MjQsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyOTM3NTFlMy02ZGU0LTRhNGMtOWNiNy1iMThjNWRkODhlODJfMTYwMHg3NjQucG5n 424w, https://reader.miniflux.app/proxy/wBkOxdwcioMfw3EsgDO5NeG67ziZFbGVfbRpLPoVg5w=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd184NDgsY19saW1pdCxmX2F1dG8scV9hdXRvOmdvb2QsZmxfcHJvZ3Jlc3NpdmU6c3RlZXAvaHR0cHMlM0ElMkYlMkZzdWJzdGFjay1wb3N0LW1lZGlhLnMzLmFtYXpvbmF3cy5jb20lMkZwdWJsaWMlMkZpbWFnZXMlMkYyOTM3NTFlMy02ZGU0LTRhNGMtOWNiNy1iMThjNWRkODhlODJfMTYwMHg3NjQucG5n 848w, https://reader.miniflux.app/proxy/P39pA9zo8L9gMAta1bIPQSULJyc_uqz-4vqb-fWtFJI=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xMjcyLGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMjkzNzUxZTMtNmRlNC00YTRjLTljYjctYjE4YzVkZDg4ZTgyXzE2MDB4NzY0LnBuZw== 1272w, https://reader.miniflux.app/proxy/Q6xycvg1U_RayFd4bhJ-BSw9HDfEAbkdSi9CLP2HZ7g=/aHR0cHM6Ly9zdWJzdGFja2Nkbi5jb20vaW1hZ2UvZmV0Y2gvd18xNDU2LGNfbGltaXQsZl9hdXRvLHFfYXV0bzpnb29kLGZsX3Byb2dyZXNzaXZlOnN0ZWVwL2h0dHBzJTNBJTJGJTJGc3Vic3RhY2stcG9zdC1tZWRpYS5zMy5hbWF6b25hd3MuY29tJTJGcHVibGljJTJGaW1hZ2VzJTJGMjkzNzUxZTMtNmRlNC00YTRjLTljYjctYjE4YzVkZDg4ZTgyXzE2MDB4NzY0LnBuZw== 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption><em>Scale AI advertises Donovan’s ability to generate novel courses of action</em>.</figcaption></figure><p><strong>The use of LLMs follows recent developments in AI-powered drones</strong>. AI systems have already been tested and deployed in autonomous flight and targeting. In 2020, DARPA’s AlphaDogfight program produced an AI pilot capable of consistently beating human pilots in simulations. A <a href="https://thebulletin.org/2021/05/was-a-flying-killer-robot-used-in-libya-quite-possibly/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">UN report</a> suggests that the first fully-autonomous drone attack occurred in Libya the same year. The company Elbit Systems is now advertising a similar <a href="https://www.youtube.com/watch?v=G7yIzY1BxuI" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">“search and attack” drone</a> that approaches humans then explodes, and the US may be <a href="https://www.newscientist.com/article/2380971-drones-with-ai-targeting-system-claimed-to-be-better-than-human/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">evaluating AI targeting systems.</a></p><p><strong>Should we be concerned?</strong> If LLMs or AI drones give militaries a competitive advantage over their adversaries, then their use might lead to an arms race dynamic. Competing nations might increasingly invest in and deploy frontier AI models. Such a dynamic has the potential to exacerbate AI risk. For example, militaries might <a href="https://www.cnas.org/publications/reports/technology-roulette" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">lose control over increasingly complex AI systems</a>.</p><p>Links:</p><ul><li><p>The UN Security Council is set to hold its <a href="https://apnews.com/article/artificial-intelligence-un-security-council-meeting-uk-f7fb6d8f8a261a9d9b23ca463ee29d3d" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">first meeting on the threats of AI to global peace.</a></p></li><li><p>The <a href="https://committees.parliament.uk/call-for-evidence/3183/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">UK parliament calls for evidence</a> on how to “respond to the opportunities and risks posed by large language models.”</p></li><li><p>Similarly, the Australian government <a href="https://consult.industry.gov.au/supporting-responsible-ai" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">asks for advice on mitigating AI risks</a>.</p></li><li><p>The director of the CIA is <a href="https://www.washingtonpost.com/opinions/2023/07/07/cia-director-william-burns-ditchley-speech-adaptation/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">concerned about the accelerating rate of AI capabilities</a>.</p></li><li><p>OpenAI releases a <a href="https://twitter.com/OpenAI/status/1677015057316872192" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">code interpreter,</a> which allows ChatGPT to run code.</p></li><li><p>Anthropic <a href="https://www.anthropic.com/index/claude-2" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">releases its newest model, Claude 2</a>.</p></li><li><p>The Boston Globe <a href="https://www.bostonglobe.com/2023/07/06/opinion/ai-safety-human-extinction-dan-hendrycks-cais/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">interviews the director of CAIS</a>.</p></li><li><p>DeepMind explores international AI governance in <a href="https://www.deepmind.com/blog/exploring-institutions-for-global-ai-governance" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">a new white paper</a>.</p></li><li><p>A <a href="https://arxiv.org/abs/2307.03718" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">new paper</a> discusses regulating frontier models.</p></li></ul><p>See also: <a href="https://www.safe.ai/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS website</a>, <a href="https://twitter.com/ai_risks?lang=en" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">CAIS twitter</a>, <a href="https://newsletter.mlsafety.org/" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">A technical safety research newsletter</a>, and <a href="https://arxiv.org/abs/2306.12001" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">An Overview of Catastrophic AI Risks</a></p><p><a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Share</a></p><p><a href="https://newsletter.safe.ai/subscribe?" rel="noopener noreferrer" target="_blank" referrerpolicy="no-referrer">Subscribe now</a></p>